{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind GANs is actually very simple. At its core, a GAN includes two agents with competing objectives that work through opposing goals. This relatively simple setup results in both of the agent's coming up with increasingly complex ways. An analogy would be a counterfeiter trying to post fake posts and a cyber vigilante trying to ascertain and flag fake posts.This is similar to the minmax in game theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider x and z to be distributions . A generator G(z) has the objective of mapping noise to real world data , \n",
    "the input x. It tries to generate data as close as possible to the real dataset. The discriminator D(X) plays \n",
    "the role of adversary here and tries to discriminate between real world data and generated data. The discriminator \n",
    "outputs a  probability that a data came from the real world dataset. The two neural networks are trained and compete\n",
    "with each other until an equilibrium is reached"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual desirable equilibrium point for the above defined GANs is that the Generator should model the real data and Discriminator should output the probability of 0.5 as the generated data is same as the real data -- that is, it is not sure if the new data coming from the generator is real or fake with equal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first we generate data randomnly using numpy and also another coordinate using a cubic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_y(x):\n",
    "    return 21 + x*x\n",
    "\n",
    "\n",
    "def sample_data(n=300, scale=100):\n",
    "    data = []\n",
    "\n",
    "    x = scale*(np.random.random_sample((n,))-0.5)\n",
    "\n",
    "    for i in range(n):\n",
    "        yi = get_y(x[i])\n",
    "        data.append([x[i], yi])\n",
    "\n",
    "    return np.array(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "inputs = [-0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# calculate outputs\n",
    "outputs = [get_y(x) for x in inputs]\n",
    "# plot the result\n",
    "plt.scatter(inputs, outputs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_data()\n",
    "# plot samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator D(X) plays the role of adversary here and tries to discriminate between real world data \n",
    "and generated data. So the discriminator is bascically a binary classification problem which tells if an image \n",
    "is fake or not. Well technically it outputs a probability that a generated image came from training data rather\n",
    "than from generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in /home/venktesh/anaconda3/envs/pysyft/lib/python3.6/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: graphviz in /home/venktesh/anaconda3/envs/pysyft/lib/python3.6/site-packages (from torchviz) (0.11.1)\r\n",
      "Requirement already satisfied: torch in /home/venktesh/anaconda3/envs/pysyft/lib/python3.6/site-packages (from torchviz) (1.1.0)\r\n",
      "Requirement already satisfied: numpy in /home/venktesh/anaconda3/envs/pysyft/lib/python3.6/site-packages (from torch->torchviz) (1.16.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"343pt\" height=\"864pt\"\n",
       " viewBox=\"0.00 0.00 343.49 864.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.842927 0.842927) rotate(0) translate(4 1021)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1021 403.5,-1021 403.5,4 -4,4\"/>\n",
       "<!-- 139990935514920 -->\n",
       "<g id=\"node1\" class=\"node\"><title>139990935514920</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"374,-21 269,-21 269,-0 374,-0 374,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"321.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">SigmoidBackward</text>\n",
       "</g>\n",
       "<!-- 139990935524968 -->\n",
       "<g id=\"node2\" class=\"node\"><title>139990935524968</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"367.5,-78 275.5,-78 275.5,-57 367.5,-57 367.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"321.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935524968&#45;&gt;139990935514920 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>139990935524968&#45;&gt;139990935514920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M321.5,-56.9197C321.5,-49.9083 321.5,-40.1442 321.5,-31.4652\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"325,-31.3408 321.5,-21.3408 318,-31.3409 325,-31.3408\"/>\n",
       "</g>\n",
       "<!-- 139990935525080 -->\n",
       "<g id=\"node3\" class=\"node\"><title>139990935525080</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"327,-142 216,-142 216,-121 327,-121 327,-142\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-128.4\" font-family=\"Times,serif\" font-size=\"12.00\">SqueezeBackward3</text>\n",
       "</g>\n",
       "<!-- 139990935525080&#45;&gt;139990935524968 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>139990935525080&#45;&gt;139990935524968</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.312,-120.812C286.754,-111.585 298.133,-97.4746 307.199,-86.233\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"310.024,-88.3054 313.577,-78.3241 304.575,-83.9111 310.024,-88.3054\"/>\n",
       "</g>\n",
       "<!-- 139990935525248 -->\n",
       "<g id=\"node4\" class=\"node\"><title>139990935525248</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"314,-206 229,-206 229,-185 314,-185 314,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-192.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 139990935525248&#45;&gt;139990935525080 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>139990935525248&#45;&gt;139990935525080</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M271.5,-184.812C271.5,-176.218 271.5,-163.388 271.5,-152.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"275,-152.324 271.5,-142.324 268,-152.324 275,-152.324\"/>\n",
       "</g>\n",
       "<!-- 139990935525360 -->\n",
       "<g id=\"node5\" class=\"node\"><title>139990935525360</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"275.5,-263 151.5,-263 151.5,-242 275.5,-242 275.5,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-249.4\" font-family=\"Times,serif\" font-size=\"12.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935525360&#45;&gt;139990935525248 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>139990935525360&#45;&gt;139990935525248</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M223.603,-241.92C231.9,-234.051 243.854,-222.716 253.743,-213.338\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"256.274,-215.761 261.122,-206.341 251.458,-210.682 256.274,-215.761\"/>\n",
       "</g>\n",
       "<!-- 139990935525528 -->\n",
       "<g id=\"node6\" class=\"node\"><title>139990935525528</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"260.5,-327 166.5,-327 166.5,-306 260.5,-306 260.5,-327\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-313.4\" font-family=\"Times,serif\" font-size=\"12.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935525528&#45;&gt;139990935525360 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>139990935525528&#45;&gt;139990935525360</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213.5,-305.812C213.5,-297.218 213.5,-284.388 213.5,-273.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217,-273.324 213.5,-263.324 210,-273.324 217,-273.324\"/>\n",
       "</g>\n",
       "<!-- 139990935525640 -->\n",
       "<g id=\"node7\" class=\"node\"><title>139990935525640</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"259.5,-391 167.5,-391 167.5,-370 259.5,-370 259.5,-391\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-377.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935525640&#45;&gt;139990935525528 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>139990935525640&#45;&gt;139990935525528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213.5,-369.812C213.5,-361.218 213.5,-348.388 213.5,-337.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217,-337.324 213.5,-327.324 210,-337.324 217,-337.324\"/>\n",
       "</g>\n",
       "<!-- 139990935525752 -->\n",
       "<g id=\"node8\" class=\"node\"><title>139990935525752</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"219,-455 108,-455 108,-434 219,-434 219,-455\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-441.4\" font-family=\"Times,serif\" font-size=\"12.00\">SqueezeBackward3</text>\n",
       "</g>\n",
       "<!-- 139990935525752&#45;&gt;139990935525640 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>139990935525752&#45;&gt;139990935525640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.312,-433.812C178.754,-424.585 190.133,-410.475 199.199,-399.233\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"202.024,-401.305 205.577,-391.324 196.575,-396.911 202.024,-401.305\"/>\n",
       "</g>\n",
       "<!-- 139990935525920 -->\n",
       "<g id=\"node9\" class=\"node\"><title>139990935525920</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"206,-519 121,-519 121,-498 206,-498 206,-519\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-505.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 139990935525920&#45;&gt;139990935525752 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>139990935525920&#45;&gt;139990935525752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-497.812C163.5,-489.218 163.5,-476.388 163.5,-465.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-465.324 163.5,-455.324 160,-465.324 167,-465.324\"/>\n",
       "</g>\n",
       "<!-- 139990935526032 -->\n",
       "<g id=\"node10\" class=\"node\"><title>139990935526032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"167.5,-576 43.5,-576 43.5,-555 167.5,-555 167.5,-576\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-562.4\" font-family=\"Times,serif\" font-size=\"12.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935526032&#45;&gt;139990935525920 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>139990935526032&#45;&gt;139990935525920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.603,-554.92C123.9,-547.051 135.854,-535.716 145.743,-526.338\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"148.274,-528.761 153.122,-519.341 143.458,-523.682 148.274,-528.761\"/>\n",
       "</g>\n",
       "<!-- 139990935526200 -->\n",
       "<g id=\"node11\" class=\"node\"><title>139990935526200</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-640 60,-640 60,-619 151,-619 151,-640\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-626.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935526200&#45;&gt;139990935526032 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>139990935526200&#45;&gt;139990935526032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105.5,-618.812C105.5,-610.218 105.5,-597.388 105.5,-586.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109,-586.324 105.5,-576.324 102,-586.324 109,-586.324\"/>\n",
       "</g>\n",
       "<!-- 139990935526312 -->\n",
       "<g id=\"node12\" class=\"node\"><title>139990935526312</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"152.5,-704 58.5,-704 58.5,-683 152.5,-683 152.5,-704\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-690.4\" font-family=\"Times,serif\" font-size=\"12.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935526312&#45;&gt;139990935526200 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>139990935526312&#45;&gt;139990935526200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105.5,-682.812C105.5,-674.218 105.5,-661.388 105.5,-650.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109,-650.324 105.5,-640.324 102,-650.324 109,-650.324\"/>\n",
       "</g>\n",
       "<!-- 139990935526424 -->\n",
       "<g id=\"node13\" class=\"node\"><title>139990935526424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151.5,-761 59.5,-761 59.5,-740 151.5,-740 151.5,-761\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-747.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935526424&#45;&gt;139990935526312 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>139990935526424&#45;&gt;139990935526312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105.5,-739.92C105.5,-732.908 105.5,-723.144 105.5,-714.465\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109,-714.341 105.5,-704.341 102,-714.341 109,-714.341\"/>\n",
       "</g>\n",
       "<!-- 139990935526536 -->\n",
       "<g id=\"node14\" class=\"node\"><title>139990935526536</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"111,-825 7.10543e-15,-825 7.10543e-15,-804 111,-804 111,-825\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-811.4\" font-family=\"Times,serif\" font-size=\"12.00\">SqueezeBackward3</text>\n",
       "</g>\n",
       "<!-- 139990935526536&#45;&gt;139990935526424 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>139990935526536&#45;&gt;139990935526424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M63.3125,-803.812C70.754,-794.585 82.1334,-780.475 91.1992,-769.233\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"94.0242,-771.305 97.5774,-761.324 88.5753,-766.911 94.0242,-771.305\"/>\n",
       "</g>\n",
       "<!-- 139990935526704 -->\n",
       "<g id=\"node15\" class=\"node\"><title>139990935526704</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"98,-889 13,-889 13,-868 98,-868 98,-889\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-875.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 139990935526704&#45;&gt;139990935526536 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>139990935526704&#45;&gt;139990935526536</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.5,-867.812C55.5,-859.218 55.5,-846.388 55.5,-835.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.0001,-835.324 55.5,-825.324 52.0001,-835.324 59.0001,-835.324\"/>\n",
       "</g>\n",
       "<!-- 139990935526816 -->\n",
       "<g id=\"node16\" class=\"node\"><title>139990935526816</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"92,-946 19,-946 19,-925 92,-925 92,-946\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-932.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 139990935526816&#45;&gt;139990935526704 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>139990935526816&#45;&gt;139990935526704</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.5,-924.92C55.5,-917.908 55.5,-908.144 55.5,-899.465\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.0001,-899.341 55.5,-889.341 52.0001,-899.341 59.0001,-899.341\"/>\n",
       "</g>\n",
       "<!-- 139990935526928 -->\n",
       "<g id=\"node17\" class=\"node\"><title>139990935526928</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"82.5,-1017 28.5,-1017 28.5,-982 82.5,-982 82.5,-1017\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-989.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (15, 2)</text>\n",
       "</g>\n",
       "<!-- 139990935526928&#45;&gt;139990935526816 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>139990935526928&#45;&gt;139990935526816</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.5,-981.885C55.5,-973.994 55.5,-964.505 55.5,-956.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.0001,-956.018 55.5,-946.018 52.0001,-956.018 59.0001,-956.018\"/>\n",
       "</g>\n",
       "<!-- 139990935526592 -->\n",
       "<g id=\"node18\" class=\"node\"><title>139990935526592</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"183.5,-832 129.5,-832 129.5,-797 183.5,-797 183.5,-832\"/>\n",
       "<text text-anchor=\"middle\" x=\"156.5\" y=\"-804.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (15)</text>\n",
       "</g>\n",
       "<!-- 139990935526592&#45;&gt;139990935526424 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>139990935526592&#45;&gt;139990935526424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.833,-796.885C135.678,-788.187 126.926,-777.547 119.694,-768.756\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.385,-766.517 113.329,-761.018 116.979,-770.964 122.385,-766.517\"/>\n",
       "</g>\n",
       "<!-- 139990935526088 -->\n",
       "<g id=\"node19\" class=\"node\"><title>139990935526088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"259,-576 186,-576 186,-555 259,-555 259,-576\"/>\n",
       "<text text-anchor=\"middle\" x=\"222.5\" y=\"-562.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 139990935526088&#45;&gt;139990935525920 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>139990935526088&#45;&gt;139990935525920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212.223,-554.92C203.782,-547.051 191.623,-535.716 181.563,-526.338\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.758,-523.599 174.057,-519.341 178.985,-528.72 183.758,-523.599\"/>\n",
       "</g>\n",
       "<!-- 139990935526256 -->\n",
       "<g id=\"node20\" class=\"node\"><title>139990935526256</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"251,-647 194,-647 194,-612 251,-612 251,-647\"/>\n",
       "<text text-anchor=\"middle\" x=\"222.5\" y=\"-619.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (15, 15)</text>\n",
       "</g>\n",
       "<!-- 139990935526256&#45;&gt;139990935526088 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>139990935526256&#45;&gt;139990935526088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.5,-611.885C222.5,-603.994 222.5,-594.505 222.5,-586.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226,-586.018 222.5,-576.018 219,-586.018 226,-586.018\"/>\n",
       "</g>\n",
       "<!-- 139990935525808 -->\n",
       "<g id=\"node21\" class=\"node\"><title>139990935525808</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"291.5,-462 237.5,-462 237.5,-427 291.5,-427 291.5,-462\"/>\n",
       "<text text-anchor=\"middle\" x=\"264.5\" y=\"-434.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (15)</text>\n",
       "</g>\n",
       "<!-- 139990935525808&#45;&gt;139990935525640 -->\n",
       "<g id=\"edge20\" class=\"edge\"><title>139990935525808&#45;&gt;139990935525640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M250.833,-426.885C243.678,-418.187 234.926,-407.547 227.694,-398.756\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"230.385,-396.517 221.329,-391.018 224.979,-400.964 230.385,-396.517\"/>\n",
       "</g>\n",
       "<!-- 139990935525416 -->\n",
       "<g id=\"node22\" class=\"node\"><title>139990935525416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"367,-263 294,-263 294,-242 367,-242 367,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"330.5\" y=\"-249.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 139990935525416&#45;&gt;139990935525248 -->\n",
       "<g id=\"edge21\" class=\"edge\"><title>139990935525416&#45;&gt;139990935525248</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M320.223,-241.92C311.782,-234.051 299.623,-222.716 289.563,-213.338\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.758,-210.599 282.057,-206.341 286.985,-215.72 291.758,-210.599\"/>\n",
       "</g>\n",
       "<!-- 139990935525584 -->\n",
       "<g id=\"node23\" class=\"node\"><title>139990935525584</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"357.5,-334 303.5,-334 303.5,-299 357.5,-299 357.5,-334\"/>\n",
       "<text text-anchor=\"middle\" x=\"330.5\" y=\"-306.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1, 15)</text>\n",
       "</g>\n",
       "<!-- 139990935525584&#45;&gt;139990935525416 -->\n",
       "<g id=\"edge22\" class=\"edge\"><title>139990935525584&#45;&gt;139990935525416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.5,-298.885C330.5,-290.994 330.5,-281.505 330.5,-273.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"334,-273.018 330.5,-263.018 327,-273.018 334,-273.018\"/>\n",
       "</g>\n",
       "<!-- 139990935525136 -->\n",
       "<g id=\"node24\" class=\"node\"><title>139990935525136</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"399.5,-149 345.5,-149 345.5,-114 399.5,-114 399.5,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"372.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139990935525136&#45;&gt;139990935524968 -->\n",
       "<g id=\"edge23\" class=\"edge\"><title>139990935525136&#45;&gt;139990935524968</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M358.833,-113.885C351.678,-105.187 342.926,-94.5469 335.694,-85.756\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"338.385,-83.5172 329.329,-78.0178 332.979,-87.9641 338.385,-83.5172\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f522dfaa1d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# define the discriminator model\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from torchviz import make_dot\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "    model = nn.Sequential(nn.Linear(n_inputs, 15),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(),\n",
    "                          nn.Linear(15,15),\n",
    "                           nn.ReLU(),\n",
    "                          nn.Linear(15,1),\n",
    "                          nn.Sigmoid()\n",
    "                         )\n",
    "    return model\n",
    "\n",
    "# define the discriminator model\n",
    "x = Variable(torch.from_numpy(np.random.randn(2))).float()\n",
    "model  = define_discriminator()\n",
    "output = model(x)\n",
    "make_dot(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator has a hidden layer with 25 neurons and uses Relu activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us generate some real samples. to the generate samples method above let us add a small piece of code to \n",
    "return labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(n=100, label='real'):\n",
    "    \"\"\"Generate samples with real or fake label\n",
    "    \"\"\"\n",
    "    x = np.random.randn(n, 1)\n",
    "    output = get_y(x)\n",
    "    \n",
    "    y = np.ones((n, 1)) * (label == 'real')\n",
    "    return np.hstack([x, output]), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_samples(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.61091576, 21.37321806],\n",
       "       [ 0.48274208, 21.23303991],\n",
       "       [-0.65266228, 21.42596805],\n",
       "       [ 0.12603831, 21.01588565],\n",
       "       [-0.47046936, 21.22134142],\n",
       "       [-0.88787289, 21.78831827],\n",
       "       [-0.5124734 , 21.26262899],\n",
       "       [ 0.68821689, 21.47364248],\n",
       "       [-0.91545177, 21.83805193],\n",
       "       [-1.8685725 , 24.49156318]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_labelled_samples(n=100, label='fake'):\n",
    "    \"\"\"Generate samples with real or fake label\n",
    "    \"\"\"\n",
    "    x = np.random.randn(n, 1) *2 + -1\n",
    "    output = get_y(x)\n",
    "    \n",
    "    y = np.zeros((n, 1)) * (label == 'fake')\n",
    "    return np.hstack([x, output]), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_steps = 100\n",
    "g_steps = 100\n",
    "def train_discriminator(model, n_epochs=3000, n_batch=128):\n",
    "    half_batch = int(n_batch/2)\n",
    "    for i in range(n_epochs):\n",
    "        running_loss=0\n",
    "        running_fake_loss=0\n",
    "        model.zero_grad()\n",
    "        X_real, y_real = generate_samples(half_batch)\n",
    "        X_real = Variable(torch.from_numpy(X_real))\n",
    "        output = model(X_real.float())\n",
    "        y_real = Variable(torch.FloatTensor(y_real))\n",
    "        loss = criterion(output, y_real)\n",
    "        loss.backward()\n",
    "        X_fake,y_fake = generate_fake_labelled_samples(half_batch)\n",
    "        X_fake = Variable(torch.from_numpy(X_fake))\n",
    "        fake_output = model(X_fake.float())\n",
    "        fake_loss = criterion(fake_output, torch.FloatTensor(y_fake))\n",
    "        fake_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_fake_loss += fake_loss.item()\n",
    "\n",
    "        print(f\"Real samples Training loss: {running_loss}\")\n",
    "        print(f\"Fake samples Training loss: {running_fake_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.5061293244361877\n",
      "Fake samples Training loss: 1.3122004270553589\n",
      "Real samples Training loss: 0.5037865042686462\n",
      "Fake samples Training loss: 1.3725450038909912\n",
      "Real samples Training loss: 0.530781090259552\n",
      "Fake samples Training loss: 1.1039958000183105\n",
      "Real samples Training loss: 0.5081878304481506\n",
      "Fake samples Training loss: 1.3019990921020508\n",
      "Real samples Training loss: 0.456003338098526\n",
      "Fake samples Training loss: 1.2855772972106934\n",
      "Real samples Training loss: 0.42575809359550476\n",
      "Fake samples Training loss: 1.3216420412063599\n",
      "Real samples Training loss: 0.5013777613639832\n",
      "Fake samples Training loss: 1.2441879510879517\n",
      "Real samples Training loss: 0.46350333094596863\n",
      "Fake samples Training loss: 1.4911941289901733\n",
      "Real samples Training loss: 0.5149351954460144\n",
      "Fake samples Training loss: 1.273166298866272\n",
      "Real samples Training loss: 0.5286311507225037\n",
      "Fake samples Training loss: 1.2764523029327393\n",
      "Real samples Training loss: 0.4643946588039398\n",
      "Fake samples Training loss: 1.179929256439209\n",
      "Real samples Training loss: 0.4554799497127533\n",
      "Fake samples Training loss: 1.191145658493042\n",
      "Real samples Training loss: 0.47589850425720215\n",
      "Fake samples Training loss: 1.29019033908844\n",
      "Real samples Training loss: 0.47041529417037964\n",
      "Fake samples Training loss: 1.256700038909912\n",
      "Real samples Training loss: 0.5471893548965454\n",
      "Fake samples Training loss: 1.2906192541122437\n",
      "Real samples Training loss: 0.5237221717834473\n",
      "Fake samples Training loss: 1.193793535232544\n",
      "Real samples Training loss: 0.5314002633094788\n",
      "Fake samples Training loss: 1.3172242641448975\n",
      "Real samples Training loss: 0.4443491995334625\n",
      "Fake samples Training loss: 1.1403506994247437\n",
      "Real samples Training loss: 0.4760960042476654\n",
      "Fake samples Training loss: 1.251455545425415\n",
      "Real samples Training loss: 0.46125665307044983\n",
      "Fake samples Training loss: 1.2165852785110474\n",
      "Real samples Training loss: 0.5144743323326111\n",
      "Fake samples Training loss: 1.3863961696624756\n",
      "Real samples Training loss: 0.5010882019996643\n",
      "Fake samples Training loss: 1.2979376316070557\n",
      "Real samples Training loss: 0.5369436740875244\n",
      "Fake samples Training loss: 1.3010507822036743\n",
      "Real samples Training loss: 0.5003975033760071\n",
      "Fake samples Training loss: 1.3141813278198242\n",
      "Real samples Training loss: 0.42301011085510254\n",
      "Fake samples Training loss: 1.2820013761520386\n",
      "Real samples Training loss: 0.45863601565361023\n",
      "Fake samples Training loss: 1.2465320825576782\n",
      "Real samples Training loss: 0.48804590106010437\n",
      "Fake samples Training loss: 1.2518384456634521\n",
      "Real samples Training loss: 0.5224294662475586\n",
      "Fake samples Training loss: 1.1714032888412476\n",
      "Real samples Training loss: 0.5016850233078003\n",
      "Fake samples Training loss: 1.2057669162750244\n",
      "Real samples Training loss: 0.3804300129413605\n",
      "Fake samples Training loss: 1.2795180082321167\n",
      "Real samples Training loss: 0.43021145462989807\n",
      "Fake samples Training loss: 1.2477632761001587\n",
      "Real samples Training loss: 0.4677308201789856\n",
      "Fake samples Training loss: 1.3702585697174072\n",
      "Real samples Training loss: 0.489335298538208\n",
      "Fake samples Training loss: 1.3200095891952515\n",
      "Real samples Training loss: 0.45744433999061584\n",
      "Fake samples Training loss: 1.2232961654663086\n",
      "Real samples Training loss: 0.4871222972869873\n",
      "Fake samples Training loss: 1.3763402700424194\n",
      "Real samples Training loss: 0.4490997791290283\n",
      "Fake samples Training loss: 1.3473864793777466\n",
      "Real samples Training loss: 0.440217524766922\n",
      "Fake samples Training loss: 1.1633052825927734\n",
      "Real samples Training loss: 0.4811577796936035\n",
      "Fake samples Training loss: 1.3367304801940918\n",
      "Real samples Training loss: 0.43110814690589905\n",
      "Fake samples Training loss: 1.2310551404953003\n",
      "Real samples Training loss: 0.4834110736846924\n",
      "Fake samples Training loss: 1.2467612028121948\n",
      "Real samples Training loss: 0.47442197799682617\n",
      "Fake samples Training loss: 1.1335793733596802\n",
      "Real samples Training loss: 0.4730043411254883\n",
      "Fake samples Training loss: 1.319710373878479\n",
      "Real samples Training loss: 0.48370814323425293\n",
      "Fake samples Training loss: 1.2559813261032104\n",
      "Real samples Training loss: 0.5453253388404846\n",
      "Fake samples Training loss: 1.1565314531326294\n",
      "Real samples Training loss: 0.47558560967445374\n",
      "Fake samples Training loss: 1.327955722808838\n",
      "Real samples Training loss: 0.47664815187454224\n",
      "Fake samples Training loss: 1.202470302581787\n",
      "Real samples Training loss: 0.49654555320739746\n",
      "Fake samples Training loss: 1.2007333040237427\n",
      "Real samples Training loss: 0.5197319388389587\n",
      "Fake samples Training loss: 1.1833621263504028\n",
      "Real samples Training loss: 0.4860045611858368\n",
      "Fake samples Training loss: 1.1860908269882202\n",
      "Real samples Training loss: 0.4262183606624603\n",
      "Fake samples Training loss: 1.2215957641601562\n",
      "Real samples Training loss: 0.49933698773384094\n",
      "Fake samples Training loss: 1.3227063417434692\n",
      "Real samples Training loss: 0.5314797759056091\n",
      "Fake samples Training loss: 1.2094500064849854\n",
      "Real samples Training loss: 0.47762301564216614\n",
      "Fake samples Training loss: 1.1388120651245117\n",
      "Real samples Training loss: 0.3811091482639313\n",
      "Fake samples Training loss: 1.1281944513320923\n",
      "Real samples Training loss: 0.5099769830703735\n",
      "Fake samples Training loss: 1.2731823921203613\n",
      "Real samples Training loss: 0.46452006697654724\n",
      "Fake samples Training loss: 1.1863224506378174\n",
      "Real samples Training loss: 0.5077886581420898\n",
      "Fake samples Training loss: 1.212136149406433\n",
      "Real samples Training loss: 0.474608838558197\n",
      "Fake samples Training loss: 1.305433988571167\n",
      "Real samples Training loss: 0.4611567556858063\n",
      "Fake samples Training loss: 1.277552604675293\n",
      "Real samples Training loss: 0.45284393429756165\n",
      "Fake samples Training loss: 1.3084262609481812\n",
      "Real samples Training loss: 0.5174212455749512\n",
      "Fake samples Training loss: 1.2512125968933105\n",
      "Real samples Training loss: 0.3996765613555908\n",
      "Fake samples Training loss: 1.2203359603881836\n",
      "Real samples Training loss: 0.47983527183532715\n",
      "Fake samples Training loss: 1.2706867456436157\n",
      "Real samples Training loss: 0.4639240503311157\n",
      "Fake samples Training loss: 1.2428594827651978\n",
      "Real samples Training loss: 0.4477791488170624\n",
      "Fake samples Training loss: 1.3053990602493286\n",
      "Real samples Training loss: 0.4850907623767853\n",
      "Fake samples Training loss: 1.1292368173599243\n",
      "Real samples Training loss: 0.4718310534954071\n",
      "Fake samples Training loss: 1.2505072355270386\n",
      "Real samples Training loss: 0.5121400952339172\n",
      "Fake samples Training loss: 1.195120930671692\n",
      "Real samples Training loss: 0.4750916063785553\n",
      "Fake samples Training loss: 1.318087100982666\n",
      "Real samples Training loss: 0.44592949748039246\n",
      "Fake samples Training loss: 1.2718998193740845\n",
      "Real samples Training loss: 0.5106740593910217\n",
      "Fake samples Training loss: 1.3197295665740967\n",
      "Real samples Training loss: 0.5169335603713989\n",
      "Fake samples Training loss: 1.291854977607727\n",
      "Real samples Training loss: 0.46529996395111084\n",
      "Fake samples Training loss: 1.186420202255249\n",
      "Real samples Training loss: 0.46035754680633545\n",
      "Fake samples Training loss: 1.1779688596725464\n",
      "Real samples Training loss: 0.49470970034599304\n",
      "Fake samples Training loss: 1.3347266912460327\n",
      "Real samples Training loss: 0.49750271439552307\n",
      "Fake samples Training loss: 1.280382752418518\n",
      "Real samples Training loss: 0.5204650163650513\n",
      "Fake samples Training loss: 1.3310153484344482\n",
      "Real samples Training loss: 0.4534699320793152\n",
      "Fake samples Training loss: 1.2697035074234009\n",
      "Real samples Training loss: 0.5117146372795105\n",
      "Fake samples Training loss: 1.277092456817627\n",
      "Real samples Training loss: 0.5329169034957886\n",
      "Fake samples Training loss: 1.1581673622131348\n",
      "Real samples Training loss: 0.4842626750469208\n",
      "Fake samples Training loss: 1.2265081405639648\n",
      "Real samples Training loss: 0.4957822263240814\n",
      "Fake samples Training loss: 1.1567414999008179\n",
      "Real samples Training loss: 0.44111520051956177\n",
      "Fake samples Training loss: 1.2113213539123535\n",
      "Real samples Training loss: 0.44959554076194763\n",
      "Fake samples Training loss: 1.290022373199463\n",
      "Real samples Training loss: 0.4812231659889221\n",
      "Fake samples Training loss: 1.1317801475524902\n",
      "Real samples Training loss: 0.43832606077194214\n",
      "Fake samples Training loss: 1.2136081457138062\n",
      "Real samples Training loss: 0.498163640499115\n",
      "Fake samples Training loss: 1.2185397148132324\n",
      "Real samples Training loss: 0.4424804747104645\n",
      "Fake samples Training loss: 1.1198426485061646\n",
      "Real samples Training loss: 0.506597638130188\n",
      "Fake samples Training loss: 1.2559385299682617\n",
      "Real samples Training loss: 0.5175807476043701\n",
      "Fake samples Training loss: 1.1977897882461548\n",
      "Real samples Training loss: 0.5024069547653198\n",
      "Fake samples Training loss: 1.134661078453064\n",
      "Real samples Training loss: 0.5333594083786011\n",
      "Fake samples Training loss: 1.240295648574829\n",
      "Real samples Training loss: 0.4790570139884949\n",
      "Fake samples Training loss: 1.2748088836669922\n",
      "Real samples Training loss: 0.45366403460502625\n",
      "Fake samples Training loss: 1.1619857549667358\n",
      "Real samples Training loss: 0.48044392466545105\n",
      "Fake samples Training loss: 1.1916908025741577\n",
      "Real samples Training loss: 0.4760843813419342\n",
      "Fake samples Training loss: 1.2384302616119385\n",
      "Real samples Training loss: 0.555682361125946\n",
      "Fake samples Training loss: 1.2657475471496582\n",
      "Real samples Training loss: 0.3810734748840332\n",
      "Fake samples Training loss: 1.2443878650665283\n",
      "Real samples Training loss: 0.47135651111602783\n",
      "Fake samples Training loss: 1.2723616361618042\n",
      "Real samples Training loss: 0.494621217250824\n",
      "Fake samples Training loss: 1.3277941942214966\n",
      "Real samples Training loss: 0.4995640516281128\n",
      "Fake samples Training loss: 1.157130241394043\n",
      "Real samples Training loss: 0.49860432744026184\n",
      "Fake samples Training loss: 1.3149323463439941\n",
      "Real samples Training loss: 0.4809339642524719\n",
      "Fake samples Training loss: 1.3547234535217285\n",
      "Real samples Training loss: 0.5101671814918518\n",
      "Fake samples Training loss: 1.3549752235412598\n",
      "Real samples Training loss: 0.5490986108779907\n",
      "Fake samples Training loss: 1.1480270624160767\n",
      "Real samples Training loss: 0.4800459146499634\n",
      "Fake samples Training loss: 1.3194341659545898\n",
      "Real samples Training loss: 0.5035451650619507\n",
      "Fake samples Training loss: 1.1202716827392578\n",
      "Real samples Training loss: 0.4523753821849823\n",
      "Fake samples Training loss: 1.2642812728881836\n",
      "Real samples Training loss: 0.49207228422164917\n",
      "Fake samples Training loss: 1.1658258438110352\n",
      "Real samples Training loss: 0.5351695418357849\n",
      "Fake samples Training loss: 1.3017480373382568\n",
      "Real samples Training loss: 0.5137849450111389\n",
      "Fake samples Training loss: 1.3514686822891235\n",
      "Real samples Training loss: 0.5074095726013184\n",
      "Fake samples Training loss: 1.138498067855835\n",
      "Real samples Training loss: 0.4758836328983307\n",
      "Fake samples Training loss: 1.2591626644134521\n",
      "Real samples Training loss: 0.49899759888648987\n",
      "Fake samples Training loss: 1.1592278480529785\n",
      "Real samples Training loss: 0.47396355867385864\n",
      "Fake samples Training loss: 1.4714953899383545\n",
      "Real samples Training loss: 0.45853644609451294\n",
      "Fake samples Training loss: 1.3654578924179077\n",
      "Real samples Training loss: 0.4656556248664856\n",
      "Fake samples Training loss: 1.249890923500061\n",
      "Real samples Training loss: 0.47803765535354614\n",
      "Fake samples Training loss: 1.1367253065109253\n",
      "Real samples Training loss: 0.48181965947151184\n",
      "Fake samples Training loss: 1.3402364253997803\n",
      "Real samples Training loss: 0.456119567155838\n",
      "Fake samples Training loss: 1.079155445098877\n",
      "Real samples Training loss: 0.49641621112823486\n",
      "Fake samples Training loss: 1.3358502388000488\n",
      "Real samples Training loss: 0.5190719962120056\n",
      "Fake samples Training loss: 1.3633778095245361\n",
      "Real samples Training loss: 0.560820460319519\n",
      "Fake samples Training loss: 1.1836953163146973\n",
      "Real samples Training loss: 0.5007529854774475\n",
      "Fake samples Training loss: 1.214377760887146\n",
      "Real samples Training loss: 0.42809879779815674\n",
      "Fake samples Training loss: 1.1311146020889282\n",
      "Real samples Training loss: 0.4236999452114105\n",
      "Fake samples Training loss: 1.2372758388519287\n",
      "Real samples Training loss: 0.43671610951423645\n",
      "Fake samples Training loss: 1.1477677822113037\n",
      "Real samples Training loss: 0.5595588088035583\n",
      "Fake samples Training loss: 1.1678646802902222\n",
      "Real samples Training loss: 0.49445438385009766\n",
      "Fake samples Training loss: 1.1970635652542114\n",
      "Real samples Training loss: 0.5019810199737549\n",
      "Fake samples Training loss: 1.2284183502197266\n",
      "Real samples Training loss: 0.5118427276611328\n",
      "Fake samples Training loss: 1.2896772623062134\n",
      "Real samples Training loss: 0.459305077791214\n",
      "Fake samples Training loss: 1.2148436307907104\n",
      "Real samples Training loss: 0.4953917860984802\n",
      "Fake samples Training loss: 1.3120436668395996\n",
      "Real samples Training loss: 0.495012491941452\n",
      "Fake samples Training loss: 1.140890121459961\n",
      "Real samples Training loss: 0.4308052659034729\n",
      "Fake samples Training loss: 1.0551128387451172\n",
      "Real samples Training loss: 0.42453470826148987\n",
      "Fake samples Training loss: 1.2863798141479492\n",
      "Real samples Training loss: 0.5207347273826599\n",
      "Fake samples Training loss: 1.117518424987793\n",
      "Real samples Training loss: 0.5561187863349915\n",
      "Fake samples Training loss: 1.3560850620269775\n",
      "Real samples Training loss: 0.44785600900650024\n",
      "Fake samples Training loss: 1.2984615564346313\n",
      "Real samples Training loss: 0.4715617299079895\n",
      "Fake samples Training loss: 1.2371466159820557\n",
      "Real samples Training loss: 0.4258253276348114\n",
      "Fake samples Training loss: 1.3000506162643433\n",
      "Real samples Training loss: 0.47138604521751404\n",
      "Fake samples Training loss: 1.2299084663391113\n",
      "Real samples Training loss: 0.49729934334754944\n",
      "Fake samples Training loss: 1.274431586265564\n",
      "Real samples Training loss: 0.45968562364578247\n",
      "Fake samples Training loss: 1.3626444339752197\n",
      "Real samples Training loss: 0.4968762993812561\n",
      "Fake samples Training loss: 1.320833444595337\n",
      "Real samples Training loss: 0.5544721484184265\n",
      "Fake samples Training loss: 1.1948117017745972\n",
      "Real samples Training loss: 0.4197761118412018\n",
      "Fake samples Training loss: 1.2533999681472778\n",
      "Real samples Training loss: 0.5001428723335266\n",
      "Fake samples Training loss: 1.0954080820083618\n",
      "Real samples Training loss: 0.5063177347183228\n",
      "Fake samples Training loss: 1.2837514877319336\n",
      "Real samples Training loss: 0.5513613820075989\n",
      "Fake samples Training loss: 1.2671869993209839\n",
      "Real samples Training loss: 0.44120046496391296\n",
      "Fake samples Training loss: 1.2389874458312988\n",
      "Real samples Training loss: 0.5229999423027039\n",
      "Fake samples Training loss: 1.3313210010528564\n",
      "Real samples Training loss: 0.49778813123703003\n",
      "Fake samples Training loss: 1.256494402885437\n",
      "Real samples Training loss: 0.5334091186523438\n",
      "Fake samples Training loss: 1.3256644010543823\n",
      "Real samples Training loss: 0.44654178619384766\n",
      "Fake samples Training loss: 1.2924877405166626\n",
      "Real samples Training loss: 0.4818868935108185\n",
      "Fake samples Training loss: 1.2776408195495605\n",
      "Real samples Training loss: 0.5085673332214355\n",
      "Fake samples Training loss: 1.1755129098892212\n",
      "Real samples Training loss: 0.4407651126384735\n",
      "Fake samples Training loss: 1.374701738357544\n",
      "Real samples Training loss: 0.4902421832084656\n",
      "Fake samples Training loss: 1.3732688426971436\n",
      "Real samples Training loss: 0.4832753837108612\n",
      "Fake samples Training loss: 1.1381337642669678\n",
      "Real samples Training loss: 0.4530636668205261\n",
      "Fake samples Training loss: 1.2959518432617188\n",
      "Real samples Training loss: 0.4701226055622101\n",
      "Fake samples Training loss: 1.2917919158935547\n",
      "Real samples Training loss: 0.477023720741272\n",
      "Fake samples Training loss: 1.2233080863952637\n",
      "Real samples Training loss: 0.4872448742389679\n",
      "Fake samples Training loss: 1.3398510217666626\n",
      "Real samples Training loss: 0.4879142940044403\n",
      "Fake samples Training loss: 1.1834837198257446\n",
      "Real samples Training loss: 0.4861895442008972\n",
      "Fake samples Training loss: 1.197229266166687\n",
      "Real samples Training loss: 0.5120102763175964\n",
      "Fake samples Training loss: 1.2482789754867554\n",
      "Real samples Training loss: 0.4889433681964874\n",
      "Fake samples Training loss: 1.321325421333313\n",
      "Real samples Training loss: 0.44538092613220215\n",
      "Fake samples Training loss: 1.2829562425613403\n",
      "Real samples Training loss: 0.4872063994407654\n",
      "Fake samples Training loss: 1.111641526222229\n",
      "Real samples Training loss: 0.4789920747280121\n",
      "Fake samples Training loss: 1.2113059759140015\n",
      "Real samples Training loss: 0.5256898403167725\n",
      "Fake samples Training loss: 1.1328858137130737\n",
      "Real samples Training loss: 0.45026302337646484\n",
      "Fake samples Training loss: 1.1889139413833618\n",
      "Real samples Training loss: 0.44288313388824463\n",
      "Fake samples Training loss: 1.347978115081787\n",
      "Real samples Training loss: 0.467070072889328\n",
      "Fake samples Training loss: 1.3405531644821167\n",
      "Real samples Training loss: 0.5009152293205261\n",
      "Fake samples Training loss: 1.331298589706421\n",
      "Real samples Training loss: 0.4152286946773529\n",
      "Fake samples Training loss: 1.23108971118927\n",
      "Real samples Training loss: 0.4568549394607544\n",
      "Fake samples Training loss: 1.19802725315094\n",
      "Real samples Training loss: 0.46288567781448364\n",
      "Fake samples Training loss: 1.250063419342041\n",
      "Real samples Training loss: 0.4821493923664093\n",
      "Fake samples Training loss: 1.2413766384124756\n",
      "Real samples Training loss: 0.5276696681976318\n",
      "Fake samples Training loss: 1.2300010919570923\n",
      "Real samples Training loss: 0.5243529677391052\n",
      "Fake samples Training loss: 1.127005934715271\n",
      "Real samples Training loss: 0.4553642272949219\n",
      "Fake samples Training loss: 1.2304264307022095\n",
      "Real samples Training loss: 0.5168156623840332\n",
      "Fake samples Training loss: 1.171445608139038\n",
      "Real samples Training loss: 0.48727238178253174\n",
      "Fake samples Training loss: 1.0785143375396729\n",
      "Real samples Training loss: 0.4533195495605469\n",
      "Fake samples Training loss: 1.261806845664978\n",
      "Real samples Training loss: 0.4638412594795227\n",
      "Fake samples Training loss: 1.2874586582183838\n",
      "Real samples Training loss: 0.5078691840171814\n",
      "Fake samples Training loss: 1.2971529960632324\n",
      "Real samples Training loss: 0.43350154161453247\n",
      "Fake samples Training loss: 1.2759274244308472\n",
      "Real samples Training loss: 0.5010617971420288\n",
      "Fake samples Training loss: 1.3111389875411987\n",
      "Real samples Training loss: 0.49350252747535706\n",
      "Fake samples Training loss: 1.3063231706619263\n",
      "Real samples Training loss: 0.42444881796836853\n",
      "Fake samples Training loss: 1.1952548027038574\n",
      "Real samples Training loss: 0.48041480779647827\n",
      "Fake samples Training loss: 1.1432901620864868\n",
      "Real samples Training loss: 0.4913996160030365\n",
      "Fake samples Training loss: 1.4006941318511963\n",
      "Real samples Training loss: 0.463096559047699\n",
      "Fake samples Training loss: 1.2203205823898315\n",
      "Real samples Training loss: 0.5224232077598572\n",
      "Fake samples Training loss: 1.2423824071884155\n",
      "Real samples Training loss: 0.49021583795547485\n",
      "Fake samples Training loss: 1.1976186037063599\n",
      "Real samples Training loss: 0.4576271176338196\n",
      "Fake samples Training loss: 1.3060113191604614\n",
      "Real samples Training loss: 0.4898969531059265\n",
      "Fake samples Training loss: 1.2240815162658691\n",
      "Real samples Training loss: 0.5231190919876099\n",
      "Fake samples Training loss: 1.1450347900390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.44388213753700256\n",
      "Fake samples Training loss: 1.3728642463684082\n",
      "Real samples Training loss: 0.41726699471473694\n",
      "Fake samples Training loss: 1.2781851291656494\n",
      "Real samples Training loss: 0.4738675057888031\n",
      "Fake samples Training loss: 1.324444055557251\n",
      "Real samples Training loss: 0.4689196050167084\n",
      "Fake samples Training loss: 1.2562659978866577\n",
      "Real samples Training loss: 0.5508899092674255\n",
      "Fake samples Training loss: 1.1989325284957886\n",
      "Real samples Training loss: 0.5112167596817017\n",
      "Fake samples Training loss: 1.2527469396591187\n",
      "Real samples Training loss: 0.4845787584781647\n",
      "Fake samples Training loss: 1.1421329975128174\n",
      "Real samples Training loss: 0.5000032782554626\n",
      "Fake samples Training loss: 1.2388046979904175\n",
      "Real samples Training loss: 0.5572108030319214\n",
      "Fake samples Training loss: 1.3806313276290894\n",
      "Real samples Training loss: 0.4426211714744568\n",
      "Fake samples Training loss: 1.0796878337860107\n",
      "Real samples Training loss: 0.47106310725212097\n",
      "Fake samples Training loss: 1.1887352466583252\n",
      "Real samples Training loss: 0.49614667892456055\n",
      "Fake samples Training loss: 1.1144051551818848\n",
      "Real samples Training loss: 0.42808419466018677\n",
      "Fake samples Training loss: 1.2315871715545654\n",
      "Real samples Training loss: 0.5140904784202576\n",
      "Fake samples Training loss: 1.286656141281128\n",
      "Real samples Training loss: 0.46200013160705566\n",
      "Fake samples Training loss: 1.214160442352295\n",
      "Real samples Training loss: 0.5460618138313293\n",
      "Fake samples Training loss: 1.1818077564239502\n",
      "Real samples Training loss: 0.4285533130168915\n",
      "Fake samples Training loss: 1.2889480590820312\n",
      "Real samples Training loss: 0.4641122817993164\n",
      "Fake samples Training loss: 1.1818153858184814\n",
      "Real samples Training loss: 0.46570733189582825\n",
      "Fake samples Training loss: 1.2321707010269165\n",
      "Real samples Training loss: 0.5023393034934998\n",
      "Fake samples Training loss: 1.3287535905838013\n",
      "Real samples Training loss: 0.49156978726387024\n",
      "Fake samples Training loss: 1.2708954811096191\n",
      "Real samples Training loss: 0.5539132356643677\n",
      "Fake samples Training loss: 1.2987091541290283\n",
      "Real samples Training loss: 0.5004720687866211\n",
      "Fake samples Training loss: 1.3773996829986572\n",
      "Real samples Training loss: 0.4326854348182678\n",
      "Fake samples Training loss: 1.241848111152649\n",
      "Real samples Training loss: 0.43123140931129456\n",
      "Fake samples Training loss: 1.1705938577651978\n",
      "Real samples Training loss: 0.5057384967803955\n",
      "Fake samples Training loss: 1.2640522718429565\n",
      "Real samples Training loss: 0.5180928707122803\n",
      "Fake samples Training loss: 1.172132968902588\n",
      "Real samples Training loss: 0.43998128175735474\n",
      "Fake samples Training loss: 1.1749529838562012\n",
      "Real samples Training loss: 0.5116342902183533\n",
      "Fake samples Training loss: 1.1833055019378662\n",
      "Real samples Training loss: 0.445113867521286\n",
      "Fake samples Training loss: 1.2926989793777466\n",
      "Real samples Training loss: 0.5108139514923096\n",
      "Fake samples Training loss: 1.1572731733322144\n",
      "Real samples Training loss: 0.4926290512084961\n",
      "Fake samples Training loss: 1.273406982421875\n",
      "Real samples Training loss: 0.44521087408065796\n",
      "Fake samples Training loss: 1.3100377321243286\n",
      "Real samples Training loss: 0.4876592457294464\n",
      "Fake samples Training loss: 1.0191975831985474\n",
      "Real samples Training loss: 0.5103025436401367\n",
      "Fake samples Training loss: 1.243865966796875\n",
      "Real samples Training loss: 0.4979332983493805\n",
      "Fake samples Training loss: 1.2314733266830444\n",
      "Real samples Training loss: 0.4548725485801697\n",
      "Fake samples Training loss: 1.1989338397979736\n",
      "Real samples Training loss: 0.48027747869491577\n",
      "Fake samples Training loss: 1.289611577987671\n",
      "Real samples Training loss: 0.4758091866970062\n",
      "Fake samples Training loss: 1.3752375841140747\n",
      "Real samples Training loss: 0.4561050236225128\n",
      "Fake samples Training loss: 1.2450613975524902\n",
      "Real samples Training loss: 0.4958902597427368\n",
      "Fake samples Training loss: 1.2544411420822144\n",
      "Real samples Training loss: 0.5464202761650085\n",
      "Fake samples Training loss: 1.2222901582717896\n",
      "Real samples Training loss: 0.42733103036880493\n",
      "Fake samples Training loss: 1.2183691263198853\n",
      "Real samples Training loss: 0.44936999678611755\n",
      "Fake samples Training loss: 1.1720649003982544\n",
      "Real samples Training loss: 0.43228626251220703\n",
      "Fake samples Training loss: 1.2652161121368408\n",
      "Real samples Training loss: 0.4873286485671997\n",
      "Fake samples Training loss: 1.2487387657165527\n",
      "Real samples Training loss: 0.5050476789474487\n",
      "Fake samples Training loss: 1.2496403455734253\n",
      "Real samples Training loss: 0.39235642552375793\n",
      "Fake samples Training loss: 1.3080780506134033\n",
      "Real samples Training loss: 0.48216623067855835\n",
      "Fake samples Training loss: 1.3770859241485596\n",
      "Real samples Training loss: 0.5084821581840515\n",
      "Fake samples Training loss: 1.2799570560455322\n",
      "Real samples Training loss: 0.46514594554901123\n",
      "Fake samples Training loss: 1.1209251880645752\n",
      "Real samples Training loss: 0.4499095380306244\n",
      "Fake samples Training loss: 1.208547830581665\n",
      "Real samples Training loss: 0.5161237120628357\n",
      "Fake samples Training loss: 1.1521930694580078\n",
      "Real samples Training loss: 0.45120272040367126\n",
      "Fake samples Training loss: 1.3422318696975708\n",
      "Real samples Training loss: 0.5252300500869751\n",
      "Fake samples Training loss: 1.1125811338424683\n",
      "Real samples Training loss: 0.5162837505340576\n",
      "Fake samples Training loss: 1.2333623170852661\n",
      "Real samples Training loss: 0.45584729313850403\n",
      "Fake samples Training loss: 1.2795941829681396\n",
      "Real samples Training loss: 0.5132436752319336\n",
      "Fake samples Training loss: 1.2303714752197266\n",
      "Real samples Training loss: 0.45166024565696716\n",
      "Fake samples Training loss: 1.2383736371994019\n",
      "Real samples Training loss: 0.4674096405506134\n",
      "Fake samples Training loss: 1.192752480506897\n",
      "Real samples Training loss: 0.4323646128177643\n",
      "Fake samples Training loss: 1.1232495307922363\n",
      "Real samples Training loss: 0.49369609355926514\n",
      "Fake samples Training loss: 1.2221251726150513\n",
      "Real samples Training loss: 0.4496356248855591\n",
      "Fake samples Training loss: 1.21548593044281\n",
      "Real samples Training loss: 0.5082347393035889\n",
      "Fake samples Training loss: 1.2867481708526611\n",
      "Real samples Training loss: 0.48268231749534607\n",
      "Fake samples Training loss: 1.2931678295135498\n",
      "Real samples Training loss: 0.502557098865509\n",
      "Fake samples Training loss: 1.284876823425293\n",
      "Real samples Training loss: 0.4574067294597626\n",
      "Fake samples Training loss: 1.2497652769088745\n",
      "Real samples Training loss: 0.500063419342041\n",
      "Fake samples Training loss: 1.1278438568115234\n",
      "Real samples Training loss: 0.503610372543335\n",
      "Fake samples Training loss: 1.3413610458374023\n",
      "Real samples Training loss: 0.459924578666687\n",
      "Fake samples Training loss: 1.334874153137207\n",
      "Real samples Training loss: 0.5078917145729065\n",
      "Fake samples Training loss: 1.225277066230774\n",
      "Real samples Training loss: 0.4661194384098053\n",
      "Fake samples Training loss: 1.2628439664840698\n",
      "Real samples Training loss: 0.4949689209461212\n",
      "Fake samples Training loss: 1.2015774250030518\n",
      "Real samples Training loss: 0.4733094274997711\n",
      "Fake samples Training loss: 1.2342206239700317\n",
      "Real samples Training loss: 0.4894358813762665\n",
      "Fake samples Training loss: 1.193097472190857\n",
      "Real samples Training loss: 0.5016516447067261\n",
      "Fake samples Training loss: 1.3481930494308472\n",
      "Real samples Training loss: 0.46017885208129883\n",
      "Fake samples Training loss: 1.262254238128662\n",
      "Real samples Training loss: 0.5137550234794617\n",
      "Fake samples Training loss: 1.1681329011917114\n",
      "Real samples Training loss: 0.5016427040100098\n",
      "Fake samples Training loss: 1.2875922918319702\n",
      "Real samples Training loss: 0.5108553171157837\n",
      "Fake samples Training loss: 1.1275653839111328\n",
      "Real samples Training loss: 0.4542054533958435\n",
      "Fake samples Training loss: 1.3715425729751587\n",
      "Real samples Training loss: 0.5122627019882202\n",
      "Fake samples Training loss: 1.1196789741516113\n",
      "Real samples Training loss: 0.46332642436027527\n",
      "Fake samples Training loss: 1.256544828414917\n",
      "Real samples Training loss: 0.4628268778324127\n",
      "Fake samples Training loss: 1.1385164260864258\n",
      "Real samples Training loss: 0.4972422420978546\n",
      "Fake samples Training loss: 1.3494526147842407\n",
      "Real samples Training loss: 0.5042433142662048\n",
      "Fake samples Training loss: 1.1669623851776123\n",
      "Real samples Training loss: 0.5423001050949097\n",
      "Fake samples Training loss: 1.2787718772888184\n",
      "Real samples Training loss: 0.41971448063850403\n",
      "Fake samples Training loss: 1.3642898797988892\n",
      "Real samples Training loss: 0.4869670271873474\n",
      "Fake samples Training loss: 1.20737886428833\n",
      "Real samples Training loss: 0.44102272391319275\n",
      "Fake samples Training loss: 1.2377593517303467\n",
      "Real samples Training loss: 0.49209117889404297\n",
      "Fake samples Training loss: 1.2943525314331055\n",
      "Real samples Training loss: 0.4911859333515167\n",
      "Fake samples Training loss: 1.1285287141799927\n",
      "Real samples Training loss: 0.5249601602554321\n",
      "Fake samples Training loss: 1.1624175310134888\n",
      "Real samples Training loss: 0.48771339654922485\n",
      "Fake samples Training loss: 1.2521347999572754\n",
      "Real samples Training loss: 0.39585739374160767\n",
      "Fake samples Training loss: 1.302839994430542\n",
      "Real samples Training loss: 0.4259473979473114\n",
      "Fake samples Training loss: 1.164726734161377\n",
      "Real samples Training loss: 0.47458910942077637\n",
      "Fake samples Training loss: 1.2760543823242188\n",
      "Real samples Training loss: 0.5627831220626831\n",
      "Fake samples Training loss: 1.2804113626480103\n",
      "Real samples Training loss: 0.42963534593582153\n",
      "Fake samples Training loss: 1.2472949028015137\n",
      "Real samples Training loss: 0.4355294108390808\n",
      "Fake samples Training loss: 1.2139396667480469\n",
      "Real samples Training loss: 0.4492886960506439\n",
      "Fake samples Training loss: 1.1791229248046875\n",
      "Real samples Training loss: 0.5113792419433594\n",
      "Fake samples Training loss: 1.3034181594848633\n",
      "Real samples Training loss: 0.5475068092346191\n",
      "Fake samples Training loss: 1.3272316455841064\n",
      "Real samples Training loss: 0.49704185128211975\n",
      "Fake samples Training loss: 1.2340983152389526\n",
      "Real samples Training loss: 0.493684321641922\n",
      "Fake samples Training loss: 1.1714235544204712\n",
      "Real samples Training loss: 0.5326429009437561\n",
      "Fake samples Training loss: 1.3626859188079834\n",
      "Real samples Training loss: 0.5132842063903809\n",
      "Fake samples Training loss: 1.2024816274642944\n",
      "Real samples Training loss: 0.511954128742218\n",
      "Fake samples Training loss: 1.2483996152877808\n",
      "Real samples Training loss: 0.5218629837036133\n",
      "Fake samples Training loss: 1.1969120502471924\n",
      "Real samples Training loss: 0.4606204330921173\n",
      "Fake samples Training loss: 1.1389044523239136\n",
      "Real samples Training loss: 0.4812726676464081\n",
      "Fake samples Training loss: 1.1932138204574585\n",
      "Real samples Training loss: 0.5022083520889282\n",
      "Fake samples Training loss: 1.1811094284057617\n",
      "Real samples Training loss: 0.43375861644744873\n",
      "Fake samples Training loss: 1.2811754941940308\n",
      "Real samples Training loss: 0.42666932940483093\n",
      "Fake samples Training loss: 1.2371493577957153\n",
      "Real samples Training loss: 0.46300771832466125\n",
      "Fake samples Training loss: 1.1919604539871216\n",
      "Real samples Training loss: 0.47387439012527466\n",
      "Fake samples Training loss: 1.0838961601257324\n",
      "Real samples Training loss: 0.5225862860679626\n",
      "Fake samples Training loss: 1.1818859577178955\n",
      "Real samples Training loss: 0.45105478167533875\n",
      "Fake samples Training loss: 1.1853636503219604\n",
      "Real samples Training loss: 0.4933311641216278\n",
      "Fake samples Training loss: 1.2636892795562744\n",
      "Real samples Training loss: 0.5131897330284119\n",
      "Fake samples Training loss: 1.2391135692596436\n",
      "Real samples Training loss: 0.49088093638420105\n",
      "Fake samples Training loss: 1.2219196557998657\n",
      "Real samples Training loss: 0.47882843017578125\n",
      "Fake samples Training loss: 1.1483230590820312\n",
      "Real samples Training loss: 0.3964230716228485\n",
      "Fake samples Training loss: 1.1641266345977783\n",
      "Real samples Training loss: 0.48663559556007385\n",
      "Fake samples Training loss: 1.3174678087234497\n",
      "Real samples Training loss: 0.4117014408111572\n",
      "Fake samples Training loss: 1.176454782485962\n",
      "Real samples Training loss: 0.5233630537986755\n",
      "Fake samples Training loss: 1.1108863353729248\n",
      "Real samples Training loss: 0.5251157283782959\n",
      "Fake samples Training loss: 1.3358991146087646\n",
      "Real samples Training loss: 0.4636078178882599\n",
      "Fake samples Training loss: 1.3872886896133423\n",
      "Real samples Training loss: 0.4756546914577484\n",
      "Fake samples Training loss: 1.2366901636123657\n",
      "Real samples Training loss: 0.474710077047348\n",
      "Fake samples Training loss: 1.0697834491729736\n",
      "Real samples Training loss: 0.5068901777267456\n",
      "Fake samples Training loss: 1.1906381845474243\n",
      "Real samples Training loss: 0.5179879069328308\n",
      "Fake samples Training loss: 1.2695356607437134\n",
      "Real samples Training loss: 0.48460623621940613\n",
      "Fake samples Training loss: 1.0276154279708862\n",
      "Real samples Training loss: 0.5066990256309509\n",
      "Fake samples Training loss: 1.1693212985992432\n",
      "Real samples Training loss: 0.41927817463874817\n",
      "Fake samples Training loss: 1.365315318107605\n",
      "Real samples Training loss: 0.4575223922729492\n",
      "Fake samples Training loss: 1.165310025215149\n",
      "Real samples Training loss: 0.4805329442024231\n",
      "Fake samples Training loss: 1.3830485343933105\n",
      "Real samples Training loss: 0.481161892414093\n",
      "Fake samples Training loss: 1.3780754804611206\n",
      "Real samples Training loss: 0.522818922996521\n",
      "Fake samples Training loss: 1.2993748188018799\n",
      "Real samples Training loss: 0.4478859603404999\n",
      "Fake samples Training loss: 1.299811840057373\n",
      "Real samples Training loss: 0.5126261115074158\n",
      "Fake samples Training loss: 1.2353192567825317\n",
      "Real samples Training loss: 0.4832194745540619\n",
      "Fake samples Training loss: 1.3762599229812622\n",
      "Real samples Training loss: 0.45099446177482605\n",
      "Fake samples Training loss: 1.281277060508728\n",
      "Real samples Training loss: 0.5593096017837524\n",
      "Fake samples Training loss: 1.2084112167358398\n",
      "Real samples Training loss: 0.44153979420661926\n",
      "Fake samples Training loss: 1.3362343311309814\n",
      "Real samples Training loss: 0.48959800601005554\n",
      "Fake samples Training loss: 1.3537131547927856\n",
      "Real samples Training loss: 0.47675758600234985\n",
      "Fake samples Training loss: 1.0963293313980103\n",
      "Real samples Training loss: 0.4908328950405121\n",
      "Fake samples Training loss: 1.1627596616744995\n",
      "Real samples Training loss: 0.41447290778160095\n",
      "Fake samples Training loss: 1.1735572814941406\n",
      "Real samples Training loss: 0.48298901319503784\n",
      "Fake samples Training loss: 1.0910872220993042\n",
      "Real samples Training loss: 0.49526676535606384\n",
      "Fake samples Training loss: 1.1584588289260864\n",
      "Real samples Training loss: 0.39527347683906555\n",
      "Fake samples Training loss: 1.2755467891693115\n",
      "Real samples Training loss: 0.4824364185333252\n",
      "Fake samples Training loss: 1.3201137781143188\n",
      "Real samples Training loss: 0.5127063989639282\n",
      "Fake samples Training loss: 1.0809437036514282\n",
      "Real samples Training loss: 0.5038431882858276\n",
      "Fake samples Training loss: 1.208861231803894\n",
      "Real samples Training loss: 0.4765186011791229\n",
      "Fake samples Training loss: 1.223364233970642\n",
      "Real samples Training loss: 0.434920996427536\n",
      "Fake samples Training loss: 1.2347687482833862\n",
      "Real samples Training loss: 0.5677966475486755\n",
      "Fake samples Training loss: 1.151187777519226\n",
      "Real samples Training loss: 0.4708220064640045\n",
      "Fake samples Training loss: 1.3741010427474976\n",
      "Real samples Training loss: 0.46452316641807556\n",
      "Fake samples Training loss: 1.2947155237197876\n",
      "Real samples Training loss: 0.43180978298187256\n",
      "Fake samples Training loss: 1.1517317295074463\n",
      "Real samples Training loss: 0.49154162406921387\n",
      "Fake samples Training loss: 1.169608473777771\n",
      "Real samples Training loss: 0.4941951632499695\n",
      "Fake samples Training loss: 1.2444095611572266\n",
      "Real samples Training loss: 0.44467270374298096\n",
      "Fake samples Training loss: 1.291542410850525\n",
      "Real samples Training loss: 0.4742639660835266\n",
      "Fake samples Training loss: 1.231498122215271\n",
      "Real samples Training loss: 0.48982810974121094\n",
      "Fake samples Training loss: 1.2176388502120972\n",
      "Real samples Training loss: 0.4894053637981415\n",
      "Fake samples Training loss: 1.1313565969467163\n",
      "Real samples Training loss: 0.4359395205974579\n",
      "Fake samples Training loss: 1.2328590154647827\n",
      "Real samples Training loss: 0.5106614232063293\n",
      "Fake samples Training loss: 1.2764580249786377\n",
      "Real samples Training loss: 0.5120019316673279\n",
      "Fake samples Training loss: 1.0853573083877563\n",
      "Real samples Training loss: 0.4994000196456909\n",
      "Fake samples Training loss: 1.2349168062210083\n",
      "Real samples Training loss: 0.5071789622306824\n",
      "Fake samples Training loss: 1.0484914779663086\n",
      "Real samples Training loss: 0.45100852847099304\n",
      "Fake samples Training loss: 1.3059231042861938\n",
      "Real samples Training loss: 0.4144851565361023\n",
      "Fake samples Training loss: 1.2398669719696045\n",
      "Real samples Training loss: 0.4553447365760803\n",
      "Fake samples Training loss: 1.28805410861969\n",
      "Real samples Training loss: 0.502742350101471\n",
      "Fake samples Training loss: 1.1975294351577759\n",
      "Real samples Training loss: 0.474959135055542\n",
      "Fake samples Training loss: 1.2747737169265747\n",
      "Real samples Training loss: 0.5123623609542847\n",
      "Fake samples Training loss: 1.2896896600723267\n",
      "Real samples Training loss: 0.4757201671600342\n",
      "Fake samples Training loss: 1.2994017601013184\n",
      "Real samples Training loss: 0.5599496364593506\n",
      "Fake samples Training loss: 1.4497767686843872\n",
      "Real samples Training loss: 0.5067380666732788\n",
      "Fake samples Training loss: 1.4101526737213135\n",
      "Real samples Training loss: 0.4686141312122345\n",
      "Fake samples Training loss: 1.1444281339645386\n",
      "Real samples Training loss: 0.4711839258670807\n",
      "Fake samples Training loss: 1.358525276184082\n",
      "Real samples Training loss: 0.42764294147491455\n",
      "Fake samples Training loss: 1.4399693012237549\n",
      "Real samples Training loss: 0.4732802212238312\n",
      "Fake samples Training loss: 1.160084843635559\n",
      "Real samples Training loss: 0.4228360950946808\n",
      "Fake samples Training loss: 1.1480257511138916\n",
      "Real samples Training loss: 0.4848431646823883\n",
      "Fake samples Training loss: 1.0965689420700073\n",
      "Real samples Training loss: 0.5227813124656677\n",
      "Fake samples Training loss: 1.1872397661209106\n",
      "Real samples Training loss: 0.4757158160209656\n",
      "Fake samples Training loss: 1.232439398765564\n",
      "Real samples Training loss: 0.5341213941574097\n",
      "Fake samples Training loss: 1.266083836555481\n",
      "Real samples Training loss: 0.49246272444725037\n",
      "Fake samples Training loss: 1.2449126243591309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.4394242465496063\n",
      "Fake samples Training loss: 1.197134017944336\n",
      "Real samples Training loss: 0.48554369807243347\n",
      "Fake samples Training loss: 1.2338180541992188\n",
      "Real samples Training loss: 0.48290789127349854\n",
      "Fake samples Training loss: 1.2937018871307373\n",
      "Real samples Training loss: 0.5477156043052673\n",
      "Fake samples Training loss: 1.3605079650878906\n",
      "Real samples Training loss: 0.45119351148605347\n",
      "Fake samples Training loss: 1.2071284055709839\n",
      "Real samples Training loss: 0.48487696051597595\n",
      "Fake samples Training loss: 1.120561957359314\n",
      "Real samples Training loss: 0.4533323347568512\n",
      "Fake samples Training loss: 1.1358752250671387\n",
      "Real samples Training loss: 0.5011807680130005\n",
      "Fake samples Training loss: 1.2180891036987305\n",
      "Real samples Training loss: 0.418171226978302\n",
      "Fake samples Training loss: 1.309892177581787\n",
      "Real samples Training loss: 0.5182958841323853\n",
      "Fake samples Training loss: 1.3030768632888794\n",
      "Real samples Training loss: 0.5115092992782593\n",
      "Fake samples Training loss: 1.3507440090179443\n",
      "Real samples Training loss: 0.44313672184944153\n",
      "Fake samples Training loss: 1.3757965564727783\n",
      "Real samples Training loss: 0.4932516813278198\n",
      "Fake samples Training loss: 1.3729482889175415\n",
      "Real samples Training loss: 0.42087721824645996\n",
      "Fake samples Training loss: 1.145139455795288\n",
      "Real samples Training loss: 0.47008016705513\n",
      "Fake samples Training loss: 1.1765611171722412\n",
      "Real samples Training loss: 0.511066198348999\n",
      "Fake samples Training loss: 1.0630353689193726\n",
      "Real samples Training loss: 0.47112753987312317\n",
      "Fake samples Training loss: 0.933781087398529\n",
      "Real samples Training loss: 0.5426422953605652\n",
      "Fake samples Training loss: 1.1181751489639282\n",
      "Real samples Training loss: 0.5099658966064453\n",
      "Fake samples Training loss: 1.3148119449615479\n",
      "Real samples Training loss: 0.44144222140312195\n",
      "Fake samples Training loss: 1.0827258825302124\n",
      "Real samples Training loss: 0.536422073841095\n",
      "Fake samples Training loss: 1.1527698040008545\n",
      "Real samples Training loss: 0.4714481234550476\n",
      "Fake samples Training loss: 1.0659558773040771\n",
      "Real samples Training loss: 0.4369827210903168\n",
      "Fake samples Training loss: 1.321600317955017\n",
      "Real samples Training loss: 0.5654438138008118\n",
      "Fake samples Training loss: 1.2790234088897705\n",
      "Real samples Training loss: 0.45799094438552856\n",
      "Fake samples Training loss: 1.3243277072906494\n",
      "Real samples Training loss: 0.459208607673645\n",
      "Fake samples Training loss: 1.2572935819625854\n",
      "Real samples Training loss: 0.5108759999275208\n",
      "Fake samples Training loss: 1.2470741271972656\n",
      "Real samples Training loss: 0.5066304206848145\n",
      "Fake samples Training loss: 1.2448631525039673\n",
      "Real samples Training loss: 0.5281763672828674\n",
      "Fake samples Training loss: 1.140354871749878\n",
      "Real samples Training loss: 0.4883559048175812\n",
      "Fake samples Training loss: 1.2998546361923218\n",
      "Real samples Training loss: 0.49150025844573975\n",
      "Fake samples Training loss: 1.1496810913085938\n",
      "Real samples Training loss: 0.5316619277000427\n",
      "Fake samples Training loss: 1.2710928916931152\n",
      "Real samples Training loss: 0.466480016708374\n",
      "Fake samples Training loss: 1.1203030347824097\n",
      "Real samples Training loss: 0.5209769606590271\n",
      "Fake samples Training loss: 1.1822335720062256\n",
      "Real samples Training loss: 0.4651413857936859\n",
      "Fake samples Training loss: 1.1392215490341187\n",
      "Real samples Training loss: 0.4950607717037201\n",
      "Fake samples Training loss: 1.2418099641799927\n",
      "Real samples Training loss: 0.4891885221004486\n",
      "Fake samples Training loss: 1.2461276054382324\n",
      "Real samples Training loss: 0.4836213290691376\n",
      "Fake samples Training loss: 1.2667738199234009\n",
      "Real samples Training loss: 0.4587702453136444\n",
      "Fake samples Training loss: 1.2717859745025635\n",
      "Real samples Training loss: 0.5074246525764465\n",
      "Fake samples Training loss: 1.2673100233078003\n",
      "Real samples Training loss: 0.5002385973930359\n",
      "Fake samples Training loss: 1.2210482358932495\n",
      "Real samples Training loss: 0.498864084482193\n",
      "Fake samples Training loss: 1.117384672164917\n",
      "Real samples Training loss: 0.5431341528892517\n",
      "Fake samples Training loss: 1.0816866159439087\n",
      "Real samples Training loss: 0.48361480236053467\n",
      "Fake samples Training loss: 1.2786428928375244\n",
      "Real samples Training loss: 0.5121625065803528\n",
      "Fake samples Training loss: 1.2599151134490967\n",
      "Real samples Training loss: 0.5222859978675842\n",
      "Fake samples Training loss: 1.3191637992858887\n",
      "Real samples Training loss: 0.45813414454460144\n",
      "Fake samples Training loss: 1.1662652492523193\n",
      "Real samples Training loss: 0.45392730832099915\n",
      "Fake samples Training loss: 1.3299459218978882\n",
      "Real samples Training loss: 0.5311746597290039\n",
      "Fake samples Training loss: 1.3463879823684692\n",
      "Real samples Training loss: 0.49177512526512146\n",
      "Fake samples Training loss: 1.051271915435791\n",
      "Real samples Training loss: 0.4752545654773712\n",
      "Fake samples Training loss: 1.1466165781021118\n",
      "Real samples Training loss: 0.42703598737716675\n",
      "Fake samples Training loss: 1.2235262393951416\n",
      "Real samples Training loss: 0.4446773827075958\n",
      "Fake samples Training loss: 1.1930480003356934\n",
      "Real samples Training loss: 0.4777586758136749\n",
      "Fake samples Training loss: 1.0780045986175537\n",
      "Real samples Training loss: 0.4924818277359009\n",
      "Fake samples Training loss: 1.1405037641525269\n",
      "Real samples Training loss: 0.5051407814025879\n",
      "Fake samples Training loss: 1.1202869415283203\n",
      "Real samples Training loss: 0.4821905493736267\n",
      "Fake samples Training loss: 1.2874133586883545\n",
      "Real samples Training loss: 0.5289022922515869\n",
      "Fake samples Training loss: 1.1337441205978394\n",
      "Real samples Training loss: 0.4749607741832733\n",
      "Fake samples Training loss: 1.1498981714248657\n",
      "Real samples Training loss: 0.5225662589073181\n",
      "Fake samples Training loss: 1.2563751935958862\n",
      "Real samples Training loss: 0.49535149335861206\n",
      "Fake samples Training loss: 1.1341065168380737\n",
      "Real samples Training loss: 0.4812556803226471\n",
      "Fake samples Training loss: 1.2631357908248901\n",
      "Real samples Training loss: 0.48284128308296204\n",
      "Fake samples Training loss: 1.208260178565979\n",
      "Real samples Training loss: 0.5327905416488647\n",
      "Fake samples Training loss: 1.2312965393066406\n",
      "Real samples Training loss: 0.43238621950149536\n",
      "Fake samples Training loss: 1.2059245109558105\n",
      "Real samples Training loss: 0.4620591700077057\n",
      "Fake samples Training loss: 1.2087420225143433\n",
      "Real samples Training loss: 0.4177399277687073\n",
      "Fake samples Training loss: 1.1644102334976196\n",
      "Real samples Training loss: 0.47433531284332275\n",
      "Fake samples Training loss: 1.3266756534576416\n",
      "Real samples Training loss: 0.4976939857006073\n",
      "Fake samples Training loss: 1.1371396780014038\n",
      "Real samples Training loss: 0.5029372572898865\n",
      "Fake samples Training loss: 1.087517499923706\n",
      "Real samples Training loss: 0.4558256268501282\n",
      "Fake samples Training loss: 1.1836049556732178\n",
      "Real samples Training loss: 0.4563089609146118\n",
      "Fake samples Training loss: 1.344946026802063\n",
      "Real samples Training loss: 0.43756598234176636\n",
      "Fake samples Training loss: 1.2815847396850586\n",
      "Real samples Training loss: 0.5054530501365662\n",
      "Fake samples Training loss: 1.2107292413711548\n",
      "Real samples Training loss: 0.44009533524513245\n",
      "Fake samples Training loss: 1.1303588151931763\n",
      "Real samples Training loss: 0.4835871756076813\n",
      "Fake samples Training loss: 1.2579602003097534\n",
      "Real samples Training loss: 0.530273973941803\n",
      "Fake samples Training loss: 1.2842730283737183\n",
      "Real samples Training loss: 0.5169169902801514\n",
      "Fake samples Training loss: 1.2431098222732544\n",
      "Real samples Training loss: 0.4860840141773224\n",
      "Fake samples Training loss: 1.2754759788513184\n",
      "Real samples Training loss: 0.4694180190563202\n",
      "Fake samples Training loss: 1.214766502380371\n",
      "Real samples Training loss: 0.5523332953453064\n",
      "Fake samples Training loss: 1.2487192153930664\n",
      "Real samples Training loss: 0.5332005620002747\n",
      "Fake samples Training loss: 1.1703083515167236\n",
      "Real samples Training loss: 0.5177608132362366\n",
      "Fake samples Training loss: 1.1455371379852295\n",
      "Real samples Training loss: 0.5500602126121521\n",
      "Fake samples Training loss: 1.1472848653793335\n",
      "Real samples Training loss: 0.5045850276947021\n",
      "Fake samples Training loss: 1.258694052696228\n",
      "Real samples Training loss: 0.5477302670478821\n",
      "Fake samples Training loss: 1.3096768856048584\n",
      "Real samples Training loss: 0.46160000562667847\n",
      "Fake samples Training loss: 1.3951385021209717\n",
      "Real samples Training loss: 0.5524754524230957\n",
      "Fake samples Training loss: 1.1566953659057617\n",
      "Real samples Training loss: 0.4972301125526428\n",
      "Fake samples Training loss: 1.2954597473144531\n",
      "Real samples Training loss: 0.5524787306785583\n",
      "Fake samples Training loss: 1.219287633895874\n",
      "Real samples Training loss: 0.47807422280311584\n",
      "Fake samples Training loss: 1.1785190105438232\n",
      "Real samples Training loss: 0.4816189110279083\n",
      "Fake samples Training loss: 1.2230496406555176\n",
      "Real samples Training loss: 0.5341066718101501\n",
      "Fake samples Training loss: 1.316771149635315\n",
      "Real samples Training loss: 0.5004544854164124\n",
      "Fake samples Training loss: 1.3064035177230835\n",
      "Real samples Training loss: 0.49986186623573303\n",
      "Fake samples Training loss: 1.2188785076141357\n",
      "Real samples Training loss: 0.49092915654182434\n",
      "Fake samples Training loss: 1.2579026222229004\n",
      "Real samples Training loss: 0.43902337551116943\n",
      "Fake samples Training loss: 1.3016427755355835\n",
      "Real samples Training loss: 0.4861302971839905\n",
      "Fake samples Training loss: 1.1317203044891357\n",
      "Real samples Training loss: 0.48917385935783386\n",
      "Fake samples Training loss: 1.425245761871338\n",
      "Real samples Training loss: 0.45482319593429565\n",
      "Fake samples Training loss: 1.3468199968338013\n",
      "Real samples Training loss: 0.5739328265190125\n",
      "Fake samples Training loss: 1.1822831630706787\n",
      "Real samples Training loss: 0.4992540776729584\n",
      "Fake samples Training loss: 1.23591148853302\n",
      "Real samples Training loss: 0.4410124719142914\n",
      "Fake samples Training loss: 1.2917780876159668\n",
      "Real samples Training loss: 0.48666203022003174\n",
      "Fake samples Training loss: 1.2324845790863037\n",
      "Real samples Training loss: 0.48964181542396545\n",
      "Fake samples Training loss: 1.32920241355896\n",
      "Real samples Training loss: 0.46180829405784607\n",
      "Fake samples Training loss: 1.1706069707870483\n",
      "Real samples Training loss: 0.43131786584854126\n",
      "Fake samples Training loss: 1.2122185230255127\n",
      "Real samples Training loss: 0.4978393316268921\n",
      "Fake samples Training loss: 1.1450706720352173\n",
      "Real samples Training loss: 0.5657291412353516\n",
      "Fake samples Training loss: 1.2849761247634888\n",
      "Real samples Training loss: 0.4058305323123932\n",
      "Fake samples Training loss: 1.2983955144882202\n",
      "Real samples Training loss: 0.47902458906173706\n",
      "Fake samples Training loss: 1.2095814943313599\n",
      "Real samples Training loss: 0.48962876200675964\n",
      "Fake samples Training loss: 1.1409854888916016\n",
      "Real samples Training loss: 0.4503645598888397\n",
      "Fake samples Training loss: 1.2563337087631226\n",
      "Real samples Training loss: 0.4860665798187256\n",
      "Fake samples Training loss: 1.196950912475586\n",
      "Real samples Training loss: 0.49948713183403015\n",
      "Fake samples Training loss: 1.451223611831665\n",
      "Real samples Training loss: 0.47174474596977234\n",
      "Fake samples Training loss: 1.0875641107559204\n",
      "Real samples Training loss: 0.5027033090591431\n",
      "Fake samples Training loss: 1.2362937927246094\n",
      "Real samples Training loss: 0.4820151925086975\n",
      "Fake samples Training loss: 1.3913376331329346\n",
      "Real samples Training loss: 0.44365671277046204\n",
      "Fake samples Training loss: 1.211530089378357\n",
      "Real samples Training loss: 0.5359438061714172\n",
      "Fake samples Training loss: 1.0974394083023071\n",
      "Real samples Training loss: 0.47077077627182007\n",
      "Fake samples Training loss: 1.3317408561706543\n",
      "Real samples Training loss: 0.5126444697380066\n",
      "Fake samples Training loss: 1.0542622804641724\n",
      "Real samples Training loss: 0.5154327750205994\n",
      "Fake samples Training loss: 1.1495851278305054\n",
      "Real samples Training loss: 0.4495028257369995\n",
      "Fake samples Training loss: 1.2666369676589966\n",
      "Real samples Training loss: 0.45951777696609497\n",
      "Fake samples Training loss: 1.307417392730713\n",
      "Real samples Training loss: 0.46164581179618835\n",
      "Fake samples Training loss: 1.1053361892700195\n",
      "Real samples Training loss: 0.44621676206588745\n",
      "Fake samples Training loss: 1.2567650079727173\n",
      "Real samples Training loss: 0.4921562671661377\n",
      "Fake samples Training loss: 1.111664056777954\n",
      "Real samples Training loss: 0.4825994670391083\n",
      "Fake samples Training loss: 1.3024921417236328\n",
      "Real samples Training loss: 0.4476338028907776\n",
      "Fake samples Training loss: 1.161142110824585\n",
      "Real samples Training loss: 0.49435919523239136\n",
      "Fake samples Training loss: 1.3119100332260132\n",
      "Real samples Training loss: 0.4485321342945099\n",
      "Fake samples Training loss: 1.1736737489700317\n",
      "Real samples Training loss: 0.45750394463539124\n",
      "Fake samples Training loss: 1.4186166524887085\n",
      "Real samples Training loss: 0.5089974403381348\n",
      "Fake samples Training loss: 1.2095000743865967\n",
      "Real samples Training loss: 0.4793909192085266\n",
      "Fake samples Training loss: 1.3662272691726685\n",
      "Real samples Training loss: 0.5183338522911072\n",
      "Fake samples Training loss: 1.2201051712036133\n",
      "Real samples Training loss: 0.4744761288166046\n",
      "Fake samples Training loss: 1.4584351778030396\n",
      "Real samples Training loss: 0.4946780204772949\n",
      "Fake samples Training loss: 1.2488421201705933\n",
      "Real samples Training loss: 0.4707263708114624\n",
      "Fake samples Training loss: 1.1210185289382935\n",
      "Real samples Training loss: 0.5045731663703918\n",
      "Fake samples Training loss: 1.2895041704177856\n",
      "Real samples Training loss: 0.40335801243782043\n",
      "Fake samples Training loss: 1.167966365814209\n",
      "Real samples Training loss: 0.4865350127220154\n",
      "Fake samples Training loss: 1.2907527685165405\n",
      "Real samples Training loss: 0.5132639408111572\n",
      "Fake samples Training loss: 1.0303024053573608\n",
      "Real samples Training loss: 0.41246554255485535\n",
      "Fake samples Training loss: 1.3760384321212769\n",
      "Real samples Training loss: 0.5095534324645996\n",
      "Fake samples Training loss: 1.3098493814468384\n",
      "Real samples Training loss: 0.5280001759529114\n",
      "Fake samples Training loss: 1.2261061668395996\n",
      "Real samples Training loss: 0.4481469392776489\n",
      "Fake samples Training loss: 1.2875851392745972\n",
      "Real samples Training loss: 0.4910360872745514\n",
      "Fake samples Training loss: 1.3138810396194458\n",
      "Real samples Training loss: 0.48720046877861023\n",
      "Fake samples Training loss: 1.3455345630645752\n",
      "Real samples Training loss: 0.4815910756587982\n",
      "Fake samples Training loss: 1.1830703020095825\n",
      "Real samples Training loss: 0.4995056986808777\n",
      "Fake samples Training loss: 1.195910096168518\n",
      "Real samples Training loss: 0.5588211417198181\n",
      "Fake samples Training loss: 1.141866683959961\n",
      "Real samples Training loss: 0.4573994278907776\n",
      "Fake samples Training loss: 1.2006529569625854\n",
      "Real samples Training loss: 0.534151554107666\n",
      "Fake samples Training loss: 1.2387893199920654\n",
      "Real samples Training loss: 0.4650532603263855\n",
      "Fake samples Training loss: 1.2650829553604126\n",
      "Real samples Training loss: 0.4747993052005768\n",
      "Fake samples Training loss: 1.3511468172073364\n",
      "Real samples Training loss: 0.41572093963623047\n",
      "Fake samples Training loss: 1.2657684087753296\n",
      "Real samples Training loss: 0.48191624879837036\n",
      "Fake samples Training loss: 1.1574593782424927\n",
      "Real samples Training loss: 0.5087937116622925\n",
      "Fake samples Training loss: 1.3479455709457397\n",
      "Real samples Training loss: 0.5118211507797241\n",
      "Fake samples Training loss: 1.2694092988967896\n",
      "Real samples Training loss: 0.4586485028266907\n",
      "Fake samples Training loss: 1.1679621934890747\n",
      "Real samples Training loss: 0.5006007552146912\n",
      "Fake samples Training loss: 1.4804606437683105\n",
      "Real samples Training loss: 0.4705430269241333\n",
      "Fake samples Training loss: 1.4126856327056885\n",
      "Real samples Training loss: 0.5123252272605896\n",
      "Fake samples Training loss: 1.2751182317733765\n",
      "Real samples Training loss: 0.4682610332965851\n",
      "Fake samples Training loss: 1.0862606763839722\n",
      "Real samples Training loss: 0.42220133543014526\n",
      "Fake samples Training loss: 0.984613835811615\n",
      "Real samples Training loss: 0.4727669954299927\n",
      "Fake samples Training loss: 1.3821464776992798\n",
      "Real samples Training loss: 0.41687339544296265\n",
      "Fake samples Training loss: 1.1539716720581055\n",
      "Real samples Training loss: 0.40768393874168396\n",
      "Fake samples Training loss: 1.1778615713119507\n",
      "Real samples Training loss: 0.4913098216056824\n",
      "Fake samples Training loss: 1.1752533912658691\n",
      "Real samples Training loss: 0.4920075535774231\n",
      "Fake samples Training loss: 1.2482054233551025\n",
      "Real samples Training loss: 0.47721627354621887\n",
      "Fake samples Training loss: 1.230163812637329\n",
      "Real samples Training loss: 0.5374202132225037\n",
      "Fake samples Training loss: 1.1147092580795288\n",
      "Real samples Training loss: 0.4544009566307068\n",
      "Fake samples Training loss: 1.2227566242218018\n",
      "Real samples Training loss: 0.4913395345211029\n",
      "Fake samples Training loss: 1.1363366842269897\n",
      "Real samples Training loss: 0.4185737669467926\n",
      "Fake samples Training loss: 1.360841989517212\n",
      "Real samples Training loss: 0.4208008348941803\n",
      "Fake samples Training loss: 1.1142877340316772\n",
      "Real samples Training loss: 0.48200637102127075\n",
      "Fake samples Training loss: 1.2055377960205078\n",
      "Real samples Training loss: 0.4829918444156647\n",
      "Fake samples Training loss: 1.1947386264801025\n",
      "Real samples Training loss: 0.4564518630504608\n",
      "Fake samples Training loss: 1.1806339025497437\n",
      "Real samples Training loss: 0.4774264693260193\n",
      "Fake samples Training loss: 1.1745121479034424\n",
      "Real samples Training loss: 0.45653975009918213\n",
      "Fake samples Training loss: 1.3385945558547974\n",
      "Real samples Training loss: 0.51775723695755\n",
      "Fake samples Training loss: 1.3718820810317993\n",
      "Real samples Training loss: 0.5440167188644409\n",
      "Fake samples Training loss: 1.1362793445587158\n",
      "Real samples Training loss: 0.4231904447078705\n",
      "Fake samples Training loss: 1.1550512313842773\n",
      "Real samples Training loss: 0.4974484145641327\n",
      "Fake samples Training loss: 1.224398136138916\n",
      "Real samples Training loss: 0.4832337200641632\n",
      "Fake samples Training loss: 1.1553711891174316\n",
      "Real samples Training loss: 0.46828189492225647\n",
      "Fake samples Training loss: 1.2211824655532837\n",
      "Real samples Training loss: 0.49419471621513367\n",
      "Fake samples Training loss: 1.3019375801086426\n",
      "Real samples Training loss: 0.4666370451450348\n",
      "Fake samples Training loss: 1.3644466400146484\n",
      "Real samples Training loss: 0.47503143548965454\n",
      "Fake samples Training loss: 1.4286398887634277\n",
      "Real samples Training loss: 0.46088603138923645\n",
      "Fake samples Training loss: 1.3065448999404907\n",
      "Real samples Training loss: 0.44168832898139954\n",
      "Fake samples Training loss: 1.2380828857421875\n",
      "Real samples Training loss: 0.44547557830810547\n",
      "Fake samples Training loss: 1.321058988571167\n",
      "Real samples Training loss: 0.45069941878318787\n",
      "Fake samples Training loss: 1.38645339012146\n",
      "Real samples Training loss: 0.4876747727394104\n",
      "Fake samples Training loss: 1.2955492734909058\n",
      "Real samples Training loss: 0.5022392272949219\n",
      "Fake samples Training loss: 1.3175830841064453\n",
      "Real samples Training loss: 0.39882418513298035\n",
      "Fake samples Training loss: 1.3326038122177124\n",
      "Real samples Training loss: 0.4627912640571594\n",
      "Fake samples Training loss: 1.2450875043869019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.48866844177246094\n",
      "Fake samples Training loss: 1.1885137557983398\n",
      "Real samples Training loss: 0.4432678818702698\n",
      "Fake samples Training loss: 1.2470284700393677\n",
      "Real samples Training loss: 0.48796823620796204\n",
      "Fake samples Training loss: 1.2047443389892578\n",
      "Real samples Training loss: 0.4847966134548187\n",
      "Fake samples Training loss: 1.296773076057434\n",
      "Real samples Training loss: 0.4573221802711487\n",
      "Fake samples Training loss: 1.2384283542633057\n",
      "Real samples Training loss: 0.49927210807800293\n",
      "Fake samples Training loss: 1.36741042137146\n",
      "Real samples Training loss: 0.4402545988559723\n",
      "Fake samples Training loss: 1.1464837789535522\n",
      "Real samples Training loss: 0.5428867936134338\n",
      "Fake samples Training loss: 1.2385475635528564\n",
      "Real samples Training loss: 0.48604872822761536\n",
      "Fake samples Training loss: 1.3171658515930176\n",
      "Real samples Training loss: 0.4566912055015564\n",
      "Fake samples Training loss: 1.3312000036239624\n",
      "Real samples Training loss: 0.47643929719924927\n",
      "Fake samples Training loss: 1.1898113489151\n",
      "Real samples Training loss: 0.5213847160339355\n",
      "Fake samples Training loss: 1.2702083587646484\n",
      "Real samples Training loss: 0.45679500699043274\n",
      "Fake samples Training loss: 1.275938868522644\n",
      "Real samples Training loss: 0.5401391983032227\n",
      "Fake samples Training loss: 1.2460464239120483\n",
      "Real samples Training loss: 0.4590153694152832\n",
      "Fake samples Training loss: 1.2036322355270386\n",
      "Real samples Training loss: 0.47080615162849426\n",
      "Fake samples Training loss: 1.3273423910140991\n",
      "Real samples Training loss: 0.4751291573047638\n",
      "Fake samples Training loss: 1.3647505044937134\n",
      "Real samples Training loss: 0.5167357325553894\n",
      "Fake samples Training loss: 1.1402909755706787\n",
      "Real samples Training loss: 0.49048441648483276\n",
      "Fake samples Training loss: 1.2888071537017822\n",
      "Real samples Training loss: 0.5569111108779907\n",
      "Fake samples Training loss: 1.2849693298339844\n",
      "Real samples Training loss: 0.5244190096855164\n",
      "Fake samples Training loss: 1.1360722780227661\n",
      "Real samples Training loss: 0.4936911165714264\n",
      "Fake samples Training loss: 1.1356791257858276\n",
      "Real samples Training loss: 0.46573516726493835\n",
      "Fake samples Training loss: 1.2239785194396973\n",
      "Real samples Training loss: 0.4225974977016449\n",
      "Fake samples Training loss: 1.3209275007247925\n",
      "Real samples Training loss: 0.5353086590766907\n",
      "Fake samples Training loss: 1.2240129709243774\n",
      "Real samples Training loss: 0.4985311031341553\n",
      "Fake samples Training loss: 1.2036851644515991\n",
      "Real samples Training loss: 0.4333382844924927\n",
      "Fake samples Training loss: 1.3635598421096802\n",
      "Real samples Training loss: 0.4508679211139679\n",
      "Fake samples Training loss: 1.2388858795166016\n",
      "Real samples Training loss: 0.5045542120933533\n",
      "Fake samples Training loss: 1.25030517578125\n",
      "Real samples Training loss: 0.4786282181739807\n",
      "Fake samples Training loss: 1.2212425470352173\n",
      "Real samples Training loss: 0.44502755999565125\n",
      "Fake samples Training loss: 1.2066434621810913\n",
      "Real samples Training loss: 0.45695140957832336\n",
      "Fake samples Training loss: 1.1766630411148071\n",
      "Real samples Training loss: 0.4671591520309448\n",
      "Fake samples Training loss: 1.0959800481796265\n",
      "Real samples Training loss: 0.4885195195674896\n",
      "Fake samples Training loss: 1.259013056755066\n",
      "Real samples Training loss: 0.4315948784351349\n",
      "Fake samples Training loss: 1.3613706827163696\n",
      "Real samples Training loss: 0.5181188583374023\n",
      "Fake samples Training loss: 1.313354730606079\n",
      "Real samples Training loss: 0.5235788822174072\n",
      "Fake samples Training loss: 1.1857547760009766\n",
      "Real samples Training loss: 0.5192843675613403\n",
      "Fake samples Training loss: 1.3386586904525757\n",
      "Real samples Training loss: 0.4381299614906311\n",
      "Fake samples Training loss: 1.3135044574737549\n",
      "Real samples Training loss: 0.44474688172340393\n",
      "Fake samples Training loss: 1.1383702754974365\n",
      "Real samples Training loss: 0.4920932650566101\n",
      "Fake samples Training loss: 1.331431269645691\n",
      "Real samples Training loss: 0.4167327582836151\n",
      "Fake samples Training loss: 1.2080597877502441\n",
      "Real samples Training loss: 0.47990286350250244\n",
      "Fake samples Training loss: 1.3182342052459717\n",
      "Real samples Training loss: 0.5744549036026001\n",
      "Fake samples Training loss: 1.3720777034759521\n",
      "Real samples Training loss: 0.49587199091911316\n",
      "Fake samples Training loss: 1.2772387266159058\n",
      "Real samples Training loss: 0.5324756503105164\n",
      "Fake samples Training loss: 1.2647420167922974\n",
      "Real samples Training loss: 0.48577114939689636\n",
      "Fake samples Training loss: 1.3652923107147217\n",
      "Real samples Training loss: 0.5129488706588745\n",
      "Fake samples Training loss: 1.1625391244888306\n",
      "Real samples Training loss: 0.5013802647590637\n",
      "Fake samples Training loss: 1.3456065654754639\n",
      "Real samples Training loss: 0.5491392612457275\n",
      "Fake samples Training loss: 1.2404664754867554\n",
      "Real samples Training loss: 0.46116358041763306\n",
      "Fake samples Training loss: 1.2214033603668213\n",
      "Real samples Training loss: 0.5257415175437927\n",
      "Fake samples Training loss: 1.1189486980438232\n",
      "Real samples Training loss: 0.412552148103714\n",
      "Fake samples Training loss: 1.139907956123352\n",
      "Real samples Training loss: 0.4968579113483429\n",
      "Fake samples Training loss: 1.2566603422164917\n",
      "Real samples Training loss: 0.5400283336639404\n",
      "Fake samples Training loss: 1.190711259841919\n",
      "Real samples Training loss: 0.48841339349746704\n",
      "Fake samples Training loss: 1.3213412761688232\n",
      "Real samples Training loss: 0.5334811806678772\n",
      "Fake samples Training loss: 1.211788296699524\n",
      "Real samples Training loss: 0.5080497860908508\n",
      "Fake samples Training loss: 1.3283202648162842\n",
      "Real samples Training loss: 0.4859492778778076\n",
      "Fake samples Training loss: 1.2353122234344482\n",
      "Real samples Training loss: 0.44152599573135376\n",
      "Fake samples Training loss: 1.281463384628296\n",
      "Real samples Training loss: 0.4676683843135834\n",
      "Fake samples Training loss: 1.1257836818695068\n",
      "Real samples Training loss: 0.48340845108032227\n",
      "Fake samples Training loss: 1.2874811887741089\n",
      "Real samples Training loss: 0.44848597049713135\n",
      "Fake samples Training loss: 1.0985565185546875\n",
      "Real samples Training loss: 0.4925328493118286\n",
      "Fake samples Training loss: 1.1959149837493896\n",
      "Real samples Training loss: 0.4973967671394348\n",
      "Fake samples Training loss: 1.2569950819015503\n",
      "Real samples Training loss: 0.44382184743881226\n",
      "Fake samples Training loss: 1.135321021080017\n",
      "Real samples Training loss: 0.47888267040252686\n",
      "Fake samples Training loss: 1.227602243423462\n",
      "Real samples Training loss: 0.4922735095024109\n",
      "Fake samples Training loss: 1.333708643913269\n",
      "Real samples Training loss: 0.4697192311286926\n",
      "Fake samples Training loss: 1.2687740325927734\n",
      "Real samples Training loss: 0.44662967324256897\n",
      "Fake samples Training loss: 1.4104830026626587\n",
      "Real samples Training loss: 0.49643585085868835\n",
      "Fake samples Training loss: 1.270227074623108\n",
      "Real samples Training loss: 0.511107861995697\n",
      "Fake samples Training loss: 1.2339448928833008\n",
      "Real samples Training loss: 0.503512442111969\n",
      "Fake samples Training loss: 1.3617233037948608\n",
      "Real samples Training loss: 0.4939022958278656\n",
      "Fake samples Training loss: 1.2392842769622803\n",
      "Real samples Training loss: 0.4689841568470001\n",
      "Fake samples Training loss: 1.304418921470642\n",
      "Real samples Training loss: 0.5369560718536377\n",
      "Fake samples Training loss: 1.12373685836792\n",
      "Real samples Training loss: 0.483943909406662\n",
      "Fake samples Training loss: 1.2675461769104004\n",
      "Real samples Training loss: 0.45287850499153137\n",
      "Fake samples Training loss: 1.121671199798584\n",
      "Real samples Training loss: 0.5252432227134705\n",
      "Fake samples Training loss: 1.2479262351989746\n",
      "Real samples Training loss: 0.4397171437740326\n",
      "Fake samples Training loss: 1.149381160736084\n",
      "Real samples Training loss: 0.5304379463195801\n",
      "Fake samples Training loss: 1.1348748207092285\n",
      "Real samples Training loss: 0.5125311613082886\n",
      "Fake samples Training loss: 1.3624247312545776\n",
      "Real samples Training loss: 0.6001366376876831\n",
      "Fake samples Training loss: 1.2699769735336304\n",
      "Real samples Training loss: 0.5667811632156372\n",
      "Fake samples Training loss: 1.275159239768982\n",
      "Real samples Training loss: 0.568511426448822\n",
      "Fake samples Training loss: 1.177337884902954\n",
      "Real samples Training loss: 0.47557002305984497\n",
      "Fake samples Training loss: 1.3172187805175781\n",
      "Real samples Training loss: 0.4226224422454834\n",
      "Fake samples Training loss: 1.2667515277862549\n",
      "Real samples Training loss: 0.47121599316596985\n",
      "Fake samples Training loss: 1.141852855682373\n",
      "Real samples Training loss: 0.4917420744895935\n",
      "Fake samples Training loss: 1.102264404296875\n",
      "Real samples Training loss: 0.4131365120410919\n",
      "Fake samples Training loss: 1.3060981035232544\n",
      "Real samples Training loss: 0.5052428245544434\n",
      "Fake samples Training loss: 1.2474561929702759\n",
      "Real samples Training loss: 0.4797404706478119\n",
      "Fake samples Training loss: 1.2594050168991089\n",
      "Real samples Training loss: 0.4659619927406311\n",
      "Fake samples Training loss: 1.2370147705078125\n",
      "Real samples Training loss: 0.5192287564277649\n",
      "Fake samples Training loss: 1.2137107849121094\n",
      "Real samples Training loss: 0.49290698766708374\n",
      "Fake samples Training loss: 1.3991872072219849\n",
      "Real samples Training loss: 0.47634199261665344\n",
      "Fake samples Training loss: 1.2005194425582886\n",
      "Real samples Training loss: 0.45673295855522156\n",
      "Fake samples Training loss: 1.2485610246658325\n",
      "Real samples Training loss: 0.4805298447608948\n",
      "Fake samples Training loss: 1.095330834388733\n",
      "Real samples Training loss: 0.45768600702285767\n",
      "Fake samples Training loss: 1.215853214263916\n",
      "Real samples Training loss: 0.4978499412536621\n",
      "Fake samples Training loss: 1.3021519184112549\n",
      "Real samples Training loss: 0.4537946283817291\n",
      "Fake samples Training loss: 1.1838353872299194\n",
      "Real samples Training loss: 0.5144751667976379\n",
      "Fake samples Training loss: 1.2857846021652222\n",
      "Real samples Training loss: 0.5183882117271423\n",
      "Fake samples Training loss: 1.3319483995437622\n",
      "Real samples Training loss: 0.5373064279556274\n",
      "Fake samples Training loss: 1.152265191078186\n",
      "Real samples Training loss: 0.4776885509490967\n",
      "Fake samples Training loss: 1.3014001846313477\n",
      "Real samples Training loss: 0.5259010791778564\n",
      "Fake samples Training loss: 1.2744804620742798\n",
      "Real samples Training loss: 0.45683684945106506\n",
      "Fake samples Training loss: 1.2501287460327148\n",
      "Real samples Training loss: 0.48780810832977295\n",
      "Fake samples Training loss: 1.3795620203018188\n",
      "Real samples Training loss: 0.48924511671066284\n",
      "Fake samples Training loss: 1.1568853855133057\n",
      "Real samples Training loss: 0.5188064575195312\n",
      "Fake samples Training loss: 0.9633630514144897\n",
      "Real samples Training loss: 0.5385133624076843\n",
      "Fake samples Training loss: 1.1768343448638916\n",
      "Real samples Training loss: 0.4548974335193634\n",
      "Fake samples Training loss: 1.0746805667877197\n",
      "Real samples Training loss: 0.4906938076019287\n",
      "Fake samples Training loss: 1.2594560384750366\n",
      "Real samples Training loss: 0.4960874617099762\n",
      "Fake samples Training loss: 1.3619691133499146\n",
      "Real samples Training loss: 0.42681190371513367\n",
      "Fake samples Training loss: 1.3570196628570557\n",
      "Real samples Training loss: 0.4968915581703186\n",
      "Fake samples Training loss: 1.2652928829193115\n",
      "Real samples Training loss: 0.4334777891635895\n",
      "Fake samples Training loss: 1.2883180379867554\n",
      "Real samples Training loss: 0.44568341970443726\n",
      "Fake samples Training loss: 1.3499337434768677\n",
      "Real samples Training loss: 0.5140723586082458\n",
      "Fake samples Training loss: 1.2326236963272095\n",
      "Real samples Training loss: 0.49469414353370667\n",
      "Fake samples Training loss: 1.408422589302063\n",
      "Real samples Training loss: 0.5028994679450989\n",
      "Fake samples Training loss: 1.1687438488006592\n",
      "Real samples Training loss: 0.5833728909492493\n",
      "Fake samples Training loss: 1.1702749729156494\n",
      "Real samples Training loss: 0.469175785779953\n",
      "Fake samples Training loss: 1.4371501207351685\n",
      "Real samples Training loss: 0.5096298456192017\n",
      "Fake samples Training loss: 1.2279921770095825\n",
      "Real samples Training loss: 0.42479023337364197\n",
      "Fake samples Training loss: 1.0501779317855835\n",
      "Real samples Training loss: 0.48115161061286926\n",
      "Fake samples Training loss: 1.241722583770752\n",
      "Real samples Training loss: 0.4531298875808716\n",
      "Fake samples Training loss: 1.076554775238037\n",
      "Real samples Training loss: 0.452178955078125\n",
      "Fake samples Training loss: 1.2398356199264526\n",
      "Real samples Training loss: 0.4870567321777344\n",
      "Fake samples Training loss: 1.2027114629745483\n",
      "Real samples Training loss: 0.5000978708267212\n",
      "Fake samples Training loss: 1.3571243286132812\n",
      "Real samples Training loss: 0.4969539940357208\n",
      "Fake samples Training loss: 1.3392047882080078\n",
      "Real samples Training loss: 0.5004626512527466\n",
      "Fake samples Training loss: 1.3012820482254028\n",
      "Real samples Training loss: 0.5102706551551819\n",
      "Fake samples Training loss: 1.3472957611083984\n",
      "Real samples Training loss: 0.517179012298584\n",
      "Fake samples Training loss: 1.1487990617752075\n",
      "Real samples Training loss: 0.5658568143844604\n",
      "Fake samples Training loss: 1.148406982421875\n",
      "Real samples Training loss: 0.4513014853000641\n",
      "Fake samples Training loss: 1.216675043106079\n",
      "Real samples Training loss: 0.4436304271221161\n",
      "Fake samples Training loss: 1.183478593826294\n",
      "Real samples Training loss: 0.4657299816608429\n",
      "Fake samples Training loss: 1.250861644744873\n",
      "Real samples Training loss: 0.4953073263168335\n",
      "Fake samples Training loss: 1.163734793663025\n",
      "Real samples Training loss: 0.4580073654651642\n",
      "Fake samples Training loss: 1.3174161911010742\n",
      "Real samples Training loss: 0.396697998046875\n",
      "Fake samples Training loss: 1.207893967628479\n",
      "Real samples Training loss: 0.5584841370582581\n",
      "Fake samples Training loss: 1.2772456407546997\n",
      "Real samples Training loss: 0.47917014360427856\n",
      "Fake samples Training loss: 1.3741331100463867\n",
      "Real samples Training loss: 0.481929749250412\n",
      "Fake samples Training loss: 1.3055670261383057\n",
      "Real samples Training loss: 0.541817843914032\n",
      "Fake samples Training loss: 1.2438573837280273\n",
      "Real samples Training loss: 0.5405453443527222\n",
      "Fake samples Training loss: 1.25762939453125\n",
      "Real samples Training loss: 0.5172401070594788\n",
      "Fake samples Training loss: 1.2854371070861816\n",
      "Real samples Training loss: 0.5261537432670593\n",
      "Fake samples Training loss: 1.0794131755828857\n",
      "Real samples Training loss: 0.4977712035179138\n",
      "Fake samples Training loss: 1.1853623390197754\n",
      "Real samples Training loss: 0.46719738841056824\n",
      "Fake samples Training loss: 1.2577860355377197\n",
      "Real samples Training loss: 0.47869113087654114\n",
      "Fake samples Training loss: 1.3089532852172852\n",
      "Real samples Training loss: 0.46234768629074097\n",
      "Fake samples Training loss: 1.302574872970581\n",
      "Real samples Training loss: 0.5658148527145386\n",
      "Fake samples Training loss: 1.3081923723220825\n",
      "Real samples Training loss: 0.44133272767066956\n",
      "Fake samples Training loss: 1.3532346487045288\n",
      "Real samples Training loss: 0.41799309849739075\n",
      "Fake samples Training loss: 1.2204681634902954\n",
      "Real samples Training loss: 0.4781154990196228\n",
      "Fake samples Training loss: 1.2673673629760742\n",
      "Real samples Training loss: 0.4422158896923065\n",
      "Fake samples Training loss: 1.1474609375\n",
      "Real samples Training loss: 0.48748335242271423\n",
      "Fake samples Training loss: 1.1429861783981323\n",
      "Real samples Training loss: 0.4620593190193176\n",
      "Fake samples Training loss: 1.2670384645462036\n",
      "Real samples Training loss: 0.4914086163043976\n",
      "Fake samples Training loss: 1.2507685422897339\n",
      "Real samples Training loss: 0.45445486903190613\n",
      "Fake samples Training loss: 1.1651110649108887\n",
      "Real samples Training loss: 0.5134916305541992\n",
      "Fake samples Training loss: 1.2084910869598389\n",
      "Real samples Training loss: 0.38420793414115906\n",
      "Fake samples Training loss: 1.398154854774475\n",
      "Real samples Training loss: 0.5280680656433105\n",
      "Fake samples Training loss: 1.3419712781906128\n",
      "Real samples Training loss: 0.48176246881484985\n",
      "Fake samples Training loss: 1.3316627740859985\n",
      "Real samples Training loss: 0.539627194404602\n",
      "Fake samples Training loss: 1.1547369956970215\n",
      "Real samples Training loss: 0.5717563033103943\n",
      "Fake samples Training loss: 1.3368299007415771\n",
      "Real samples Training loss: 0.4282086491584778\n",
      "Fake samples Training loss: 1.167901873588562\n",
      "Real samples Training loss: 0.4835222065448761\n",
      "Fake samples Training loss: 1.226085901260376\n",
      "Real samples Training loss: 0.515670120716095\n",
      "Fake samples Training loss: 1.1861218214035034\n",
      "Real samples Training loss: 0.48339077830314636\n",
      "Fake samples Training loss: 1.2944386005401611\n",
      "Real samples Training loss: 0.47227758169174194\n",
      "Fake samples Training loss: 1.2180079221725464\n",
      "Real samples Training loss: 0.460843026638031\n",
      "Fake samples Training loss: 1.1703262329101562\n",
      "Real samples Training loss: 0.553115725517273\n",
      "Fake samples Training loss: 1.3255116939544678\n",
      "Real samples Training loss: 0.4966755509376526\n",
      "Fake samples Training loss: 1.1708977222442627\n",
      "Real samples Training loss: 0.48429617285728455\n",
      "Fake samples Training loss: 1.3307303190231323\n",
      "Real samples Training loss: 0.4617752730846405\n",
      "Fake samples Training loss: 1.227853536605835\n",
      "Real samples Training loss: 0.5118826627731323\n",
      "Fake samples Training loss: 1.3442769050598145\n",
      "Real samples Training loss: 0.45754164457321167\n",
      "Fake samples Training loss: 1.2037711143493652\n",
      "Real samples Training loss: 0.4969949424266815\n",
      "Fake samples Training loss: 1.3632149696350098\n",
      "Real samples Training loss: 0.4853403866291046\n",
      "Fake samples Training loss: 1.282355546951294\n",
      "Real samples Training loss: 0.5261411070823669\n",
      "Fake samples Training loss: 1.2118830680847168\n",
      "Real samples Training loss: 0.48727327585220337\n",
      "Fake samples Training loss: 1.1965394020080566\n",
      "Real samples Training loss: 0.4351545572280884\n",
      "Fake samples Training loss: 1.345566987991333\n",
      "Real samples Training loss: 0.48732179403305054\n",
      "Fake samples Training loss: 1.2954277992248535\n",
      "Real samples Training loss: 0.4815179407596588\n",
      "Fake samples Training loss: 1.0998778343200684\n",
      "Real samples Training loss: 0.5607247352600098\n",
      "Fake samples Training loss: 1.278290867805481\n",
      "Real samples Training loss: 0.458856999874115\n",
      "Fake samples Training loss: 1.155898928642273\n",
      "Real samples Training loss: 0.46620386838912964\n",
      "Fake samples Training loss: 1.2997952699661255\n",
      "Real samples Training loss: 0.46573173999786377\n",
      "Fake samples Training loss: 1.2854315042495728\n",
      "Real samples Training loss: 0.43627792596817017\n",
      "Fake samples Training loss: 1.3439723253250122\n",
      "Real samples Training loss: 0.5184871554374695\n",
      "Fake samples Training loss: 1.3288729190826416\n",
      "Real samples Training loss: 0.4864160716533661\n",
      "Fake samples Training loss: 1.2721011638641357\n",
      "Real samples Training loss: 0.4816184341907501\n",
      "Fake samples Training loss: 1.147428035736084\n",
      "Real samples Training loss: 0.5085901021957397\n",
      "Fake samples Training loss: 1.27481210231781\n",
      "Real samples Training loss: 0.4884766936302185\n",
      "Fake samples Training loss: 1.0456604957580566\n",
      "Real samples Training loss: 0.4416314661502838\n",
      "Fake samples Training loss: 1.3106576204299927\n",
      "Real samples Training loss: 0.5190746784210205\n",
      "Fake samples Training loss: 1.2392090559005737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.48375484347343445\n",
      "Fake samples Training loss: 1.2777926921844482\n",
      "Real samples Training loss: 0.48760277032852173\n",
      "Fake samples Training loss: 1.2449361085891724\n",
      "Real samples Training loss: 0.44094306230545044\n",
      "Fake samples Training loss: 1.1327190399169922\n",
      "Real samples Training loss: 0.4595966041088104\n",
      "Fake samples Training loss: 1.0651408433914185\n",
      "Real samples Training loss: 0.5699142813682556\n",
      "Fake samples Training loss: 1.2205259799957275\n",
      "Real samples Training loss: 0.47339344024658203\n",
      "Fake samples Training loss: 1.141525149345398\n",
      "Real samples Training loss: 0.4515455365180969\n",
      "Fake samples Training loss: 1.276634931564331\n",
      "Real samples Training loss: 0.44729968905448914\n",
      "Fake samples Training loss: 1.0499223470687866\n",
      "Real samples Training loss: 0.4692080616950989\n",
      "Fake samples Training loss: 1.3724358081817627\n",
      "Real samples Training loss: 0.5010554194450378\n",
      "Fake samples Training loss: 1.1272255182266235\n",
      "Real samples Training loss: 0.482797771692276\n",
      "Fake samples Training loss: 1.317918300628662\n",
      "Real samples Training loss: 0.46438267827033997\n",
      "Fake samples Training loss: 1.1855047941207886\n",
      "Real samples Training loss: 0.47475960850715637\n",
      "Fake samples Training loss: 1.2790729999542236\n",
      "Real samples Training loss: 0.48628509044647217\n",
      "Fake samples Training loss: 1.1538375616073608\n",
      "Real samples Training loss: 0.45788949728012085\n",
      "Fake samples Training loss: 1.1504416465759277\n",
      "Real samples Training loss: 0.5041397213935852\n",
      "Fake samples Training loss: 1.2492438554763794\n",
      "Real samples Training loss: 0.4802079498767853\n",
      "Fake samples Training loss: 1.3218448162078857\n",
      "Real samples Training loss: 0.44593361020088196\n",
      "Fake samples Training loss: 1.3233407735824585\n",
      "Real samples Training loss: 0.5073281526565552\n",
      "Fake samples Training loss: 1.3042290210723877\n",
      "Real samples Training loss: 0.4625588059425354\n",
      "Fake samples Training loss: 1.3613289594650269\n",
      "Real samples Training loss: 0.3979960083961487\n",
      "Fake samples Training loss: 1.3175740242004395\n",
      "Real samples Training loss: 0.45505303144454956\n",
      "Fake samples Training loss: 1.1376217603683472\n",
      "Real samples Training loss: 0.5120955109596252\n",
      "Fake samples Training loss: 1.2486425638198853\n",
      "Real samples Training loss: 0.5058351159095764\n",
      "Fake samples Training loss: 1.286279320716858\n",
      "Real samples Training loss: 0.532795786857605\n",
      "Fake samples Training loss: 1.3494104146957397\n",
      "Real samples Training loss: 0.4578014314174652\n",
      "Fake samples Training loss: 1.3784862756729126\n",
      "Real samples Training loss: 0.5090673565864563\n",
      "Fake samples Training loss: 1.2101870775222778\n",
      "Real samples Training loss: 0.5108076930046082\n",
      "Fake samples Training loss: 1.1135655641555786\n",
      "Real samples Training loss: 0.5162925124168396\n",
      "Fake samples Training loss: 1.3075647354125977\n",
      "Real samples Training loss: 0.4238016605377197\n",
      "Fake samples Training loss: 1.271203875541687\n",
      "Real samples Training loss: 0.5215462446212769\n",
      "Fake samples Training loss: 1.246556043624878\n",
      "Real samples Training loss: 0.5116066932678223\n",
      "Fake samples Training loss: 1.2790230512619019\n",
      "Real samples Training loss: 0.48878735303878784\n",
      "Fake samples Training loss: 1.3381478786468506\n",
      "Real samples Training loss: 0.4622882306575775\n",
      "Fake samples Training loss: 1.2508562803268433\n",
      "Real samples Training loss: 0.4340645968914032\n",
      "Fake samples Training loss: 1.2335621118545532\n",
      "Real samples Training loss: 0.42092472314834595\n",
      "Fake samples Training loss: 1.1685967445373535\n",
      "Real samples Training loss: 0.4639589786529541\n",
      "Fake samples Training loss: 1.157685399055481\n",
      "Real samples Training loss: 0.5047450661659241\n",
      "Fake samples Training loss: 1.1956133842468262\n",
      "Real samples Training loss: 0.4500046372413635\n",
      "Fake samples Training loss: 1.2175828218460083\n",
      "Real samples Training loss: 0.46582698822021484\n",
      "Fake samples Training loss: 1.2960994243621826\n",
      "Real samples Training loss: 0.5250277519226074\n",
      "Fake samples Training loss: 1.1764289140701294\n",
      "Real samples Training loss: 0.5118976831436157\n",
      "Fake samples Training loss: 1.2237114906311035\n",
      "Real samples Training loss: 0.5235716700553894\n",
      "Fake samples Training loss: 1.2813539505004883\n",
      "Real samples Training loss: 0.459110289812088\n",
      "Fake samples Training loss: 1.3493516445159912\n",
      "Real samples Training loss: 0.4498206675052643\n",
      "Fake samples Training loss: 1.1894625425338745\n",
      "Real samples Training loss: 0.5048230886459351\n",
      "Fake samples Training loss: 1.2337311506271362\n",
      "Real samples Training loss: 0.47497984766960144\n",
      "Fake samples Training loss: 1.1640534400939941\n",
      "Real samples Training loss: 0.5154977440834045\n",
      "Fake samples Training loss: 1.2254889011383057\n",
      "Real samples Training loss: 0.5051426887512207\n",
      "Fake samples Training loss: 1.308958888053894\n",
      "Real samples Training loss: 0.476952463388443\n",
      "Fake samples Training loss: 1.3797591924667358\n",
      "Real samples Training loss: 0.46673110127449036\n",
      "Fake samples Training loss: 1.3313497304916382\n",
      "Real samples Training loss: 0.44666820764541626\n",
      "Fake samples Training loss: 1.2016409635543823\n",
      "Real samples Training loss: 0.5261871218681335\n",
      "Fake samples Training loss: 1.3196872472763062\n",
      "Real samples Training loss: 0.4580115079879761\n",
      "Fake samples Training loss: 1.186449408531189\n",
      "Real samples Training loss: 0.5183027386665344\n",
      "Fake samples Training loss: 1.1479148864746094\n",
      "Real samples Training loss: 0.47781097888946533\n",
      "Fake samples Training loss: 1.4599583148956299\n",
      "Real samples Training loss: 0.49692440032958984\n",
      "Fake samples Training loss: 1.3222616910934448\n",
      "Real samples Training loss: 0.4333042502403259\n",
      "Fake samples Training loss: 1.2060002088546753\n",
      "Real samples Training loss: 0.468408465385437\n",
      "Fake samples Training loss: 1.3217577934265137\n",
      "Real samples Training loss: 0.47001633048057556\n",
      "Fake samples Training loss: 1.3195759057998657\n",
      "Real samples Training loss: 0.46649202704429626\n",
      "Fake samples Training loss: 1.190685749053955\n",
      "Real samples Training loss: 0.49692559242248535\n",
      "Fake samples Training loss: 1.185044765472412\n",
      "Real samples Training loss: 0.5079408884048462\n",
      "Fake samples Training loss: 1.37933349609375\n",
      "Real samples Training loss: 0.5144490599632263\n",
      "Fake samples Training loss: 1.2976164817810059\n",
      "Real samples Training loss: 0.48672759532928467\n",
      "Fake samples Training loss: 1.2908464670181274\n",
      "Real samples Training loss: 0.4862581193447113\n",
      "Fake samples Training loss: 1.1873483657836914\n",
      "Real samples Training loss: 0.4127402603626251\n",
      "Fake samples Training loss: 1.3716086149215698\n",
      "Real samples Training loss: 0.47908341884613037\n",
      "Fake samples Training loss: 1.2986551523208618\n",
      "Real samples Training loss: 0.41491174697875977\n",
      "Fake samples Training loss: 1.187832236289978\n",
      "Real samples Training loss: 0.5149227380752563\n",
      "Fake samples Training loss: 1.3932300806045532\n",
      "Real samples Training loss: 0.5204652547836304\n",
      "Fake samples Training loss: 1.2391090393066406\n",
      "Real samples Training loss: 0.48670297861099243\n",
      "Fake samples Training loss: 1.1603974103927612\n",
      "Real samples Training loss: 0.5152966976165771\n",
      "Fake samples Training loss: 1.2640962600708008\n",
      "Real samples Training loss: 0.5210564136505127\n",
      "Fake samples Training loss: 1.2717416286468506\n",
      "Real samples Training loss: 0.45985814929008484\n",
      "Fake samples Training loss: 1.2187929153442383\n",
      "Real samples Training loss: 0.46987512707710266\n",
      "Fake samples Training loss: 1.1766514778137207\n",
      "Real samples Training loss: 0.47759580612182617\n",
      "Fake samples Training loss: 1.2203636169433594\n",
      "Real samples Training loss: 0.5202827453613281\n",
      "Fake samples Training loss: 1.1084096431732178\n",
      "Real samples Training loss: 0.4987126588821411\n",
      "Fake samples Training loss: 1.2451444864273071\n",
      "Real samples Training loss: 0.4830749034881592\n",
      "Fake samples Training loss: 1.319291114807129\n",
      "Real samples Training loss: 0.4755551815032959\n",
      "Fake samples Training loss: 1.2617018222808838\n",
      "Real samples Training loss: 0.5379736423492432\n",
      "Fake samples Training loss: 1.2617971897125244\n",
      "Real samples Training loss: 0.48834964632987976\n",
      "Fake samples Training loss: 1.2424052953720093\n",
      "Real samples Training loss: 0.48401641845703125\n",
      "Fake samples Training loss: 1.1801519393920898\n",
      "Real samples Training loss: 0.45341014862060547\n",
      "Fake samples Training loss: 1.2770408391952515\n",
      "Real samples Training loss: 0.5022182464599609\n",
      "Fake samples Training loss: 1.2513315677642822\n",
      "Real samples Training loss: 0.4497021436691284\n",
      "Fake samples Training loss: 1.3557294607162476\n",
      "Real samples Training loss: 0.4776938259601593\n",
      "Fake samples Training loss: 1.4513170719146729\n",
      "Real samples Training loss: 0.4078534245491028\n",
      "Fake samples Training loss: 1.3572434186935425\n",
      "Real samples Training loss: 0.4803965091705322\n",
      "Fake samples Training loss: 1.3260265588760376\n",
      "Real samples Training loss: 0.49466410279273987\n",
      "Fake samples Training loss: 1.48612380027771\n",
      "Real samples Training loss: 0.4446280002593994\n",
      "Fake samples Training loss: 1.2385872602462769\n",
      "Real samples Training loss: 0.5078309178352356\n",
      "Fake samples Training loss: 1.1870875358581543\n",
      "Real samples Training loss: 0.44280388951301575\n",
      "Fake samples Training loss: 1.2315856218338013\n",
      "Real samples Training loss: 0.4687027335166931\n",
      "Fake samples Training loss: 1.2967946529388428\n",
      "Real samples Training loss: 0.44568419456481934\n",
      "Fake samples Training loss: 1.2402690649032593\n",
      "Real samples Training loss: 0.5357949733734131\n",
      "Fake samples Training loss: 1.2344856262207031\n",
      "Real samples Training loss: 0.49787428975105286\n",
      "Fake samples Training loss: 1.3689101934432983\n",
      "Real samples Training loss: 0.5049015879631042\n",
      "Fake samples Training loss: 1.2955424785614014\n",
      "Real samples Training loss: 0.49245235323905945\n",
      "Fake samples Training loss: 1.055100917816162\n",
      "Real samples Training loss: 0.43929362297058105\n",
      "Fake samples Training loss: 1.1244513988494873\n",
      "Real samples Training loss: 0.4288860261440277\n",
      "Fake samples Training loss: 1.2265973091125488\n",
      "Real samples Training loss: 0.4806381165981293\n",
      "Fake samples Training loss: 1.109596610069275\n",
      "Real samples Training loss: 0.4364803433418274\n",
      "Fake samples Training loss: 1.2004446983337402\n",
      "Real samples Training loss: 0.42799657583236694\n",
      "Fake samples Training loss: 1.2742230892181396\n",
      "Real samples Training loss: 0.508674144744873\n",
      "Fake samples Training loss: 1.2999130487442017\n",
      "Real samples Training loss: 0.4764631688594818\n",
      "Fake samples Training loss: 1.250049352645874\n",
      "Real samples Training loss: 0.46610891819000244\n",
      "Fake samples Training loss: 1.3228743076324463\n",
      "Real samples Training loss: 0.6138280630111694\n",
      "Fake samples Training loss: 1.3466730117797852\n",
      "Real samples Training loss: 0.4556768834590912\n",
      "Fake samples Training loss: 1.2747442722320557\n",
      "Real samples Training loss: 0.5334349870681763\n",
      "Fake samples Training loss: 1.1574883460998535\n",
      "Real samples Training loss: 0.48386678099632263\n",
      "Fake samples Training loss: 1.1932533979415894\n",
      "Real samples Training loss: 0.45872148871421814\n",
      "Fake samples Training loss: 1.3878421783447266\n",
      "Real samples Training loss: 0.5212293267250061\n",
      "Fake samples Training loss: 1.215959906578064\n",
      "Real samples Training loss: 0.457773357629776\n",
      "Fake samples Training loss: 1.2991174459457397\n",
      "Real samples Training loss: 0.48080095648765564\n",
      "Fake samples Training loss: 1.2976261377334595\n",
      "Real samples Training loss: 0.5212792158126831\n",
      "Fake samples Training loss: 1.224673867225647\n",
      "Real samples Training loss: 0.4711199700832367\n",
      "Fake samples Training loss: 1.2431823015213013\n",
      "Real samples Training loss: 0.5404748916625977\n",
      "Fake samples Training loss: 1.2795480489730835\n",
      "Real samples Training loss: 0.43529394268989563\n",
      "Fake samples Training loss: 1.2493720054626465\n",
      "Real samples Training loss: 0.4685114026069641\n",
      "Fake samples Training loss: 1.2276263236999512\n",
      "Real samples Training loss: 0.43179410696029663\n",
      "Fake samples Training loss: 1.1720737218856812\n",
      "Real samples Training loss: 0.43917009234428406\n",
      "Fake samples Training loss: 1.1175638437271118\n",
      "Real samples Training loss: 0.4771221876144409\n",
      "Fake samples Training loss: 1.2619794607162476\n",
      "Real samples Training loss: 0.5479877591133118\n",
      "Fake samples Training loss: 1.2150753736495972\n",
      "Real samples Training loss: 0.4482150375843048\n",
      "Fake samples Training loss: 1.24630868434906\n",
      "Real samples Training loss: 0.4693085551261902\n",
      "Fake samples Training loss: 1.2806671857833862\n",
      "Real samples Training loss: 0.48068496584892273\n",
      "Fake samples Training loss: 1.0655288696289062\n",
      "Real samples Training loss: 0.4624652564525604\n",
      "Fake samples Training loss: 1.233345627784729\n",
      "Real samples Training loss: 0.5148259997367859\n",
      "Fake samples Training loss: 1.2152752876281738\n",
      "Real samples Training loss: 0.4707379639148712\n",
      "Fake samples Training loss: 1.2136695384979248\n",
      "Real samples Training loss: 0.509698212146759\n",
      "Fake samples Training loss: 1.2821006774902344\n",
      "Real samples Training loss: 0.4911229908466339\n",
      "Fake samples Training loss: 1.1932077407836914\n",
      "Real samples Training loss: 0.45452427864074707\n",
      "Fake samples Training loss: 1.171278953552246\n",
      "Real samples Training loss: 0.44753992557525635\n",
      "Fake samples Training loss: 1.316402792930603\n",
      "Real samples Training loss: 0.5144097208976746\n",
      "Fake samples Training loss: 1.1570483446121216\n",
      "Real samples Training loss: 0.47704648971557617\n",
      "Fake samples Training loss: 1.2596900463104248\n",
      "Real samples Training loss: 0.525177001953125\n",
      "Fake samples Training loss: 1.379171371459961\n",
      "Real samples Training loss: 0.5201971530914307\n",
      "Fake samples Training loss: 1.1529148817062378\n",
      "Real samples Training loss: 0.4599013328552246\n",
      "Fake samples Training loss: 1.0485990047454834\n",
      "Real samples Training loss: 0.46446508169174194\n",
      "Fake samples Training loss: 1.1156702041625977\n",
      "Real samples Training loss: 0.5476166009902954\n",
      "Fake samples Training loss: 1.227542757987976\n",
      "Real samples Training loss: 0.5245298147201538\n",
      "Fake samples Training loss: 1.1055076122283936\n",
      "Real samples Training loss: 0.48866182565689087\n",
      "Fake samples Training loss: 1.0988565683364868\n",
      "Real samples Training loss: 0.47053062915802\n",
      "Fake samples Training loss: 1.27779221534729\n",
      "Real samples Training loss: 0.5165762901306152\n",
      "Fake samples Training loss: 1.1238842010498047\n",
      "Real samples Training loss: 0.46533530950546265\n",
      "Fake samples Training loss: 1.1347845792770386\n",
      "Real samples Training loss: 0.4629809558391571\n",
      "Fake samples Training loss: 1.38556706905365\n",
      "Real samples Training loss: 0.5189253687858582\n",
      "Fake samples Training loss: 1.23899245262146\n",
      "Real samples Training loss: 0.4900892972946167\n",
      "Fake samples Training loss: 1.1682405471801758\n",
      "Real samples Training loss: 0.48002395033836365\n",
      "Fake samples Training loss: 1.1126724481582642\n",
      "Real samples Training loss: 0.5104115605354309\n",
      "Fake samples Training loss: 1.3651052713394165\n",
      "Real samples Training loss: 0.42890769243240356\n",
      "Fake samples Training loss: 1.1528376340866089\n",
      "Real samples Training loss: 0.4798807203769684\n",
      "Fake samples Training loss: 1.304674744606018\n",
      "Real samples Training loss: 0.5191060304641724\n",
      "Fake samples Training loss: 1.1312055587768555\n",
      "Real samples Training loss: 0.486162394285202\n",
      "Fake samples Training loss: 1.2049124240875244\n",
      "Real samples Training loss: 0.49675506353378296\n",
      "Fake samples Training loss: 1.1350384950637817\n",
      "Real samples Training loss: 0.5527999401092529\n",
      "Fake samples Training loss: 1.2551156282424927\n",
      "Real samples Training loss: 0.5412279963493347\n",
      "Fake samples Training loss: 1.2284618616104126\n",
      "Real samples Training loss: 0.44238370656967163\n",
      "Fake samples Training loss: 1.3561934232711792\n",
      "Real samples Training loss: 0.49432989954948425\n",
      "Fake samples Training loss: 1.1619311571121216\n",
      "Real samples Training loss: 0.4452402889728546\n",
      "Fake samples Training loss: 1.2641507387161255\n",
      "Real samples Training loss: 0.4900057017803192\n",
      "Fake samples Training loss: 1.271989107131958\n",
      "Real samples Training loss: 0.4747430384159088\n",
      "Fake samples Training loss: 1.1184669733047485\n",
      "Real samples Training loss: 0.43961191177368164\n",
      "Fake samples Training loss: 1.3657969236373901\n",
      "Real samples Training loss: 0.49788355827331543\n",
      "Fake samples Training loss: 1.217044711112976\n",
      "Real samples Training loss: 0.44283097982406616\n",
      "Fake samples Training loss: 1.405896544456482\n",
      "Real samples Training loss: 0.509265661239624\n",
      "Fake samples Training loss: 1.2555651664733887\n",
      "Real samples Training loss: 0.4937804043292999\n",
      "Fake samples Training loss: 1.3063855171203613\n",
      "Real samples Training loss: 0.5530590415000916\n",
      "Fake samples Training loss: 1.2168033123016357\n",
      "Real samples Training loss: 0.4909540116786957\n",
      "Fake samples Training loss: 1.1941633224487305\n",
      "Real samples Training loss: 0.5074838995933533\n",
      "Fake samples Training loss: 1.3509238958358765\n",
      "Real samples Training loss: 0.4852166771888733\n",
      "Fake samples Training loss: 1.1582578420639038\n",
      "Real samples Training loss: 0.3976321220397949\n",
      "Fake samples Training loss: 1.2961621284484863\n",
      "Real samples Training loss: 0.5156944990158081\n",
      "Fake samples Training loss: 1.1284992694854736\n",
      "Real samples Training loss: 0.45148903131484985\n",
      "Fake samples Training loss: 1.2058827877044678\n",
      "Real samples Training loss: 0.5430271029472351\n",
      "Fake samples Training loss: 1.0600117444992065\n",
      "Real samples Training loss: 0.5311965942382812\n",
      "Fake samples Training loss: 1.3552298545837402\n",
      "Real samples Training loss: 0.5299834609031677\n",
      "Fake samples Training loss: 1.1842129230499268\n",
      "Real samples Training loss: 0.4885198473930359\n",
      "Fake samples Training loss: 1.149215579032898\n",
      "Real samples Training loss: 0.5087225437164307\n",
      "Fake samples Training loss: 1.2205408811569214\n",
      "Real samples Training loss: 0.5441094040870667\n",
      "Fake samples Training loss: 1.3317009210586548\n",
      "Real samples Training loss: 0.4295313060283661\n",
      "Fake samples Training loss: 1.1538598537445068\n",
      "Real samples Training loss: 0.4390221834182739\n",
      "Fake samples Training loss: 1.2012083530426025\n",
      "Real samples Training loss: 0.5581805109977722\n",
      "Fake samples Training loss: 1.2926744222640991\n",
      "Real samples Training loss: 0.5309909582138062\n",
      "Fake samples Training loss: 1.3161125183105469\n",
      "Real samples Training loss: 0.48164474964141846\n",
      "Fake samples Training loss: 1.3346836566925049\n",
      "Real samples Training loss: 0.543471097946167\n",
      "Fake samples Training loss: 1.3594186305999756\n",
      "Real samples Training loss: 0.4647630751132965\n",
      "Fake samples Training loss: 1.238666296005249\n",
      "Real samples Training loss: 0.48135024309158325\n",
      "Fake samples Training loss: 1.0665172338485718\n",
      "Real samples Training loss: 0.4527176320552826\n",
      "Fake samples Training loss: 1.2545037269592285\n",
      "Real samples Training loss: 0.44332748651504517\n",
      "Fake samples Training loss: 1.2371866703033447\n",
      "Real samples Training loss: 0.4676639437675476\n",
      "Fake samples Training loss: 1.2230876684188843\n",
      "Real samples Training loss: 0.4883641004562378\n",
      "Fake samples Training loss: 1.1780476570129395\n",
      "Real samples Training loss: 0.5414815545082092\n",
      "Fake samples Training loss: 1.2361305952072144\n",
      "Real samples Training loss: 0.45707961916923523\n",
      "Fake samples Training loss: 1.2985641956329346\n",
      "Real samples Training loss: 0.46432921290397644\n",
      "Fake samples Training loss: 1.1359460353851318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.4591583013534546\n",
      "Fake samples Training loss: 1.1661522388458252\n",
      "Real samples Training loss: 0.5076692700386047\n",
      "Fake samples Training loss: 1.3147306442260742\n",
      "Real samples Training loss: 0.5221908688545227\n",
      "Fake samples Training loss: 1.2430232763290405\n",
      "Real samples Training loss: 0.4409486949443817\n",
      "Fake samples Training loss: 1.2344807386398315\n",
      "Real samples Training loss: 0.4946812093257904\n",
      "Fake samples Training loss: 1.418971061706543\n",
      "Real samples Training loss: 0.4448060691356659\n",
      "Fake samples Training loss: 1.234413743019104\n",
      "Real samples Training loss: 0.43738752603530884\n",
      "Fake samples Training loss: 1.1992288827896118\n",
      "Real samples Training loss: 0.45156243443489075\n",
      "Fake samples Training loss: 1.1177952289581299\n",
      "Real samples Training loss: 0.5281597971916199\n",
      "Fake samples Training loss: 1.19268000125885\n",
      "Real samples Training loss: 0.44225576519966125\n",
      "Fake samples Training loss: 1.3051879405975342\n",
      "Real samples Training loss: 0.5052037835121155\n",
      "Fake samples Training loss: 1.3876210451126099\n",
      "Real samples Training loss: 0.5110798478126526\n",
      "Fake samples Training loss: 1.2664097547531128\n",
      "Real samples Training loss: 0.44980645179748535\n",
      "Fake samples Training loss: 1.3946189880371094\n",
      "Real samples Training loss: 0.45484164357185364\n",
      "Fake samples Training loss: 1.3309916257858276\n",
      "Real samples Training loss: 0.4815412759780884\n",
      "Fake samples Training loss: 1.2860591411590576\n",
      "Real samples Training loss: 0.5497073531150818\n",
      "Fake samples Training loss: 1.4886631965637207\n",
      "Real samples Training loss: 0.5048410296440125\n",
      "Fake samples Training loss: 1.2400625944137573\n",
      "Real samples Training loss: 0.5364348888397217\n",
      "Fake samples Training loss: 1.3387553691864014\n",
      "Real samples Training loss: 0.42197185754776\n",
      "Fake samples Training loss: 1.140191674232483\n",
      "Real samples Training loss: 0.4785913825035095\n",
      "Fake samples Training loss: 1.2495009899139404\n",
      "Real samples Training loss: 0.49776241183280945\n",
      "Fake samples Training loss: 1.074650764465332\n",
      "Real samples Training loss: 0.4881341755390167\n",
      "Fake samples Training loss: 1.209841251373291\n",
      "Real samples Training loss: 0.49933546781539917\n",
      "Fake samples Training loss: 1.159016489982605\n",
      "Real samples Training loss: 0.4625653922557831\n",
      "Fake samples Training loss: 1.3203084468841553\n",
      "Real samples Training loss: 0.48176026344299316\n",
      "Fake samples Training loss: 1.3126697540283203\n",
      "Real samples Training loss: 0.489055335521698\n",
      "Fake samples Training loss: 1.213323950767517\n",
      "Real samples Training loss: 0.5127706527709961\n",
      "Fake samples Training loss: 1.2967565059661865\n",
      "Real samples Training loss: 0.48211434483528137\n",
      "Fake samples Training loss: 1.1882539987564087\n",
      "Real samples Training loss: 0.5436378121376038\n",
      "Fake samples Training loss: 1.0994590520858765\n",
      "Real samples Training loss: 0.4852845072746277\n",
      "Fake samples Training loss: 1.31471586227417\n",
      "Real samples Training loss: 0.4715547561645508\n",
      "Fake samples Training loss: 1.2212541103363037\n",
      "Real samples Training loss: 0.4481984078884125\n",
      "Fake samples Training loss: 1.2745109796524048\n",
      "Real samples Training loss: 0.5614425539970398\n",
      "Fake samples Training loss: 1.2198412418365479\n",
      "Real samples Training loss: 0.4729005694389343\n",
      "Fake samples Training loss: 1.3344814777374268\n",
      "Real samples Training loss: 0.5016205310821533\n",
      "Fake samples Training loss: 1.3771098852157593\n",
      "Real samples Training loss: 0.4443921446800232\n",
      "Fake samples Training loss: 1.3994293212890625\n",
      "Real samples Training loss: 0.4999050498008728\n",
      "Fake samples Training loss: 1.3212240934371948\n",
      "Real samples Training loss: 0.4168095588684082\n",
      "Fake samples Training loss: 1.339106798171997\n",
      "Real samples Training loss: 0.5024611353874207\n",
      "Fake samples Training loss: 1.3194258213043213\n",
      "Real samples Training loss: 0.47867822647094727\n",
      "Fake samples Training loss: 1.2077808380126953\n",
      "Real samples Training loss: 0.4165116548538208\n",
      "Fake samples Training loss: 1.2605472803115845\n",
      "Real samples Training loss: 0.46995967626571655\n",
      "Fake samples Training loss: 1.2348194122314453\n",
      "Real samples Training loss: 0.5047350525856018\n",
      "Fake samples Training loss: 1.034320592880249\n",
      "Real samples Training loss: 0.43995630741119385\n",
      "Fake samples Training loss: 1.2254682779312134\n",
      "Real samples Training loss: 0.46900174021720886\n",
      "Fake samples Training loss: 1.2307170629501343\n",
      "Real samples Training loss: 0.4598044753074646\n",
      "Fake samples Training loss: 1.311052918434143\n",
      "Real samples Training loss: 0.49680477380752563\n",
      "Fake samples Training loss: 1.2444677352905273\n",
      "Real samples Training loss: 0.5108608603477478\n",
      "Fake samples Training loss: 1.267486810684204\n",
      "Real samples Training loss: 0.45462653040885925\n",
      "Fake samples Training loss: 1.2120039463043213\n",
      "Real samples Training loss: 0.48655685782432556\n",
      "Fake samples Training loss: 1.2934587001800537\n",
      "Real samples Training loss: 0.47890031337738037\n",
      "Fake samples Training loss: 1.2405999898910522\n",
      "Real samples Training loss: 0.46133461594581604\n",
      "Fake samples Training loss: 1.1873325109481812\n",
      "Real samples Training loss: 0.524254560470581\n",
      "Fake samples Training loss: 1.2717608213424683\n",
      "Real samples Training loss: 0.42780396342277527\n",
      "Fake samples Training loss: 1.149401307106018\n",
      "Real samples Training loss: 0.5294463634490967\n",
      "Fake samples Training loss: 1.2223756313323975\n",
      "Real samples Training loss: 0.4892364740371704\n",
      "Fake samples Training loss: 1.2362453937530518\n",
      "Real samples Training loss: 0.5033031702041626\n",
      "Fake samples Training loss: 1.2684414386749268\n",
      "Real samples Training loss: 0.4924064874649048\n",
      "Fake samples Training loss: 1.0840284824371338\n",
      "Real samples Training loss: 0.517926812171936\n",
      "Fake samples Training loss: 1.2705634832382202\n",
      "Real samples Training loss: 0.5471875071525574\n",
      "Fake samples Training loss: 1.1269896030426025\n",
      "Real samples Training loss: 0.4777371287345886\n",
      "Fake samples Training loss: 1.298791766166687\n",
      "Real samples Training loss: 0.4713651239871979\n",
      "Fake samples Training loss: 1.275456428527832\n",
      "Real samples Training loss: 0.4740063548088074\n",
      "Fake samples Training loss: 1.2473527193069458\n",
      "Real samples Training loss: 0.527308464050293\n",
      "Fake samples Training loss: 1.4212313890457153\n",
      "Real samples Training loss: 0.48893722891807556\n",
      "Fake samples Training loss: 1.1708695888519287\n",
      "Real samples Training loss: 0.4904532730579376\n",
      "Fake samples Training loss: 1.1772938966751099\n",
      "Real samples Training loss: 0.4845804274082184\n",
      "Fake samples Training loss: 1.3454145193099976\n",
      "Real samples Training loss: 0.42705708742141724\n",
      "Fake samples Training loss: 1.1972898244857788\n",
      "Real samples Training loss: 0.484713613986969\n",
      "Fake samples Training loss: 1.3499829769134521\n",
      "Real samples Training loss: 0.5047176480293274\n",
      "Fake samples Training loss: 1.337436556816101\n",
      "Real samples Training loss: 0.45191070437431335\n",
      "Fake samples Training loss: 1.18193519115448\n",
      "Real samples Training loss: 0.5424888134002686\n",
      "Fake samples Training loss: 1.1376246213912964\n",
      "Real samples Training loss: 0.47480717301368713\n",
      "Fake samples Training loss: 1.1206278800964355\n",
      "Real samples Training loss: 0.41886940598487854\n",
      "Fake samples Training loss: 1.3222533464431763\n",
      "Real samples Training loss: 0.4786379933357239\n",
      "Fake samples Training loss: 1.262054681777954\n",
      "Real samples Training loss: 0.4751633107662201\n",
      "Fake samples Training loss: 1.2666324377059937\n",
      "Real samples Training loss: 0.41983580589294434\n",
      "Fake samples Training loss: 1.2532634735107422\n",
      "Real samples Training loss: 0.39632171392440796\n",
      "Fake samples Training loss: 1.2938212156295776\n",
      "Real samples Training loss: 0.5118739008903503\n",
      "Fake samples Training loss: 1.2758455276489258\n",
      "Real samples Training loss: 0.5316429138183594\n",
      "Fake samples Training loss: 1.4588623046875\n",
      "Real samples Training loss: 0.5231235027313232\n",
      "Fake samples Training loss: 1.0972492694854736\n",
      "Real samples Training loss: 0.42003998160362244\n",
      "Fake samples Training loss: 1.2784260511398315\n",
      "Real samples Training loss: 0.46628206968307495\n",
      "Fake samples Training loss: 1.176110863685608\n",
      "Real samples Training loss: 0.4465661644935608\n",
      "Fake samples Training loss: 1.1957496404647827\n",
      "Real samples Training loss: 0.40566286444664\n",
      "Fake samples Training loss: 1.1806631088256836\n",
      "Real samples Training loss: 0.4228481948375702\n",
      "Fake samples Training loss: 1.1370136737823486\n",
      "Real samples Training loss: 0.5461982488632202\n",
      "Fake samples Training loss: 1.3211324214935303\n",
      "Real samples Training loss: 0.40812230110168457\n",
      "Fake samples Training loss: 1.195212483406067\n",
      "Real samples Training loss: 0.4646458029747009\n",
      "Fake samples Training loss: 1.1730438470840454\n",
      "Real samples Training loss: 0.4098226726055145\n",
      "Fake samples Training loss: 1.2250970602035522\n",
      "Real samples Training loss: 0.46278056502342224\n",
      "Fake samples Training loss: 1.3267650604248047\n",
      "Real samples Training loss: 0.46654215455055237\n",
      "Fake samples Training loss: 1.2092132568359375\n",
      "Real samples Training loss: 0.4577491283416748\n",
      "Fake samples Training loss: 1.4060354232788086\n",
      "Real samples Training loss: 0.42978060245513916\n",
      "Fake samples Training loss: 1.2013399600982666\n",
      "Real samples Training loss: 0.5630823373794556\n",
      "Fake samples Training loss: 1.371385097503662\n",
      "Real samples Training loss: 0.5293431282043457\n",
      "Fake samples Training loss: 1.1121563911437988\n",
      "Real samples Training loss: 0.46645960211753845\n",
      "Fake samples Training loss: 1.1834981441497803\n",
      "Real samples Training loss: 0.4520999491214752\n",
      "Fake samples Training loss: 1.3405143022537231\n",
      "Real samples Training loss: 0.44784092903137207\n",
      "Fake samples Training loss: 1.2331207990646362\n",
      "Real samples Training loss: 0.5583136081695557\n",
      "Fake samples Training loss: 1.4158982038497925\n",
      "Real samples Training loss: 0.5533474087715149\n",
      "Fake samples Training loss: 1.4829504489898682\n",
      "Real samples Training loss: 0.46860063076019287\n",
      "Fake samples Training loss: 1.3139235973358154\n",
      "Real samples Training loss: 0.47075358033180237\n",
      "Fake samples Training loss: 1.1062175035476685\n",
      "Real samples Training loss: 0.45671984553337097\n",
      "Fake samples Training loss: 1.1616532802581787\n",
      "Real samples Training loss: 0.4434815049171448\n",
      "Fake samples Training loss: 1.215557336807251\n",
      "Real samples Training loss: 0.4871761202812195\n",
      "Fake samples Training loss: 1.1597840785980225\n",
      "Real samples Training loss: 0.5280846953392029\n",
      "Fake samples Training loss: 1.254448413848877\n",
      "Real samples Training loss: 0.5260083675384521\n",
      "Fake samples Training loss: 1.404919981956482\n",
      "Real samples Training loss: 0.4987490475177765\n",
      "Fake samples Training loss: 1.2540475130081177\n",
      "Real samples Training loss: 0.3970779776573181\n",
      "Fake samples Training loss: 1.3559297323226929\n",
      "Real samples Training loss: 0.44902142882347107\n",
      "Fake samples Training loss: 1.2238211631774902\n",
      "Real samples Training loss: 0.44818615913391113\n",
      "Fake samples Training loss: 1.222543716430664\n",
      "Real samples Training loss: 0.49803999066352844\n",
      "Fake samples Training loss: 1.2817490100860596\n",
      "Real samples Training loss: 0.4862949848175049\n",
      "Fake samples Training loss: 1.078377366065979\n",
      "Real samples Training loss: 0.4217407703399658\n",
      "Fake samples Training loss: 1.0339102745056152\n",
      "Real samples Training loss: 0.48815202713012695\n",
      "Fake samples Training loss: 1.2019929885864258\n",
      "Real samples Training loss: 0.5457355976104736\n",
      "Fake samples Training loss: 1.3617662191390991\n",
      "Real samples Training loss: 0.5508177280426025\n",
      "Fake samples Training loss: 1.172822117805481\n",
      "Real samples Training loss: 0.4756564795970917\n",
      "Fake samples Training loss: 1.3214257955551147\n",
      "Real samples Training loss: 0.5239111185073853\n",
      "Fake samples Training loss: 1.4209539890289307\n",
      "Real samples Training loss: 0.4531727135181427\n",
      "Fake samples Training loss: 1.226869821548462\n",
      "Real samples Training loss: 0.5019367933273315\n",
      "Fake samples Training loss: 1.2094839811325073\n",
      "Real samples Training loss: 0.5306926369667053\n",
      "Fake samples Training loss: 1.2236835956573486\n",
      "Real samples Training loss: 0.5442469716072083\n",
      "Fake samples Training loss: 0.9944690465927124\n",
      "Real samples Training loss: 0.5095347166061401\n",
      "Fake samples Training loss: 1.274794340133667\n",
      "Real samples Training loss: 0.4508480429649353\n",
      "Fake samples Training loss: 1.36616849899292\n",
      "Real samples Training loss: 0.4296494126319885\n",
      "Fake samples Training loss: 1.3429213762283325\n",
      "Real samples Training loss: 0.4236254394054413\n",
      "Fake samples Training loss: 1.1861740350723267\n",
      "Real samples Training loss: 0.48379233479499817\n",
      "Fake samples Training loss: 1.2637251615524292\n",
      "Real samples Training loss: 0.4532025158405304\n",
      "Fake samples Training loss: 1.1379503011703491\n",
      "Real samples Training loss: 0.48265501856803894\n",
      "Fake samples Training loss: 1.2440296411514282\n",
      "Real samples Training loss: 0.4819413721561432\n",
      "Fake samples Training loss: 1.1671074628829956\n",
      "Real samples Training loss: 0.5019705295562744\n",
      "Fake samples Training loss: 1.0779731273651123\n",
      "Real samples Training loss: 0.5410662293434143\n",
      "Fake samples Training loss: 1.1561768054962158\n",
      "Real samples Training loss: 0.4886329770088196\n",
      "Fake samples Training loss: 1.1700139045715332\n",
      "Real samples Training loss: 0.41750168800354004\n",
      "Fake samples Training loss: 1.1620635986328125\n",
      "Real samples Training loss: 0.525794506072998\n",
      "Fake samples Training loss: 1.1518807411193848\n",
      "Real samples Training loss: 0.5179816484451294\n",
      "Fake samples Training loss: 1.33457612991333\n",
      "Real samples Training loss: 0.47605130076408386\n",
      "Fake samples Training loss: 1.3669171333312988\n",
      "Real samples Training loss: 0.45285171270370483\n",
      "Fake samples Training loss: 1.234349250793457\n",
      "Real samples Training loss: 0.466974139213562\n",
      "Fake samples Training loss: 1.0710803270339966\n",
      "Real samples Training loss: 0.47936001420021057\n",
      "Fake samples Training loss: 1.1619330644607544\n",
      "Real samples Training loss: 0.4270626902580261\n",
      "Fake samples Training loss: 1.295124888420105\n",
      "Real samples Training loss: 0.4332776367664337\n",
      "Fake samples Training loss: 1.0906766653060913\n",
      "Real samples Training loss: 0.47080475091934204\n",
      "Fake samples Training loss: 1.2445639371871948\n",
      "Real samples Training loss: 0.48293188214302063\n",
      "Fake samples Training loss: 1.3739404678344727\n",
      "Real samples Training loss: 0.5073068141937256\n",
      "Fake samples Training loss: 1.2186036109924316\n",
      "Real samples Training loss: 0.5155054926872253\n",
      "Fake samples Training loss: 1.2080124616622925\n",
      "Real samples Training loss: 0.48607584834098816\n",
      "Fake samples Training loss: 1.2262710332870483\n",
      "Real samples Training loss: 0.4842303395271301\n",
      "Fake samples Training loss: 1.135521411895752\n",
      "Real samples Training loss: 0.529265284538269\n",
      "Fake samples Training loss: 1.4118931293487549\n",
      "Real samples Training loss: 0.4433062672615051\n",
      "Fake samples Training loss: 1.3136588335037231\n",
      "Real samples Training loss: 0.5016376972198486\n",
      "Fake samples Training loss: 1.2087585926055908\n",
      "Real samples Training loss: 0.515716552734375\n",
      "Fake samples Training loss: 1.340720772743225\n",
      "Real samples Training loss: 0.5575194954872131\n",
      "Fake samples Training loss: 1.2288949489593506\n",
      "Real samples Training loss: 0.44887474179267883\n",
      "Fake samples Training loss: 1.3356008529663086\n",
      "Real samples Training loss: 0.4603688418865204\n",
      "Fake samples Training loss: 1.163256049156189\n",
      "Real samples Training loss: 0.47863173484802246\n",
      "Fake samples Training loss: 1.3708659410476685\n",
      "Real samples Training loss: 0.47618910670280457\n",
      "Fake samples Training loss: 1.357816219329834\n",
      "Real samples Training loss: 0.49570906162261963\n",
      "Fake samples Training loss: 1.1828210353851318\n",
      "Real samples Training loss: 0.5033815503120422\n",
      "Fake samples Training loss: 1.2140922546386719\n",
      "Real samples Training loss: 0.4900302290916443\n",
      "Fake samples Training loss: 1.242775321006775\n",
      "Real samples Training loss: 0.49604934453964233\n",
      "Fake samples Training loss: 1.0965098142623901\n",
      "Real samples Training loss: 0.46526333689689636\n",
      "Fake samples Training loss: 1.2058827877044678\n",
      "Real samples Training loss: 0.4736267626285553\n",
      "Fake samples Training loss: 1.2367721796035767\n",
      "Real samples Training loss: 0.5129749774932861\n",
      "Fake samples Training loss: 1.1747833490371704\n",
      "Real samples Training loss: 0.4430309236049652\n",
      "Fake samples Training loss: 1.181854248046875\n",
      "Real samples Training loss: 0.5062108039855957\n",
      "Fake samples Training loss: 1.250770092010498\n",
      "Real samples Training loss: 0.498101145029068\n",
      "Fake samples Training loss: 1.1529396772384644\n",
      "Real samples Training loss: 0.5237008333206177\n",
      "Fake samples Training loss: 1.2721785306930542\n",
      "Real samples Training loss: 0.5162869095802307\n",
      "Fake samples Training loss: 1.2371280193328857\n",
      "Real samples Training loss: 0.4276968538761139\n",
      "Fake samples Training loss: 1.3452893495559692\n",
      "Real samples Training loss: 0.4952593147754669\n",
      "Fake samples Training loss: 1.159830927848816\n",
      "Real samples Training loss: 0.5086917877197266\n",
      "Fake samples Training loss: 1.2388380765914917\n",
      "Real samples Training loss: 0.4290616810321808\n",
      "Fake samples Training loss: 1.2762596607208252\n",
      "Real samples Training loss: 0.47840383648872375\n",
      "Fake samples Training loss: 1.321863055229187\n",
      "Real samples Training loss: 0.5167892575263977\n",
      "Fake samples Training loss: 1.2755213975906372\n",
      "Real samples Training loss: 0.4720722436904907\n",
      "Fake samples Training loss: 1.1149566173553467\n",
      "Real samples Training loss: 0.4090566635131836\n",
      "Fake samples Training loss: 1.2528421878814697\n",
      "Real samples Training loss: 0.4128751754760742\n",
      "Fake samples Training loss: 1.240932583808899\n",
      "Real samples Training loss: 0.4923866093158722\n",
      "Fake samples Training loss: 1.1908351182937622\n",
      "Real samples Training loss: 0.529754102230072\n",
      "Fake samples Training loss: 1.334892749786377\n",
      "Real samples Training loss: 0.480416864156723\n",
      "Fake samples Training loss: 1.1949462890625\n",
      "Real samples Training loss: 0.4652998447418213\n",
      "Fake samples Training loss: 1.3402401208877563\n",
      "Real samples Training loss: 0.477134644985199\n",
      "Fake samples Training loss: 1.2758116722106934\n",
      "Real samples Training loss: 0.4597822427749634\n",
      "Fake samples Training loss: 1.1672673225402832\n",
      "Real samples Training loss: 0.4632296562194824\n",
      "Fake samples Training loss: 1.2653477191925049\n",
      "Real samples Training loss: 0.5144218802452087\n",
      "Fake samples Training loss: 1.0988987684249878\n",
      "Real samples Training loss: 0.44329994916915894\n",
      "Fake samples Training loss: 1.1794809103012085\n",
      "Real samples Training loss: 0.47560644149780273\n",
      "Fake samples Training loss: 1.2976967096328735\n",
      "Real samples Training loss: 0.4170137047767639\n",
      "Fake samples Training loss: 1.39218270778656\n",
      "Real samples Training loss: 0.4751976430416107\n",
      "Fake samples Training loss: 1.3669352531433105\n",
      "Real samples Training loss: 0.4855893552303314\n",
      "Fake samples Training loss: 1.3450323343276978\n",
      "Real samples Training loss: 0.4298115074634552\n",
      "Fake samples Training loss: 1.2492382526397705\n",
      "Real samples Training loss: 0.4546133577823639\n",
      "Fake samples Training loss: 1.0687609910964966\n",
      "Real samples Training loss: 0.46031418442726135\n",
      "Fake samples Training loss: 1.3179149627685547\n",
      "Real samples Training loss: 0.48704737424850464\n",
      "Fake samples Training loss: 1.200242042541504\n",
      "Real samples Training loss: 0.47435903549194336\n",
      "Fake samples Training loss: 1.230751395225525\n",
      "Real samples Training loss: 0.5139954090118408\n",
      "Fake samples Training loss: 1.3441853523254395\n",
      "Real samples Training loss: 0.4278610944747925\n",
      "Fake samples Training loss: 1.396889567375183\n",
      "Real samples Training loss: 0.48902758955955505\n",
      "Fake samples Training loss: 1.173794150352478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.5052912831306458\n",
      "Fake samples Training loss: 1.249782681465149\n",
      "Real samples Training loss: 0.4816110134124756\n",
      "Fake samples Training loss: 1.4094637632369995\n",
      "Real samples Training loss: 0.4851190447807312\n",
      "Fake samples Training loss: 1.1671102046966553\n",
      "Real samples Training loss: 0.4466707408428192\n",
      "Fake samples Training loss: 1.5398043394088745\n",
      "Real samples Training loss: 0.41222789883613586\n",
      "Fake samples Training loss: 1.2382502555847168\n",
      "Real samples Training loss: 0.47033944725990295\n",
      "Fake samples Training loss: 1.1507799625396729\n",
      "Real samples Training loss: 0.453389436006546\n",
      "Fake samples Training loss: 1.5961079597473145\n",
      "Real samples Training loss: 0.5114468932151794\n",
      "Fake samples Training loss: 1.2011302709579468\n",
      "Real samples Training loss: 0.5045715570449829\n",
      "Fake samples Training loss: 1.2067800760269165\n",
      "Real samples Training loss: 0.5099182724952698\n",
      "Fake samples Training loss: 1.2089262008666992\n",
      "Real samples Training loss: 0.4441966116428375\n",
      "Fake samples Training loss: 1.3051174879074097\n",
      "Real samples Training loss: 0.49332135915756226\n",
      "Fake samples Training loss: 1.1854833364486694\n",
      "Real samples Training loss: 0.47092872858047485\n",
      "Fake samples Training loss: 1.2228795289993286\n",
      "Real samples Training loss: 0.45358169078826904\n",
      "Fake samples Training loss: 1.2055928707122803\n",
      "Real samples Training loss: 0.45896556973457336\n",
      "Fake samples Training loss: 1.279710054397583\n",
      "Real samples Training loss: 0.49865826964378357\n",
      "Fake samples Training loss: 1.288289189338684\n",
      "Real samples Training loss: 0.5391895174980164\n",
      "Fake samples Training loss: 1.37627375125885\n",
      "Real samples Training loss: 0.4730605185031891\n",
      "Fake samples Training loss: 1.2574422359466553\n",
      "Real samples Training loss: 0.4672033488750458\n",
      "Fake samples Training loss: 1.232149362564087\n",
      "Real samples Training loss: 0.4649371802806854\n",
      "Fake samples Training loss: 1.3454755544662476\n",
      "Real samples Training loss: 0.4707391858100891\n",
      "Fake samples Training loss: 1.2871166467666626\n",
      "Real samples Training loss: 0.48353704810142517\n",
      "Fake samples Training loss: 1.2034845352172852\n",
      "Real samples Training loss: 0.5170081853866577\n",
      "Fake samples Training loss: 1.181698203086853\n",
      "Real samples Training loss: 0.49671685695648193\n",
      "Fake samples Training loss: 1.2079098224639893\n",
      "Real samples Training loss: 0.4854904115200043\n",
      "Fake samples Training loss: 1.3217693567276\n",
      "Real samples Training loss: 0.4792645275592804\n",
      "Fake samples Training loss: 1.0820425748825073\n",
      "Real samples Training loss: 0.44996771216392517\n",
      "Fake samples Training loss: 1.4188456535339355\n",
      "Real samples Training loss: 0.5652479529380798\n",
      "Fake samples Training loss: 1.1915757656097412\n",
      "Real samples Training loss: 0.4899064302444458\n",
      "Fake samples Training loss: 1.2359998226165771\n",
      "Real samples Training loss: 0.4697037637233734\n",
      "Fake samples Training loss: 1.1695327758789062\n",
      "Real samples Training loss: 0.4746165871620178\n",
      "Fake samples Training loss: 1.1168617010116577\n",
      "Real samples Training loss: 0.4933032691478729\n",
      "Fake samples Training loss: 1.1371791362762451\n",
      "Real samples Training loss: 0.4551061689853668\n",
      "Fake samples Training loss: 1.2955958843231201\n",
      "Real samples Training loss: 0.42888104915618896\n",
      "Fake samples Training loss: 1.2523216009140015\n",
      "Real samples Training loss: 0.4901970326900482\n",
      "Fake samples Training loss: 1.1722592115402222\n",
      "Real samples Training loss: 0.45611000061035156\n",
      "Fake samples Training loss: 1.1667245626449585\n",
      "Real samples Training loss: 0.5118877291679382\n",
      "Fake samples Training loss: 1.392086148262024\n",
      "Real samples Training loss: 0.47557565569877625\n",
      "Fake samples Training loss: 1.1414515972137451\n",
      "Real samples Training loss: 0.5173344016075134\n",
      "Fake samples Training loss: 1.4046273231506348\n",
      "Real samples Training loss: 0.48393601179122925\n",
      "Fake samples Training loss: 1.243072509765625\n",
      "Real samples Training loss: 0.43029871582984924\n",
      "Fake samples Training loss: 1.2356274127960205\n",
      "Real samples Training loss: 0.5198565125465393\n",
      "Fake samples Training loss: 1.244946002960205\n",
      "Real samples Training loss: 0.48771369457244873\n",
      "Fake samples Training loss: 1.221610426902771\n",
      "Real samples Training loss: 0.43546414375305176\n",
      "Fake samples Training loss: 1.1797380447387695\n",
      "Real samples Training loss: 0.4214824140071869\n",
      "Fake samples Training loss: 1.3619130849838257\n",
      "Real samples Training loss: 0.47555267810821533\n",
      "Fake samples Training loss: 1.187746524810791\n",
      "Real samples Training loss: 0.44614967703819275\n",
      "Fake samples Training loss: 1.166117787361145\n",
      "Real samples Training loss: 0.5119247436523438\n",
      "Fake samples Training loss: 1.1992515325546265\n",
      "Real samples Training loss: 0.5028767585754395\n",
      "Fake samples Training loss: 1.2057543992996216\n",
      "Real samples Training loss: 0.48828303813934326\n",
      "Fake samples Training loss: 1.2028236389160156\n",
      "Real samples Training loss: 0.5423340201377869\n",
      "Fake samples Training loss: 1.25313401222229\n",
      "Real samples Training loss: 0.47309067845344543\n",
      "Fake samples Training loss: 1.2025922536849976\n",
      "Real samples Training loss: 0.44847002625465393\n",
      "Fake samples Training loss: 1.2425912618637085\n",
      "Real samples Training loss: 0.46206849813461304\n",
      "Fake samples Training loss: 1.2020307779312134\n",
      "Real samples Training loss: 0.4763665199279785\n",
      "Fake samples Training loss: 1.1521034240722656\n",
      "Real samples Training loss: 0.5216510891914368\n",
      "Fake samples Training loss: 1.3054401874542236\n",
      "Real samples Training loss: 0.5201533436775208\n",
      "Fake samples Training loss: 1.0623670816421509\n",
      "Real samples Training loss: 0.5059447288513184\n",
      "Fake samples Training loss: 1.1581838130950928\n",
      "Real samples Training loss: 0.4099651873111725\n",
      "Fake samples Training loss: 1.251463770866394\n",
      "Real samples Training loss: 0.49616098403930664\n",
      "Fake samples Training loss: 1.240777850151062\n",
      "Real samples Training loss: 0.466257244348526\n",
      "Fake samples Training loss: 1.201587200164795\n",
      "Real samples Training loss: 0.4713112413883209\n",
      "Fake samples Training loss: 1.0398313999176025\n",
      "Real samples Training loss: 0.4766363203525543\n",
      "Fake samples Training loss: 1.3178447484970093\n",
      "Real samples Training loss: 0.499065637588501\n",
      "Fake samples Training loss: 1.2960563898086548\n",
      "Real samples Training loss: 0.46932217478752136\n",
      "Fake samples Training loss: 1.3364533185958862\n",
      "Real samples Training loss: 0.4852369725704193\n",
      "Fake samples Training loss: 1.1274631023406982\n",
      "Real samples Training loss: 0.4308013319969177\n",
      "Fake samples Training loss: 1.0634329319000244\n",
      "Real samples Training loss: 0.48291701078414917\n",
      "Fake samples Training loss: 1.3526612520217896\n",
      "Real samples Training loss: 0.44578397274017334\n",
      "Fake samples Training loss: 1.095799207687378\n",
      "Real samples Training loss: 0.5113937854766846\n",
      "Fake samples Training loss: 1.234210729598999\n",
      "Real samples Training loss: 0.432862251996994\n",
      "Fake samples Training loss: 1.287535309791565\n",
      "Real samples Training loss: 0.5533424615859985\n",
      "Fake samples Training loss: 1.2278424501419067\n",
      "Real samples Training loss: 0.46612414717674255\n",
      "Fake samples Training loss: 1.2185832262039185\n",
      "Real samples Training loss: 0.42759668827056885\n",
      "Fake samples Training loss: 1.1941264867782593\n",
      "Real samples Training loss: 0.5347449779510498\n",
      "Fake samples Training loss: 1.177172064781189\n",
      "Real samples Training loss: 0.4831978678703308\n",
      "Fake samples Training loss: 1.2120752334594727\n",
      "Real samples Training loss: 0.503127932548523\n",
      "Fake samples Training loss: 1.3718262910842896\n",
      "Real samples Training loss: 0.4544246196746826\n",
      "Fake samples Training loss: 1.2826615571975708\n",
      "Real samples Training loss: 0.4836405813694\n",
      "Fake samples Training loss: 1.196569561958313\n",
      "Real samples Training loss: 0.46132221817970276\n",
      "Fake samples Training loss: 1.2387748956680298\n",
      "Real samples Training loss: 0.5113108158111572\n",
      "Fake samples Training loss: 1.3378701210021973\n",
      "Real samples Training loss: 0.5080252885818481\n",
      "Fake samples Training loss: 1.224690556526184\n",
      "Real samples Training loss: 0.4639400541782379\n",
      "Fake samples Training loss: 1.1901957988739014\n",
      "Real samples Training loss: 0.4846062660217285\n",
      "Fake samples Training loss: 1.2741904258728027\n",
      "Real samples Training loss: 0.46989181637763977\n",
      "Fake samples Training loss: 1.3140121698379517\n",
      "Real samples Training loss: 0.5249622464179993\n",
      "Fake samples Training loss: 1.3172001838684082\n",
      "Real samples Training loss: 0.4436427354812622\n",
      "Fake samples Training loss: 1.155008316040039\n",
      "Real samples Training loss: 0.5070552825927734\n",
      "Fake samples Training loss: 1.3090896606445312\n",
      "Real samples Training loss: 0.41053545475006104\n",
      "Fake samples Training loss: 1.2395572662353516\n",
      "Real samples Training loss: 0.5103725790977478\n",
      "Fake samples Training loss: 1.1923600435256958\n",
      "Real samples Training loss: 0.4426625669002533\n",
      "Fake samples Training loss: 1.3182650804519653\n",
      "Real samples Training loss: 0.5135141015052795\n",
      "Fake samples Training loss: 1.1547268629074097\n",
      "Real samples Training loss: 0.5308322310447693\n",
      "Fake samples Training loss: 1.1604764461517334\n",
      "Real samples Training loss: 0.5293436646461487\n",
      "Fake samples Training loss: 1.2714588642120361\n",
      "Real samples Training loss: 0.5244724154472351\n",
      "Fake samples Training loss: 1.2017111778259277\n",
      "Real samples Training loss: 0.5310853719711304\n",
      "Fake samples Training loss: 1.2694544792175293\n",
      "Real samples Training loss: 0.43646201491355896\n",
      "Fake samples Training loss: 1.3118581771850586\n",
      "Real samples Training loss: 0.4260367155075073\n",
      "Fake samples Training loss: 1.2797263860702515\n",
      "Real samples Training loss: 0.5157239437103271\n",
      "Fake samples Training loss: 1.1920678615570068\n",
      "Real samples Training loss: 0.499671995639801\n",
      "Fake samples Training loss: 1.0414416790008545\n",
      "Real samples Training loss: 0.5520300269126892\n",
      "Fake samples Training loss: 1.2215460538864136\n",
      "Real samples Training loss: 0.48354780673980713\n",
      "Fake samples Training loss: 1.2846261262893677\n",
      "Real samples Training loss: 0.5230633616447449\n",
      "Fake samples Training loss: 1.4154599905014038\n",
      "Real samples Training loss: 0.46032804250717163\n",
      "Fake samples Training loss: 1.2243815660476685\n",
      "Real samples Training loss: 0.4997484087944031\n",
      "Fake samples Training loss: 1.2978557348251343\n",
      "Real samples Training loss: 0.5343236327171326\n",
      "Fake samples Training loss: 1.2471299171447754\n",
      "Real samples Training loss: 0.4822717308998108\n",
      "Fake samples Training loss: 1.2131086587905884\n",
      "Real samples Training loss: 0.5004470348358154\n",
      "Fake samples Training loss: 1.1263641119003296\n",
      "Real samples Training loss: 0.4464174807071686\n",
      "Fake samples Training loss: 1.2933121919631958\n",
      "Real samples Training loss: 0.5052121877670288\n",
      "Fake samples Training loss: 1.2135895490646362\n",
      "Real samples Training loss: 0.46584710478782654\n",
      "Fake samples Training loss: 1.384719967842102\n",
      "Real samples Training loss: 0.47120535373687744\n",
      "Fake samples Training loss: 1.3832341432571411\n",
      "Real samples Training loss: 0.46489399671554565\n",
      "Fake samples Training loss: 1.169997215270996\n",
      "Real samples Training loss: 0.4405355453491211\n",
      "Fake samples Training loss: 1.1831685304641724\n",
      "Real samples Training loss: 0.4913160800933838\n",
      "Fake samples Training loss: 1.2521922588348389\n",
      "Real samples Training loss: 0.49129563570022583\n",
      "Fake samples Training loss: 1.3517524003982544\n",
      "Real samples Training loss: 0.5111872553825378\n",
      "Fake samples Training loss: 1.1919593811035156\n",
      "Real samples Training loss: 0.5293626189231873\n",
      "Fake samples Training loss: 1.5231175422668457\n",
      "Real samples Training loss: 0.4961741864681244\n",
      "Fake samples Training loss: 1.2722928524017334\n",
      "Real samples Training loss: 0.5014024972915649\n",
      "Fake samples Training loss: 1.2239089012145996\n",
      "Real samples Training loss: 0.4954688847064972\n",
      "Fake samples Training loss: 1.286597490310669\n",
      "Real samples Training loss: 0.4542633891105652\n",
      "Fake samples Training loss: 1.426973819732666\n",
      "Real samples Training loss: 0.49870720505714417\n",
      "Fake samples Training loss: 1.2751684188842773\n",
      "Real samples Training loss: 0.5467013716697693\n",
      "Fake samples Training loss: 1.2476171255111694\n",
      "Real samples Training loss: 0.43466517329216003\n",
      "Fake samples Training loss: 1.2535816431045532\n",
      "Real samples Training loss: 0.5127608776092529\n",
      "Fake samples Training loss: 1.1066027879714966\n",
      "Real samples Training loss: 0.46489307284355164\n",
      "Fake samples Training loss: 1.0666821002960205\n",
      "Real samples Training loss: 0.4732174277305603\n",
      "Fake samples Training loss: 1.1752742528915405\n",
      "Real samples Training loss: 0.4898020029067993\n",
      "Fake samples Training loss: 1.2367621660232544\n",
      "Real samples Training loss: 0.4380101263523102\n",
      "Fake samples Training loss: 1.2396364212036133\n",
      "Real samples Training loss: 0.5206366181373596\n",
      "Fake samples Training loss: 1.2699183225631714\n",
      "Real samples Training loss: 0.4412563145160675\n",
      "Fake samples Training loss: 1.2344526052474976\n",
      "Real samples Training loss: 0.4487975835800171\n",
      "Fake samples Training loss: 1.2108694314956665\n",
      "Real samples Training loss: 0.5199133157730103\n",
      "Fake samples Training loss: 1.1316900253295898\n",
      "Real samples Training loss: 0.46172982454299927\n",
      "Fake samples Training loss: 1.05568265914917\n",
      "Real samples Training loss: 0.44230782985687256\n",
      "Fake samples Training loss: 1.222486138343811\n",
      "Real samples Training loss: 0.4557090103626251\n",
      "Fake samples Training loss: 1.040623426437378\n",
      "Real samples Training loss: 0.44110703468322754\n",
      "Fake samples Training loss: 1.092057466506958\n",
      "Real samples Training loss: 0.4765647351741791\n",
      "Fake samples Training loss: 1.2100058794021606\n",
      "Real samples Training loss: 0.4135875701904297\n",
      "Fake samples Training loss: 1.291822075843811\n",
      "Real samples Training loss: 0.4565613269805908\n",
      "Fake samples Training loss: 1.3390369415283203\n",
      "Real samples Training loss: 0.4427473545074463\n",
      "Fake samples Training loss: 1.200432538986206\n",
      "Real samples Training loss: 0.43774351477622986\n",
      "Fake samples Training loss: 1.4207745790481567\n",
      "Real samples Training loss: 0.4996563792228699\n",
      "Fake samples Training loss: 1.093232274055481\n",
      "Real samples Training loss: 0.5186209082603455\n",
      "Fake samples Training loss: 1.26426100730896\n",
      "Real samples Training loss: 0.5209567546844482\n",
      "Fake samples Training loss: 1.1893268823623657\n",
      "Real samples Training loss: 0.4803248345851898\n",
      "Fake samples Training loss: 1.2283554077148438\n",
      "Real samples Training loss: 0.4129067361354828\n",
      "Fake samples Training loss: 1.177903413772583\n",
      "Real samples Training loss: 0.4060058891773224\n",
      "Fake samples Training loss: 1.2988460063934326\n",
      "Real samples Training loss: 0.4460836350917816\n",
      "Fake samples Training loss: 1.2902882099151611\n",
      "Real samples Training loss: 0.4633462727069855\n",
      "Fake samples Training loss: 1.2885903120040894\n",
      "Real samples Training loss: 0.5266645550727844\n",
      "Fake samples Training loss: 1.276106357574463\n",
      "Real samples Training loss: 0.5900755524635315\n",
      "Fake samples Training loss: 1.2335829734802246\n",
      "Real samples Training loss: 0.4697643518447876\n",
      "Fake samples Training loss: 1.2986088991165161\n",
      "Real samples Training loss: 0.4729851484298706\n",
      "Fake samples Training loss: 1.20901620388031\n",
      "Real samples Training loss: 0.4708627462387085\n",
      "Fake samples Training loss: 1.3410066366195679\n",
      "Real samples Training loss: 0.4907349944114685\n",
      "Fake samples Training loss: 1.2501485347747803\n",
      "Real samples Training loss: 0.5130699872970581\n",
      "Fake samples Training loss: 1.3813611268997192\n",
      "Real samples Training loss: 0.4855920672416687\n",
      "Fake samples Training loss: 1.1597530841827393\n",
      "Real samples Training loss: 0.44191133975982666\n",
      "Fake samples Training loss: 1.3380085229873657\n",
      "Real samples Training loss: 0.4639257788658142\n",
      "Fake samples Training loss: 1.1968950033187866\n",
      "Real samples Training loss: 0.44008561968803406\n",
      "Fake samples Training loss: 1.3279030323028564\n",
      "Real samples Training loss: 0.5151221752166748\n",
      "Fake samples Training loss: 1.0884581804275513\n",
      "Real samples Training loss: 0.4737846553325653\n",
      "Fake samples Training loss: 1.3824727535247803\n",
      "Real samples Training loss: 0.44720566272735596\n",
      "Fake samples Training loss: 1.1797761917114258\n",
      "Real samples Training loss: 0.48840460181236267\n",
      "Fake samples Training loss: 1.3401495218276978\n",
      "Real samples Training loss: 0.4649002254009247\n",
      "Fake samples Training loss: 1.105586290359497\n",
      "Real samples Training loss: 0.42473164200782776\n",
      "Fake samples Training loss: 1.2524902820587158\n",
      "Real samples Training loss: 0.4991718530654907\n",
      "Fake samples Training loss: 1.1421517133712769\n",
      "Real samples Training loss: 0.49209263920783997\n",
      "Fake samples Training loss: 1.4270113706588745\n",
      "Real samples Training loss: 0.4441896975040436\n",
      "Fake samples Training loss: 1.3737356662750244\n",
      "Real samples Training loss: 0.4461222290992737\n",
      "Fake samples Training loss: 1.3913066387176514\n",
      "Real samples Training loss: 0.4425477385520935\n",
      "Fake samples Training loss: 1.3830633163452148\n",
      "Real samples Training loss: 0.568966805934906\n",
      "Fake samples Training loss: 1.0181208848953247\n",
      "Real samples Training loss: 0.4367581307888031\n",
      "Fake samples Training loss: 1.2436124086380005\n",
      "Real samples Training loss: 0.49595656991004944\n",
      "Fake samples Training loss: 1.0801981687545776\n",
      "Real samples Training loss: 0.4641833007335663\n",
      "Fake samples Training loss: 1.2491023540496826\n",
      "Real samples Training loss: 0.48289865255355835\n",
      "Fake samples Training loss: 1.1552256345748901\n",
      "Real samples Training loss: 0.5106896758079529\n",
      "Fake samples Training loss: 1.234178066253662\n",
      "Real samples Training loss: 0.42446473240852356\n",
      "Fake samples Training loss: 1.322394847869873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.48967376351356506\n",
      "Fake samples Training loss: 1.3938281536102295\n",
      "Real samples Training loss: 0.5767343044281006\n",
      "Fake samples Training loss: 1.2033610343933105\n",
      "Real samples Training loss: 0.5672666430473328\n",
      "Fake samples Training loss: 1.4145516157150269\n",
      "Real samples Training loss: 0.501390278339386\n",
      "Fake samples Training loss: 1.2030644416809082\n",
      "Real samples Training loss: 0.4140281677246094\n",
      "Fake samples Training loss: 1.203892707824707\n",
      "Real samples Training loss: 0.4494147002696991\n",
      "Fake samples Training loss: 1.2011196613311768\n",
      "Real samples Training loss: 0.45762011408805847\n",
      "Fake samples Training loss: 1.296699047088623\n",
      "Real samples Training loss: 0.4667462408542633\n",
      "Fake samples Training loss: 1.434253454208374\n",
      "Real samples Training loss: 0.43540409207344055\n",
      "Fake samples Training loss: 1.3907099962234497\n",
      "Real samples Training loss: 0.4583800435066223\n",
      "Fake samples Training loss: 1.3032366037368774\n",
      "Real samples Training loss: 0.45763808488845825\n",
      "Fake samples Training loss: 1.1294937133789062\n",
      "Real samples Training loss: 0.5036482214927673\n",
      "Fake samples Training loss: 1.2742300033569336\n",
      "Real samples Training loss: 0.4540073573589325\n",
      "Fake samples Training loss: 1.1705365180969238\n",
      "Real samples Training loss: 0.5020262598991394\n",
      "Fake samples Training loss: 1.236920952796936\n",
      "Real samples Training loss: 0.4634856581687927\n",
      "Fake samples Training loss: 1.265903353691101\n",
      "Real samples Training loss: 0.4855210781097412\n",
      "Fake samples Training loss: 1.1687161922454834\n",
      "Real samples Training loss: 0.43364861607551575\n",
      "Fake samples Training loss: 1.356492280960083\n",
      "Real samples Training loss: 0.5241377353668213\n",
      "Fake samples Training loss: 1.1670913696289062\n",
      "Real samples Training loss: 0.5423445701599121\n",
      "Fake samples Training loss: 1.149788737297058\n",
      "Real samples Training loss: 0.5074757933616638\n",
      "Fake samples Training loss: 1.1713204383850098\n",
      "Real samples Training loss: 0.49330663681030273\n",
      "Fake samples Training loss: 1.3612910509109497\n",
      "Real samples Training loss: 0.4549005627632141\n",
      "Fake samples Training loss: 1.2255462408065796\n",
      "Real samples Training loss: 0.521112322807312\n",
      "Fake samples Training loss: 1.2148189544677734\n",
      "Real samples Training loss: 0.4675179123878479\n",
      "Fake samples Training loss: 1.281443476676941\n",
      "Real samples Training loss: 0.48406144976615906\n",
      "Fake samples Training loss: 1.151587963104248\n",
      "Real samples Training loss: 0.4977984130382538\n",
      "Fake samples Training loss: 1.2130540609359741\n",
      "Real samples Training loss: 0.5065872073173523\n",
      "Fake samples Training loss: 1.2146464586257935\n",
      "Real samples Training loss: 0.48886412382125854\n",
      "Fake samples Training loss: 1.1328126192092896\n",
      "Real samples Training loss: 0.430768758058548\n",
      "Fake samples Training loss: 1.2934134006500244\n",
      "Real samples Training loss: 0.5260302424430847\n",
      "Fake samples Training loss: 1.116567850112915\n",
      "Real samples Training loss: 0.5464239120483398\n",
      "Fake samples Training loss: 1.3904807567596436\n",
      "Real samples Training loss: 0.5512136816978455\n",
      "Fake samples Training loss: 1.371812105178833\n",
      "Real samples Training loss: 0.40355584025382996\n",
      "Fake samples Training loss: 1.1976350545883179\n",
      "Real samples Training loss: 0.4996919333934784\n",
      "Fake samples Training loss: 1.2932839393615723\n",
      "Real samples Training loss: 0.5048030614852905\n",
      "Fake samples Training loss: 1.3257715702056885\n",
      "Real samples Training loss: 0.4551520347595215\n",
      "Fake samples Training loss: 1.2691800594329834\n",
      "Real samples Training loss: 0.4696289002895355\n",
      "Fake samples Training loss: 1.1832225322723389\n",
      "Real samples Training loss: 0.4991717040538788\n",
      "Fake samples Training loss: 1.164952278137207\n",
      "Real samples Training loss: 0.484364777803421\n",
      "Fake samples Training loss: 1.2121458053588867\n",
      "Real samples Training loss: 0.4633876383304596\n",
      "Fake samples Training loss: 1.2348097562789917\n",
      "Real samples Training loss: 0.4994557499885559\n",
      "Fake samples Training loss: 1.179401159286499\n",
      "Real samples Training loss: 0.45898309350013733\n",
      "Fake samples Training loss: 1.248321771621704\n",
      "Real samples Training loss: 0.45956873893737793\n",
      "Fake samples Training loss: 1.1577372550964355\n",
      "Real samples Training loss: 0.47238826751708984\n",
      "Fake samples Training loss: 1.2341097593307495\n",
      "Real samples Training loss: 0.5504481792449951\n",
      "Fake samples Training loss: 1.2691376209259033\n",
      "Real samples Training loss: 0.4839477837085724\n",
      "Fake samples Training loss: 1.2168896198272705\n",
      "Real samples Training loss: 0.42431876063346863\n",
      "Fake samples Training loss: 1.134408950805664\n",
      "Real samples Training loss: 0.47380882501602173\n",
      "Fake samples Training loss: 1.222400426864624\n",
      "Real samples Training loss: 0.5623322129249573\n",
      "Fake samples Training loss: 1.1631532907485962\n",
      "Real samples Training loss: 0.4599592685699463\n",
      "Fake samples Training loss: 1.2132614850997925\n",
      "Real samples Training loss: 0.5047022700309753\n",
      "Fake samples Training loss: 1.215653657913208\n",
      "Real samples Training loss: 0.5400580167770386\n",
      "Fake samples Training loss: 1.4062267541885376\n",
      "Real samples Training loss: 0.5415985584259033\n",
      "Fake samples Training loss: 1.12857186794281\n",
      "Real samples Training loss: 0.5099175572395325\n",
      "Fake samples Training loss: 1.299911379814148\n",
      "Real samples Training loss: 0.5235245823860168\n",
      "Fake samples Training loss: 1.2634185552597046\n",
      "Real samples Training loss: 0.4602183699607849\n",
      "Fake samples Training loss: 1.2764778137207031\n",
      "Real samples Training loss: 0.4800891876220703\n",
      "Fake samples Training loss: 1.1519547700881958\n",
      "Real samples Training loss: 0.4758673906326294\n",
      "Fake samples Training loss: 1.2953821420669556\n",
      "Real samples Training loss: 0.5118699669837952\n",
      "Fake samples Training loss: 1.255785584449768\n",
      "Real samples Training loss: 0.4844510853290558\n",
      "Fake samples Training loss: 1.2268106937408447\n",
      "Real samples Training loss: 0.4584510326385498\n",
      "Fake samples Training loss: 1.1852202415466309\n",
      "Real samples Training loss: 0.5013474822044373\n",
      "Fake samples Training loss: 1.228597640991211\n",
      "Real samples Training loss: 0.48638397455215454\n",
      "Fake samples Training loss: 1.219815969467163\n",
      "Real samples Training loss: 0.47448980808258057\n",
      "Fake samples Training loss: 1.194562554359436\n",
      "Real samples Training loss: 0.5709832906723022\n",
      "Fake samples Training loss: 1.3645604848861694\n",
      "Real samples Training loss: 0.5568073391914368\n",
      "Fake samples Training loss: 1.0760278701782227\n",
      "Real samples Training loss: 0.49855995178222656\n",
      "Fake samples Training loss: 1.2555214166641235\n",
      "Real samples Training loss: 0.4516116976737976\n",
      "Fake samples Training loss: 1.2255806922912598\n",
      "Real samples Training loss: 0.42727765440940857\n",
      "Fake samples Training loss: 1.2141227722167969\n",
      "Real samples Training loss: 0.46466150879859924\n",
      "Fake samples Training loss: 1.094342589378357\n",
      "Real samples Training loss: 0.44013887643814087\n",
      "Fake samples Training loss: 1.190707802772522\n",
      "Real samples Training loss: 0.49634355306625366\n",
      "Fake samples Training loss: 1.294176459312439\n",
      "Real samples Training loss: 0.48095831274986267\n",
      "Fake samples Training loss: 1.331095814704895\n",
      "Real samples Training loss: 0.46389633417129517\n",
      "Fake samples Training loss: 1.2127890586853027\n",
      "Real samples Training loss: 0.5338369607925415\n",
      "Fake samples Training loss: 1.3661830425262451\n",
      "Real samples Training loss: 0.43950775265693665\n",
      "Fake samples Training loss: 1.2278248071670532\n",
      "Real samples Training loss: 0.5124850869178772\n",
      "Fake samples Training loss: 1.4149725437164307\n",
      "Real samples Training loss: 0.5127702951431274\n",
      "Fake samples Training loss: 1.1767827272415161\n",
      "Real samples Training loss: 0.47769469022750854\n",
      "Fake samples Training loss: 1.3627461194992065\n",
      "Real samples Training loss: 0.5023794174194336\n",
      "Fake samples Training loss: 1.2637298107147217\n",
      "Real samples Training loss: 0.46429890394210815\n",
      "Fake samples Training loss: 1.1591845750808716\n",
      "Real samples Training loss: 0.4687558710575104\n",
      "Fake samples Training loss: 1.342387080192566\n",
      "Real samples Training loss: 0.4578999876976013\n",
      "Fake samples Training loss: 1.1508370637893677\n",
      "Real samples Training loss: 0.4533982276916504\n",
      "Fake samples Training loss: 1.218925952911377\n",
      "Real samples Training loss: 0.5320587158203125\n",
      "Fake samples Training loss: 1.2653831243515015\n",
      "Real samples Training loss: 0.4833550453186035\n",
      "Fake samples Training loss: 1.201357364654541\n",
      "Real samples Training loss: 0.45871731638908386\n",
      "Fake samples Training loss: 1.3353209495544434\n",
      "Real samples Training loss: 0.5124971866607666\n",
      "Fake samples Training loss: 1.255568504333496\n",
      "Real samples Training loss: 0.5372703075408936\n",
      "Fake samples Training loss: 1.160017490386963\n",
      "Real samples Training loss: 0.4660915434360504\n",
      "Fake samples Training loss: 1.1827142238616943\n",
      "Real samples Training loss: 0.5205433964729309\n",
      "Fake samples Training loss: 0.9473414421081543\n",
      "Real samples Training loss: 0.5766835808753967\n",
      "Fake samples Training loss: 1.1575113534927368\n",
      "Real samples Training loss: 0.4632365107536316\n",
      "Fake samples Training loss: 1.2724812030792236\n",
      "Real samples Training loss: 0.4729125201702118\n",
      "Fake samples Training loss: 1.2274807691574097\n",
      "Real samples Training loss: 0.5472732782363892\n",
      "Fake samples Training loss: 1.1972600221633911\n",
      "Real samples Training loss: 0.5124201774597168\n",
      "Fake samples Training loss: 1.1590160131454468\n",
      "Real samples Training loss: 0.45800134539604187\n",
      "Fake samples Training loss: 1.1848177909851074\n",
      "Real samples Training loss: 0.4663960337638855\n",
      "Fake samples Training loss: 1.3673540353775024\n",
      "Real samples Training loss: 0.48268160223960876\n",
      "Fake samples Training loss: 1.1614658832550049\n",
      "Real samples Training loss: 0.4665621221065521\n",
      "Fake samples Training loss: 1.1820799112319946\n",
      "Real samples Training loss: 0.5385337471961975\n",
      "Fake samples Training loss: 1.376936674118042\n",
      "Real samples Training loss: 0.4785550832748413\n",
      "Fake samples Training loss: 1.2063037157058716\n",
      "Real samples Training loss: 0.49365314841270447\n",
      "Fake samples Training loss: 1.15874445438385\n",
      "Real samples Training loss: 0.448943555355072\n",
      "Fake samples Training loss: 1.3101032972335815\n",
      "Real samples Training loss: 0.4464096128940582\n",
      "Fake samples Training loss: 1.2000812292099\n",
      "Real samples Training loss: 0.46130695939064026\n",
      "Fake samples Training loss: 1.2081270217895508\n",
      "Real samples Training loss: 0.45872530341148376\n",
      "Fake samples Training loss: 1.1511352062225342\n",
      "Real samples Training loss: 0.47927507758140564\n",
      "Fake samples Training loss: 1.4224910736083984\n",
      "Real samples Training loss: 0.45255085825920105\n",
      "Fake samples Training loss: 1.2987329959869385\n",
      "Real samples Training loss: 0.5249248743057251\n",
      "Fake samples Training loss: 1.1483999490737915\n",
      "Real samples Training loss: 0.4403735399246216\n",
      "Fake samples Training loss: 1.2699744701385498\n",
      "Real samples Training loss: 0.4492590129375458\n",
      "Fake samples Training loss: 1.1748818159103394\n",
      "Real samples Training loss: 0.4699205458164215\n",
      "Fake samples Training loss: 1.1626850366592407\n",
      "Real samples Training loss: 0.5021547675132751\n",
      "Fake samples Training loss: 1.3347227573394775\n",
      "Real samples Training loss: 0.4635353684425354\n",
      "Fake samples Training loss: 1.351151704788208\n",
      "Real samples Training loss: 0.4915260374546051\n",
      "Fake samples Training loss: 1.259100317955017\n",
      "Real samples Training loss: 0.4239029884338379\n",
      "Fake samples Training loss: 1.2501076459884644\n",
      "Real samples Training loss: 0.4791409373283386\n",
      "Fake samples Training loss: 1.3921208381652832\n",
      "Real samples Training loss: 0.45256122946739197\n",
      "Fake samples Training loss: 1.3220010995864868\n",
      "Real samples Training loss: 0.48612111806869507\n",
      "Fake samples Training loss: 1.291445016860962\n",
      "Real samples Training loss: 0.49249592423439026\n",
      "Fake samples Training loss: 1.1913954019546509\n",
      "Real samples Training loss: 0.4566507637500763\n",
      "Fake samples Training loss: 1.3453123569488525\n",
      "Real samples Training loss: 0.4378480315208435\n",
      "Fake samples Training loss: 1.1507360935211182\n",
      "Real samples Training loss: 0.4816696345806122\n",
      "Fake samples Training loss: 1.2005592584609985\n",
      "Real samples Training loss: 0.508463978767395\n",
      "Fake samples Training loss: 1.0605443716049194\n",
      "Real samples Training loss: 0.46591877937316895\n",
      "Fake samples Training loss: 1.3684768676757812\n",
      "Real samples Training loss: 0.4698319435119629\n",
      "Fake samples Training loss: 1.1599459648132324\n",
      "Real samples Training loss: 0.48464110493659973\n",
      "Fake samples Training loss: 1.2471319437026978\n",
      "Real samples Training loss: 0.424813836812973\n",
      "Fake samples Training loss: 1.38621187210083\n",
      "Real samples Training loss: 0.5373473167419434\n",
      "Fake samples Training loss: 1.2854052782058716\n",
      "Real samples Training loss: 0.4410320818424225\n",
      "Fake samples Training loss: 1.1562608480453491\n",
      "Real samples Training loss: 0.4947986900806427\n",
      "Fake samples Training loss: 1.1687703132629395\n",
      "Real samples Training loss: 0.47997602820396423\n",
      "Fake samples Training loss: 1.3055468797683716\n",
      "Real samples Training loss: 0.47331053018569946\n",
      "Fake samples Training loss: 1.1566548347473145\n",
      "Real samples Training loss: 0.4740036725997925\n",
      "Fake samples Training loss: 1.219337821006775\n",
      "Real samples Training loss: 0.46792396903038025\n",
      "Fake samples Training loss: 1.2036545276641846\n",
      "Real samples Training loss: 0.4720604419708252\n",
      "Fake samples Training loss: 1.366729736328125\n",
      "Real samples Training loss: 0.5198820233345032\n",
      "Fake samples Training loss: 1.2654393911361694\n",
      "Real samples Training loss: 0.5075711011886597\n",
      "Fake samples Training loss: 1.1528481245040894\n",
      "Real samples Training loss: 0.4816269278526306\n",
      "Fake samples Training loss: 1.2535722255706787\n",
      "Real samples Training loss: 0.5047836899757385\n",
      "Fake samples Training loss: 1.1401050090789795\n",
      "Real samples Training loss: 0.4052540063858032\n",
      "Fake samples Training loss: 1.2311468124389648\n",
      "Real samples Training loss: 0.5911532044410706\n",
      "Fake samples Training loss: 1.2471592426300049\n",
      "Real samples Training loss: 0.49375730752944946\n",
      "Fake samples Training loss: 1.1725316047668457\n",
      "Real samples Training loss: 0.45853501558303833\n",
      "Fake samples Training loss: 1.1739869117736816\n",
      "Real samples Training loss: 0.4122307002544403\n",
      "Fake samples Training loss: 1.2398428916931152\n",
      "Real samples Training loss: 0.46146512031555176\n",
      "Fake samples Training loss: 1.3320900201797485\n",
      "Real samples Training loss: 0.5690138339996338\n",
      "Fake samples Training loss: 1.3586273193359375\n",
      "Real samples Training loss: 0.4992389678955078\n",
      "Fake samples Training loss: 1.296169400215149\n",
      "Real samples Training loss: 0.47603604197502136\n",
      "Fake samples Training loss: 1.261275291442871\n",
      "Real samples Training loss: 0.4391399323940277\n",
      "Fake samples Training loss: 1.3702960014343262\n",
      "Real samples Training loss: 0.5117319226264954\n",
      "Fake samples Training loss: 1.1406793594360352\n",
      "Real samples Training loss: 0.496891587972641\n",
      "Fake samples Training loss: 1.2092055082321167\n",
      "Real samples Training loss: 0.42901524901390076\n",
      "Fake samples Training loss: 1.1951674222946167\n",
      "Real samples Training loss: 0.5002521276473999\n",
      "Fake samples Training loss: 1.3284467458724976\n",
      "Real samples Training loss: 0.48319733142852783\n",
      "Fake samples Training loss: 1.2549153566360474\n",
      "Real samples Training loss: 0.5372325778007507\n",
      "Fake samples Training loss: 1.3243879079818726\n",
      "Real samples Training loss: 0.505535364151001\n",
      "Fake samples Training loss: 1.3872265815734863\n",
      "Real samples Training loss: 0.42079633474349976\n",
      "Fake samples Training loss: 1.25217604637146\n",
      "Real samples Training loss: 0.4682684540748596\n",
      "Fake samples Training loss: 1.2513251304626465\n",
      "Real samples Training loss: 0.4524073600769043\n",
      "Fake samples Training loss: 1.3236709833145142\n",
      "Real samples Training loss: 0.44226908683776855\n",
      "Fake samples Training loss: 1.3030776977539062\n",
      "Real samples Training loss: 0.5239465236663818\n",
      "Fake samples Training loss: 1.3414080142974854\n",
      "Real samples Training loss: 0.5600565671920776\n",
      "Fake samples Training loss: 1.2768281698226929\n",
      "Real samples Training loss: 0.46248841285705566\n",
      "Fake samples Training loss: 1.2255908250808716\n",
      "Real samples Training loss: 0.4823656380176544\n",
      "Fake samples Training loss: 1.2093771696090698\n",
      "Real samples Training loss: 0.5671225786209106\n",
      "Fake samples Training loss: 1.1179522275924683\n",
      "Real samples Training loss: 0.4699881374835968\n",
      "Fake samples Training loss: 1.3122551441192627\n",
      "Real samples Training loss: 0.4085814952850342\n",
      "Fake samples Training loss: 1.5023385286331177\n",
      "Real samples Training loss: 0.5002797842025757\n",
      "Fake samples Training loss: 1.2206776142120361\n",
      "Real samples Training loss: 0.4420204162597656\n",
      "Fake samples Training loss: 1.1624619960784912\n",
      "Real samples Training loss: 0.4638647139072418\n",
      "Fake samples Training loss: 1.2053799629211426\n",
      "Real samples Training loss: 0.4919850528240204\n",
      "Fake samples Training loss: 1.1398828029632568\n",
      "Real samples Training loss: 0.5201521515846252\n",
      "Fake samples Training loss: 1.2821071147918701\n",
      "Real samples Training loss: 0.4783267080783844\n",
      "Fake samples Training loss: 1.409820795059204\n",
      "Real samples Training loss: 0.45534998178482056\n",
      "Fake samples Training loss: 1.1209402084350586\n",
      "Real samples Training loss: 0.48675337433815\n",
      "Fake samples Training loss: 1.282592535018921\n",
      "Real samples Training loss: 0.48328012228012085\n",
      "Fake samples Training loss: 1.1542774438858032\n",
      "Real samples Training loss: 0.464796781539917\n",
      "Fake samples Training loss: 1.1936390399932861\n",
      "Real samples Training loss: 0.5208405256271362\n",
      "Fake samples Training loss: 1.2243027687072754\n",
      "Real samples Training loss: 0.5020502209663391\n",
      "Fake samples Training loss: 1.1520999670028687\n",
      "Real samples Training loss: 0.5152322053909302\n",
      "Fake samples Training loss: 1.327954888343811\n",
      "Real samples Training loss: 0.5098099708557129\n",
      "Fake samples Training loss: 1.2421026229858398\n",
      "Real samples Training loss: 0.4700123965740204\n",
      "Fake samples Training loss: 1.2897709608078003\n",
      "Real samples Training loss: 0.505378246307373\n",
      "Fake samples Training loss: 1.320328712463379\n",
      "Real samples Training loss: 0.5149003863334656\n",
      "Fake samples Training loss: 1.4912937879562378\n",
      "Real samples Training loss: 0.4963054358959198\n",
      "Fake samples Training loss: 1.2424453496932983\n",
      "Real samples Training loss: 0.45456549525260925\n",
      "Fake samples Training loss: 1.3397693634033203\n",
      "Real samples Training loss: 0.42177483439445496\n",
      "Fake samples Training loss: 1.1436357498168945\n",
      "Real samples Training loss: 0.5066193342208862\n",
      "Fake samples Training loss: 1.1604074239730835\n",
      "Real samples Training loss: 0.5485562682151794\n",
      "Fake samples Training loss: 1.1076843738555908\n",
      "Real samples Training loss: 0.4740578830242157\n",
      "Fake samples Training loss: 1.260073184967041\n",
      "Real samples Training loss: 0.4381471574306488\n",
      "Fake samples Training loss: 1.200571894645691\n",
      "Real samples Training loss: 0.5024903416633606\n",
      "Fake samples Training loss: 1.2591650485992432\n",
      "Real samples Training loss: 0.4703538417816162\n",
      "Fake samples Training loss: 1.1947340965270996\n",
      "Real samples Training loss: 0.3971095681190491\n",
      "Fake samples Training loss: 1.2139812707901\n",
      "Real samples Training loss: 0.4794861376285553\n",
      "Fake samples Training loss: 1.3646585941314697\n",
      "Real samples Training loss: 0.46902140974998474\n",
      "Fake samples Training loss: 1.2469463348388672\n",
      "Real samples Training loss: 0.5365291237831116\n",
      "Fake samples Training loss: 1.0829817056655884\n",
      "Real samples Training loss: 0.41876038908958435\n",
      "Fake samples Training loss: 1.2340384721755981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.47779715061187744\n",
      "Fake samples Training loss: 1.2808232307434082\n",
      "Real samples Training loss: 0.4740827679634094\n",
      "Fake samples Training loss: 1.2096697092056274\n",
      "Real samples Training loss: 0.5002649426460266\n",
      "Fake samples Training loss: 1.2929435968399048\n",
      "Real samples Training loss: 0.4891529381275177\n",
      "Fake samples Training loss: 1.2975142002105713\n",
      "Real samples Training loss: 0.4145019054412842\n",
      "Fake samples Training loss: 1.1690689325332642\n",
      "Real samples Training loss: 0.4726288616657257\n",
      "Fake samples Training loss: 1.1824476718902588\n",
      "Real samples Training loss: 0.4659959375858307\n",
      "Fake samples Training loss: 1.3511486053466797\n",
      "Real samples Training loss: 0.48760366439819336\n",
      "Fake samples Training loss: 1.0964720249176025\n",
      "Real samples Training loss: 0.47095704078674316\n",
      "Fake samples Training loss: 1.18824303150177\n",
      "Real samples Training loss: 0.4801654517650604\n",
      "Fake samples Training loss: 1.2436249256134033\n",
      "Real samples Training loss: 0.5126186013221741\n",
      "Fake samples Training loss: 1.2392606735229492\n",
      "Real samples Training loss: 0.5229509472846985\n",
      "Fake samples Training loss: 1.2433665990829468\n",
      "Real samples Training loss: 0.49377602338790894\n",
      "Fake samples Training loss: 1.1620310544967651\n",
      "Real samples Training loss: 0.45166999101638794\n",
      "Fake samples Training loss: 1.2111446857452393\n",
      "Real samples Training loss: 0.43950018286705017\n",
      "Fake samples Training loss: 1.098325252532959\n",
      "Real samples Training loss: 0.4440382719039917\n",
      "Fake samples Training loss: 1.2439568042755127\n",
      "Real samples Training loss: 0.5095447897911072\n",
      "Fake samples Training loss: 1.237822413444519\n",
      "Real samples Training loss: 0.39909598231315613\n",
      "Fake samples Training loss: 1.170545220375061\n",
      "Real samples Training loss: 0.46826282143592834\n",
      "Fake samples Training loss: 1.261911392211914\n",
      "Real samples Training loss: 0.49109572172164917\n",
      "Fake samples Training loss: 1.2084448337554932\n",
      "Real samples Training loss: 0.45966672897338867\n",
      "Fake samples Training loss: 1.2889878749847412\n",
      "Real samples Training loss: 0.4526018500328064\n",
      "Fake samples Training loss: 1.3589322566986084\n",
      "Real samples Training loss: 0.47849947214126587\n",
      "Fake samples Training loss: 1.355936884880066\n",
      "Real samples Training loss: 0.499067485332489\n",
      "Fake samples Training loss: 1.1724662780761719\n",
      "Real samples Training loss: 0.47885170578956604\n",
      "Fake samples Training loss: 1.3207603693008423\n",
      "Real samples Training loss: 0.5306929349899292\n",
      "Fake samples Training loss: 1.1894797086715698\n",
      "Real samples Training loss: 0.4858437478542328\n",
      "Fake samples Training loss: 1.3933900594711304\n",
      "Real samples Training loss: 0.522805392742157\n",
      "Fake samples Training loss: 1.2151107788085938\n",
      "Real samples Training loss: 0.4913299083709717\n",
      "Fake samples Training loss: 1.157753825187683\n",
      "Real samples Training loss: 0.5211942791938782\n",
      "Fake samples Training loss: 1.0479918718338013\n",
      "Real samples Training loss: 0.5203469395637512\n",
      "Fake samples Training loss: 1.2824443578720093\n",
      "Real samples Training loss: 0.43385252356529236\n",
      "Fake samples Training loss: 1.2373322248458862\n",
      "Real samples Training loss: 0.4966188669204712\n",
      "Fake samples Training loss: 1.295214295387268\n",
      "Real samples Training loss: 0.48427823185920715\n",
      "Fake samples Training loss: 1.276086449623108\n",
      "Real samples Training loss: 0.5419636368751526\n",
      "Fake samples Training loss: 1.3758959770202637\n",
      "Real samples Training loss: 0.529779851436615\n",
      "Fake samples Training loss: 1.3726497888565063\n",
      "Real samples Training loss: 0.516692042350769\n",
      "Fake samples Training loss: 1.2010468244552612\n",
      "Real samples Training loss: 0.4976353347301483\n",
      "Fake samples Training loss: 1.0987263917922974\n",
      "Real samples Training loss: 0.4692753851413727\n",
      "Fake samples Training loss: 1.2837069034576416\n",
      "Real samples Training loss: 0.4803730845451355\n",
      "Fake samples Training loss: 1.1054961681365967\n",
      "Real samples Training loss: 0.492301344871521\n",
      "Fake samples Training loss: 1.271868109703064\n",
      "Real samples Training loss: 0.4698411524295807\n",
      "Fake samples Training loss: 1.2395687103271484\n",
      "Real samples Training loss: 0.4644300937652588\n",
      "Fake samples Training loss: 1.3076653480529785\n",
      "Real samples Training loss: 0.5606914162635803\n",
      "Fake samples Training loss: 1.2404741048812866\n",
      "Real samples Training loss: 0.41828957200050354\n",
      "Fake samples Training loss: 1.3871601819992065\n",
      "Real samples Training loss: 0.4231742322444916\n",
      "Fake samples Training loss: 1.179496169090271\n",
      "Real samples Training loss: 0.49813342094421387\n",
      "Fake samples Training loss: 1.155867338180542\n",
      "Real samples Training loss: 0.4771636128425598\n",
      "Fake samples Training loss: 1.306449055671692\n",
      "Real samples Training loss: 0.5412452220916748\n",
      "Fake samples Training loss: 1.242087960243225\n",
      "Real samples Training loss: 0.4909062087535858\n",
      "Fake samples Training loss: 1.1787878274917603\n",
      "Real samples Training loss: 0.4941273033618927\n",
      "Fake samples Training loss: 1.2525689601898193\n",
      "Real samples Training loss: 0.4785810708999634\n",
      "Fake samples Training loss: 1.325982689857483\n",
      "Real samples Training loss: 0.48011887073516846\n",
      "Fake samples Training loss: 1.175997257232666\n",
      "Real samples Training loss: 0.42175906896591187\n",
      "Fake samples Training loss: 1.2284421920776367\n",
      "Real samples Training loss: 0.532193124294281\n",
      "Fake samples Training loss: 1.2887382507324219\n",
      "Real samples Training loss: 0.47961312532424927\n",
      "Fake samples Training loss: 1.311409592628479\n",
      "Real samples Training loss: 0.5134836435317993\n",
      "Fake samples Training loss: 1.2646294832229614\n",
      "Real samples Training loss: 0.49560075998306274\n",
      "Fake samples Training loss: 1.331346869468689\n",
      "Real samples Training loss: 0.4884382486343384\n",
      "Fake samples Training loss: 1.3190867900848389\n",
      "Real samples Training loss: 0.5137796998023987\n",
      "Fake samples Training loss: 1.2108666896820068\n",
      "Real samples Training loss: 0.5047721862792969\n",
      "Fake samples Training loss: 1.2013700008392334\n",
      "Real samples Training loss: 0.4488542973995209\n",
      "Fake samples Training loss: 1.2343038320541382\n",
      "Real samples Training loss: 0.4211006462574005\n",
      "Fake samples Training loss: 1.5886247158050537\n",
      "Real samples Training loss: 0.49319928884506226\n",
      "Fake samples Training loss: 1.1595525741577148\n",
      "Real samples Training loss: 0.4400656521320343\n",
      "Fake samples Training loss: 1.289354920387268\n",
      "Real samples Training loss: 0.4696750044822693\n",
      "Fake samples Training loss: 1.3345969915390015\n",
      "Real samples Training loss: 0.46227559447288513\n",
      "Fake samples Training loss: 1.15738046169281\n",
      "Real samples Training loss: 0.4906395673751831\n",
      "Fake samples Training loss: 1.2927743196487427\n",
      "Real samples Training loss: 0.4149380028247833\n",
      "Fake samples Training loss: 1.2530006170272827\n",
      "Real samples Training loss: 0.5150812268257141\n",
      "Fake samples Training loss: 1.1932928562164307\n",
      "Real samples Training loss: 0.5077260732650757\n",
      "Fake samples Training loss: 1.4691256284713745\n",
      "Real samples Training loss: 0.4526212811470032\n",
      "Fake samples Training loss: 1.3299530744552612\n",
      "Real samples Training loss: 0.46590346097946167\n",
      "Fake samples Training loss: 1.2151683568954468\n",
      "Real samples Training loss: 0.514567494392395\n",
      "Fake samples Training loss: 1.2192018032073975\n",
      "Real samples Training loss: 0.4564443528652191\n",
      "Fake samples Training loss: 1.3357423543930054\n",
      "Real samples Training loss: 0.4910738468170166\n",
      "Fake samples Training loss: 1.2886080741882324\n",
      "Real samples Training loss: 0.4706261157989502\n",
      "Fake samples Training loss: 1.369506597518921\n",
      "Real samples Training loss: 0.44026094675064087\n",
      "Fake samples Training loss: 1.2532801628112793\n",
      "Real samples Training loss: 0.5030863881111145\n",
      "Fake samples Training loss: 1.1595827341079712\n",
      "Real samples Training loss: 0.441557377576828\n",
      "Fake samples Training loss: 1.20518159866333\n",
      "Real samples Training loss: 0.5046432018280029\n",
      "Fake samples Training loss: 1.286130666732788\n",
      "Real samples Training loss: 0.4853111803531647\n",
      "Fake samples Training loss: 1.3418790102005005\n",
      "Real samples Training loss: 0.47543996572494507\n",
      "Fake samples Training loss: 1.0651723146438599\n",
      "Real samples Training loss: 0.49488839507102966\n",
      "Fake samples Training loss: 1.1944286823272705\n",
      "Real samples Training loss: 0.5034560561180115\n",
      "Fake samples Training loss: 1.285815715789795\n",
      "Real samples Training loss: 0.47787922620773315\n",
      "Fake samples Training loss: 1.2459710836410522\n",
      "Real samples Training loss: 0.5640591979026794\n",
      "Fake samples Training loss: 1.278245449066162\n",
      "Real samples Training loss: 0.4821240305900574\n",
      "Fake samples Training loss: 1.1847941875457764\n",
      "Real samples Training loss: 0.5090529322624207\n",
      "Fake samples Training loss: 1.1758215427398682\n",
      "Real samples Training loss: 0.4587814509868622\n",
      "Fake samples Training loss: 1.2719687223434448\n",
      "Real samples Training loss: 0.46288007497787476\n",
      "Fake samples Training loss: 1.0332852602005005\n",
      "Real samples Training loss: 0.47191375494003296\n",
      "Fake samples Training loss: 1.3015791177749634\n",
      "Real samples Training loss: 0.4688315689563751\n",
      "Fake samples Training loss: 1.1788854598999023\n",
      "Real samples Training loss: 0.4720120131969452\n",
      "Fake samples Training loss: 1.2871427536010742\n",
      "Real samples Training loss: 0.45980241894721985\n",
      "Fake samples Training loss: 1.2777585983276367\n",
      "Real samples Training loss: 0.5457034111022949\n",
      "Fake samples Training loss: 1.2440035343170166\n",
      "Real samples Training loss: 0.3723292946815491\n",
      "Fake samples Training loss: 1.2563062906265259\n",
      "Real samples Training loss: 0.4576806426048279\n",
      "Fake samples Training loss: 1.23027765750885\n",
      "Real samples Training loss: 0.5521929860115051\n",
      "Fake samples Training loss: 1.0228755474090576\n",
      "Real samples Training loss: 0.47090286016464233\n",
      "Fake samples Training loss: 1.3717589378356934\n",
      "Real samples Training loss: 0.5005080699920654\n",
      "Fake samples Training loss: 1.3607161045074463\n",
      "Real samples Training loss: 0.4883502721786499\n",
      "Fake samples Training loss: 1.2082223892211914\n",
      "Real samples Training loss: 0.49816977977752686\n",
      "Fake samples Training loss: 1.3741341829299927\n",
      "Real samples Training loss: 0.48243898153305054\n",
      "Fake samples Training loss: 1.202441930770874\n",
      "Real samples Training loss: 0.461397647857666\n",
      "Fake samples Training loss: 1.3109676837921143\n",
      "Real samples Training loss: 0.40752890706062317\n",
      "Fake samples Training loss: 1.16658616065979\n",
      "Real samples Training loss: 0.5496928691864014\n",
      "Fake samples Training loss: 1.2920688390731812\n",
      "Real samples Training loss: 0.5704773664474487\n",
      "Fake samples Training loss: 1.3146436214447021\n",
      "Real samples Training loss: 0.42885175347328186\n",
      "Fake samples Training loss: 1.2620912790298462\n",
      "Real samples Training loss: 0.5028491020202637\n",
      "Fake samples Training loss: 1.3098328113555908\n",
      "Real samples Training loss: 0.47585028409957886\n",
      "Fake samples Training loss: 1.1471575498580933\n",
      "Real samples Training loss: 0.49027249217033386\n",
      "Fake samples Training loss: 1.2179367542266846\n",
      "Real samples Training loss: 0.4198821783065796\n",
      "Fake samples Training loss: 1.3088291883468628\n",
      "Real samples Training loss: 0.47357797622680664\n",
      "Fake samples Training loss: 1.0424394607543945\n",
      "Real samples Training loss: 0.4135620594024658\n",
      "Fake samples Training loss: 1.234141230583191\n",
      "Real samples Training loss: 0.446646124124527\n",
      "Fake samples Training loss: 1.4872783422470093\n",
      "Real samples Training loss: 0.46796953678131104\n",
      "Fake samples Training loss: 1.2243928909301758\n",
      "Real samples Training loss: 0.5059508681297302\n",
      "Fake samples Training loss: 1.1328407526016235\n",
      "Real samples Training loss: 0.45977193117141724\n",
      "Fake samples Training loss: 1.2952325344085693\n",
      "Real samples Training loss: 0.5185835361480713\n",
      "Fake samples Training loss: 1.3507660627365112\n",
      "Real samples Training loss: 0.4618726968765259\n",
      "Fake samples Training loss: 1.3196446895599365\n",
      "Real samples Training loss: 0.4506308138370514\n",
      "Fake samples Training loss: 1.1226826906204224\n",
      "Real samples Training loss: 0.4631021320819855\n",
      "Fake samples Training loss: 1.131947636604309\n",
      "Real samples Training loss: 0.46936142444610596\n",
      "Fake samples Training loss: 1.26790452003479\n",
      "Real samples Training loss: 0.442659854888916\n",
      "Fake samples Training loss: 1.207021713256836\n",
      "Real samples Training loss: 0.48027876019477844\n",
      "Fake samples Training loss: 1.1570255756378174\n",
      "Real samples Training loss: 0.446317583322525\n",
      "Fake samples Training loss: 1.2650057077407837\n",
      "Real samples Training loss: 0.466966450214386\n",
      "Fake samples Training loss: 1.2528879642486572\n",
      "Real samples Training loss: 0.4991455674171448\n",
      "Fake samples Training loss: 1.287039875984192\n",
      "Real samples Training loss: 0.5217108726501465\n",
      "Fake samples Training loss: 1.2500882148742676\n",
      "Real samples Training loss: 0.5113134980201721\n",
      "Fake samples Training loss: 1.2814552783966064\n",
      "Real samples Training loss: 0.5095105171203613\n",
      "Fake samples Training loss: 1.2724555730819702\n",
      "Real samples Training loss: 0.5424016118049622\n",
      "Fake samples Training loss: 1.1960768699645996\n",
      "Real samples Training loss: 0.46698060631752014\n",
      "Fake samples Training loss: 1.1858915090560913\n",
      "Real samples Training loss: 0.4571589231491089\n",
      "Fake samples Training loss: 1.2798768281936646\n",
      "Real samples Training loss: 0.4679570198059082\n",
      "Fake samples Training loss: 1.2111434936523438\n",
      "Real samples Training loss: 0.5050259828567505\n",
      "Fake samples Training loss: 1.175279140472412\n",
      "Real samples Training loss: 0.5024540424346924\n",
      "Fake samples Training loss: 1.1947120428085327\n",
      "Real samples Training loss: 0.5369796752929688\n",
      "Fake samples Training loss: 1.1202908754348755\n",
      "Real samples Training loss: 0.47995954751968384\n",
      "Fake samples Training loss: 1.2593441009521484\n",
      "Real samples Training loss: 0.445707231760025\n",
      "Fake samples Training loss: 1.0731980800628662\n",
      "Real samples Training loss: 0.5012310147285461\n",
      "Fake samples Training loss: 1.3025025129318237\n",
      "Real samples Training loss: 0.4911389946937561\n",
      "Fake samples Training loss: 1.2217918634414673\n",
      "Real samples Training loss: 0.4284243881702423\n",
      "Fake samples Training loss: 1.300146460533142\n",
      "Real samples Training loss: 0.5125212669372559\n",
      "Fake samples Training loss: 1.2728561162948608\n",
      "Real samples Training loss: 0.5100394487380981\n",
      "Fake samples Training loss: 1.3311491012573242\n",
      "Real samples Training loss: 0.44271767139434814\n",
      "Fake samples Training loss: 1.2353531122207642\n",
      "Real samples Training loss: 0.4611000418663025\n",
      "Fake samples Training loss: 1.356968879699707\n",
      "Real samples Training loss: 0.45058199763298035\n",
      "Fake samples Training loss: 1.2729209661483765\n",
      "Real samples Training loss: 0.4729591906070709\n",
      "Fake samples Training loss: 1.1543387174606323\n",
      "Real samples Training loss: 0.4559139609336853\n",
      "Fake samples Training loss: 1.2819583415985107\n",
      "Real samples Training loss: 0.46193447709083557\n",
      "Fake samples Training loss: 1.1563689708709717\n",
      "Real samples Training loss: 0.48330366611480713\n",
      "Fake samples Training loss: 1.1626302003860474\n",
      "Real samples Training loss: 0.5097679495811462\n",
      "Fake samples Training loss: 1.3617850542068481\n",
      "Real samples Training loss: 0.4817139804363251\n",
      "Fake samples Training loss: 1.3085869550704956\n",
      "Real samples Training loss: 0.5198091268539429\n",
      "Fake samples Training loss: 1.2030344009399414\n",
      "Real samples Training loss: 0.4645140767097473\n",
      "Fake samples Training loss: 1.2204095125198364\n",
      "Real samples Training loss: 0.4921368956565857\n",
      "Fake samples Training loss: 1.1557289361953735\n",
      "Real samples Training loss: 0.41779205203056335\n",
      "Fake samples Training loss: 1.3263018131256104\n",
      "Real samples Training loss: 0.48154327273368835\n",
      "Fake samples Training loss: 1.1923561096191406\n",
      "Real samples Training loss: 0.437975138425827\n",
      "Fake samples Training loss: 1.1577531099319458\n",
      "Real samples Training loss: 0.46865561604499817\n",
      "Fake samples Training loss: 1.2829259634017944\n",
      "Real samples Training loss: 0.5201030373573303\n",
      "Fake samples Training loss: 1.2879410982131958\n",
      "Real samples Training loss: 0.46935397386550903\n",
      "Fake samples Training loss: 1.1950064897537231\n",
      "Real samples Training loss: 0.46846115589141846\n",
      "Fake samples Training loss: 1.2261943817138672\n",
      "Real samples Training loss: 0.4488436281681061\n",
      "Fake samples Training loss: 1.2167127132415771\n",
      "Real samples Training loss: 0.48411279916763306\n",
      "Fake samples Training loss: 1.2879775762557983\n",
      "Real samples Training loss: 0.5024993419647217\n",
      "Fake samples Training loss: 1.2489405870437622\n",
      "Real samples Training loss: 0.481201708316803\n",
      "Fake samples Training loss: 1.144865870475769\n",
      "Real samples Training loss: 0.5608135461807251\n",
      "Fake samples Training loss: 1.0524373054504395\n",
      "Real samples Training loss: 0.5039647221565247\n",
      "Fake samples Training loss: 1.3625929355621338\n",
      "Real samples Training loss: 0.5103628039360046\n",
      "Fake samples Training loss: 1.15738844871521\n",
      "Real samples Training loss: 0.5148606896400452\n",
      "Fake samples Training loss: 1.2533479928970337\n",
      "Real samples Training loss: 0.44112879037857056\n",
      "Fake samples Training loss: 1.2374720573425293\n",
      "Real samples Training loss: 0.44322654604911804\n",
      "Fake samples Training loss: 1.2769744396209717\n",
      "Real samples Training loss: 0.4717622995376587\n",
      "Fake samples Training loss: 1.3412806987762451\n",
      "Real samples Training loss: 0.4925944209098816\n",
      "Fake samples Training loss: 1.2793391942977905\n",
      "Real samples Training loss: 0.47404900193214417\n",
      "Fake samples Training loss: 1.2074240446090698\n",
      "Real samples Training loss: 0.4646635055541992\n",
      "Fake samples Training loss: 1.3079488277435303\n",
      "Real samples Training loss: 0.41738155484199524\n",
      "Fake samples Training loss: 1.2204153537750244\n",
      "Real samples Training loss: 0.4542858302593231\n",
      "Fake samples Training loss: 1.2871721982955933\n",
      "Real samples Training loss: 0.4817904829978943\n",
      "Fake samples Training loss: 1.3405870199203491\n",
      "Real samples Training loss: 0.5313200950622559\n",
      "Fake samples Training loss: 1.4351379871368408\n",
      "Real samples Training loss: 0.5432069897651672\n",
      "Fake samples Training loss: 1.2350990772247314\n",
      "Real samples Training loss: 0.5098021626472473\n",
      "Fake samples Training loss: 1.2901705503463745\n",
      "Real samples Training loss: 0.4646030366420746\n",
      "Fake samples Training loss: 1.2988011837005615\n",
      "Real samples Training loss: 0.4535055458545685\n",
      "Fake samples Training loss: 1.242926836013794\n",
      "Real samples Training loss: 0.5010412335395813\n",
      "Fake samples Training loss: 1.2251300811767578\n",
      "Real samples Training loss: 0.47076427936553955\n",
      "Fake samples Training loss: 1.182730793952942\n",
      "Real samples Training loss: 0.5266582369804382\n",
      "Fake samples Training loss: 1.1480212211608887\n",
      "Real samples Training loss: 0.5636228322982788\n",
      "Fake samples Training loss: 1.2460256814956665\n",
      "Real samples Training loss: 0.46667343378067017\n",
      "Fake samples Training loss: 1.1861538887023926\n",
      "Real samples Training loss: 0.45065292716026306\n",
      "Fake samples Training loss: 1.4238345623016357\n",
      "Real samples Training loss: 0.38163429498672485\n",
      "Fake samples Training loss: 1.1428940296173096\n",
      "Real samples Training loss: 0.5155450701713562\n",
      "Fake samples Training loss: 1.2070720195770264\n",
      "Real samples Training loss: 0.3925354480743408\n",
      "Fake samples Training loss: 1.319832682609558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.557274580001831\n",
      "Fake samples Training loss: 1.226719856262207\n",
      "Real samples Training loss: 0.44394510984420776\n",
      "Fake samples Training loss: 1.1744263172149658\n",
      "Real samples Training loss: 0.4932575523853302\n",
      "Fake samples Training loss: 1.3844138383865356\n",
      "Real samples Training loss: 0.48791295289993286\n",
      "Fake samples Training loss: 1.0988250970840454\n",
      "Real samples Training loss: 0.5102052688598633\n",
      "Fake samples Training loss: 1.069177508354187\n",
      "Real samples Training loss: 0.4673663377761841\n",
      "Fake samples Training loss: 1.25159752368927\n",
      "Real samples Training loss: 0.5312507748603821\n",
      "Fake samples Training loss: 1.3075616359710693\n",
      "Real samples Training loss: 0.5543920397758484\n",
      "Fake samples Training loss: 1.0620665550231934\n",
      "Real samples Training loss: 0.4959753453731537\n",
      "Fake samples Training loss: 1.3465276956558228\n",
      "Real samples Training loss: 0.483873188495636\n",
      "Fake samples Training loss: 1.1932356357574463\n",
      "Real samples Training loss: 0.515789806842804\n",
      "Fake samples Training loss: 1.3122968673706055\n",
      "Real samples Training loss: 0.524493932723999\n",
      "Fake samples Training loss: 1.1959404945373535\n",
      "Real samples Training loss: 0.4476490318775177\n",
      "Fake samples Training loss: 1.3005359172821045\n",
      "Real samples Training loss: 0.4814240038394928\n",
      "Fake samples Training loss: 1.2198797464370728\n",
      "Real samples Training loss: 0.5149168968200684\n",
      "Fake samples Training loss: 1.236068844795227\n",
      "Real samples Training loss: 0.5020947456359863\n",
      "Fake samples Training loss: 1.253630518913269\n",
      "Real samples Training loss: 0.4622386395931244\n",
      "Fake samples Training loss: 1.2453796863555908\n",
      "Real samples Training loss: 0.5519672632217407\n",
      "Fake samples Training loss: 1.2937973737716675\n",
      "Real samples Training loss: 0.4847366213798523\n",
      "Fake samples Training loss: 1.1904029846191406\n",
      "Real samples Training loss: 0.5014182329177856\n",
      "Fake samples Training loss: 1.3502107858657837\n",
      "Real samples Training loss: 0.5124152302742004\n",
      "Fake samples Training loss: 1.2125515937805176\n",
      "Real samples Training loss: 0.5078150629997253\n",
      "Fake samples Training loss: 1.3737941980361938\n",
      "Real samples Training loss: 0.42646706104278564\n",
      "Fake samples Training loss: 1.1485122442245483\n",
      "Real samples Training loss: 0.4671819508075714\n",
      "Fake samples Training loss: 1.2505149841308594\n",
      "Real samples Training loss: 0.4663914442062378\n",
      "Fake samples Training loss: 1.3804454803466797\n",
      "Real samples Training loss: 0.4973887801170349\n",
      "Fake samples Training loss: 1.0899778604507446\n",
      "Real samples Training loss: 0.5006330013275146\n",
      "Fake samples Training loss: 1.2886425256729126\n",
      "Real samples Training loss: 0.45388123393058777\n",
      "Fake samples Training loss: 1.166627287864685\n",
      "Real samples Training loss: 0.4995990991592407\n",
      "Fake samples Training loss: 1.2824554443359375\n",
      "Real samples Training loss: 0.48417267203330994\n",
      "Fake samples Training loss: 1.2219241857528687\n",
      "Real samples Training loss: 0.5070538520812988\n",
      "Fake samples Training loss: 1.2328613996505737\n",
      "Real samples Training loss: 0.5203538537025452\n",
      "Fake samples Training loss: 1.2683608531951904\n",
      "Real samples Training loss: 0.4792127013206482\n",
      "Fake samples Training loss: 1.2957026958465576\n",
      "Real samples Training loss: 0.45334339141845703\n",
      "Fake samples Training loss: 1.176618218421936\n",
      "Real samples Training loss: 0.4612763524055481\n",
      "Fake samples Training loss: 1.2345384359359741\n",
      "Real samples Training loss: 0.5068746209144592\n",
      "Fake samples Training loss: 1.1752028465270996\n",
      "Real samples Training loss: 0.5088348984718323\n",
      "Fake samples Training loss: 1.2587921619415283\n",
      "Real samples Training loss: 0.4418604373931885\n",
      "Fake samples Training loss: 1.3202781677246094\n",
      "Real samples Training loss: 0.49485859274864197\n",
      "Fake samples Training loss: 1.2552895545959473\n",
      "Real samples Training loss: 0.5009334087371826\n",
      "Fake samples Training loss: 1.1335667371749878\n",
      "Real samples Training loss: 0.4989769458770752\n",
      "Fake samples Training loss: 1.1482481956481934\n",
      "Real samples Training loss: 0.5319573283195496\n",
      "Fake samples Training loss: 1.2375798225402832\n",
      "Real samples Training loss: 0.4502734839916229\n",
      "Fake samples Training loss: 1.3049827814102173\n",
      "Real samples Training loss: 0.4866635203361511\n",
      "Fake samples Training loss: 1.2072832584381104\n",
      "Real samples Training loss: 0.5068003535270691\n",
      "Fake samples Training loss: 1.1300371885299683\n",
      "Real samples Training loss: 0.5001951456069946\n",
      "Fake samples Training loss: 1.1020762920379639\n",
      "Real samples Training loss: 0.44794172048568726\n",
      "Fake samples Training loss: 1.1814700365066528\n",
      "Real samples Training loss: 0.42652609944343567\n",
      "Fake samples Training loss: 1.2572270631790161\n",
      "Real samples Training loss: 0.4388599097728729\n",
      "Fake samples Training loss: 1.263253927230835\n",
      "Real samples Training loss: 0.5389353036880493\n",
      "Fake samples Training loss: 1.083792805671692\n",
      "Real samples Training loss: 0.5056854486465454\n",
      "Fake samples Training loss: 1.272803783416748\n",
      "Real samples Training loss: 0.4367532432079315\n",
      "Fake samples Training loss: 1.318050742149353\n",
      "Real samples Training loss: 0.5364257097244263\n",
      "Fake samples Training loss: 1.2339365482330322\n",
      "Real samples Training loss: 0.548085629940033\n",
      "Fake samples Training loss: 1.314507007598877\n",
      "Real samples Training loss: 0.4515617787837982\n",
      "Fake samples Training loss: 1.1553902626037598\n",
      "Real samples Training loss: 0.5571697950363159\n",
      "Fake samples Training loss: 1.3138781785964966\n",
      "Real samples Training loss: 0.5288942456245422\n",
      "Fake samples Training loss: 1.3390363454818726\n",
      "Real samples Training loss: 0.468172550201416\n",
      "Fake samples Training loss: 1.2826907634735107\n",
      "Real samples Training loss: 0.4719006419181824\n",
      "Fake samples Training loss: 1.323291540145874\n",
      "Real samples Training loss: 0.5025424361228943\n",
      "Fake samples Training loss: 1.2258754968643188\n",
      "Real samples Training loss: 0.48079943656921387\n",
      "Fake samples Training loss: 1.2687855958938599\n",
      "Real samples Training loss: 0.47419553995132446\n",
      "Fake samples Training loss: 1.201882004737854\n",
      "Real samples Training loss: 0.46122556924819946\n",
      "Fake samples Training loss: 1.380778193473816\n",
      "Real samples Training loss: 0.46901935338974\n",
      "Fake samples Training loss: 1.345518946647644\n",
      "Real samples Training loss: 0.45226842164993286\n",
      "Fake samples Training loss: 1.341341495513916\n",
      "Real samples Training loss: 0.5135836005210876\n",
      "Fake samples Training loss: 1.2731629610061646\n",
      "Real samples Training loss: 0.5213934779167175\n",
      "Fake samples Training loss: 1.2157522439956665\n",
      "Real samples Training loss: 0.45844870805740356\n",
      "Fake samples Training loss: 1.193563461303711\n",
      "Real samples Training loss: 0.5421004295349121\n",
      "Fake samples Training loss: 1.1519663333892822\n",
      "Real samples Training loss: 0.4107901155948639\n",
      "Fake samples Training loss: 1.2166955471038818\n",
      "Real samples Training loss: 0.4999210238456726\n",
      "Fake samples Training loss: 1.2587629556655884\n",
      "Real samples Training loss: 0.426922082901001\n",
      "Fake samples Training loss: 1.2228060960769653\n",
      "Real samples Training loss: 0.49952587485313416\n",
      "Fake samples Training loss: 1.2531641721725464\n",
      "Real samples Training loss: 0.4816468060016632\n",
      "Fake samples Training loss: 1.1982990503311157\n",
      "Real samples Training loss: 0.4851035177707672\n",
      "Fake samples Training loss: 1.197341799736023\n",
      "Real samples Training loss: 0.4948427677154541\n",
      "Fake samples Training loss: 1.2266019582748413\n",
      "Real samples Training loss: 0.48800402879714966\n",
      "Fake samples Training loss: 1.2629461288452148\n",
      "Real samples Training loss: 0.45867612957954407\n",
      "Fake samples Training loss: 1.2490999698638916\n",
      "Real samples Training loss: 0.49631044268608093\n",
      "Fake samples Training loss: 1.2642515897750854\n",
      "Real samples Training loss: 0.46684595942497253\n",
      "Fake samples Training loss: 1.3427339792251587\n",
      "Real samples Training loss: 0.49246278405189514\n",
      "Fake samples Training loss: 1.2450377941131592\n",
      "Real samples Training loss: 0.536072850227356\n",
      "Fake samples Training loss: 1.5465675592422485\n",
      "Real samples Training loss: 0.4792693555355072\n",
      "Fake samples Training loss: 1.442115068435669\n",
      "Real samples Training loss: 0.46063756942749023\n",
      "Fake samples Training loss: 1.3611299991607666\n",
      "Real samples Training loss: 0.48116347193717957\n",
      "Fake samples Training loss: 1.203256607055664\n",
      "Real samples Training loss: 0.4776770770549774\n",
      "Fake samples Training loss: 1.1752381324768066\n",
      "Real samples Training loss: 0.4500422179698944\n",
      "Fake samples Training loss: 1.324912190437317\n",
      "Real samples Training loss: 0.41892027854919434\n",
      "Fake samples Training loss: 1.183876872062683\n",
      "Real samples Training loss: 0.466329962015152\n",
      "Fake samples Training loss: 1.128570318222046\n",
      "Real samples Training loss: 0.4327844977378845\n",
      "Fake samples Training loss: 1.254028558731079\n",
      "Real samples Training loss: 0.4050764739513397\n",
      "Fake samples Training loss: 1.1708203554153442\n",
      "Real samples Training loss: 0.49064865708351135\n",
      "Fake samples Training loss: 1.2896630764007568\n",
      "Real samples Training loss: 0.48891541361808777\n",
      "Fake samples Training loss: 1.222517967224121\n",
      "Real samples Training loss: 0.48069220781326294\n",
      "Fake samples Training loss: 1.1719540357589722\n",
      "Real samples Training loss: 0.49754437804222107\n",
      "Fake samples Training loss: 1.282414197921753\n",
      "Real samples Training loss: 0.43389490246772766\n",
      "Fake samples Training loss: 1.450047492980957\n",
      "Real samples Training loss: 0.48548194766044617\n",
      "Fake samples Training loss: 1.266402244567871\n",
      "Real samples Training loss: 0.45103511214256287\n",
      "Fake samples Training loss: 1.2243191003799438\n",
      "Real samples Training loss: 0.4965464174747467\n",
      "Fake samples Training loss: 1.3237299919128418\n",
      "Real samples Training loss: 0.4584411084651947\n",
      "Fake samples Training loss: 1.2527897357940674\n",
      "Real samples Training loss: 0.4863376319408417\n",
      "Fake samples Training loss: 1.2018189430236816\n",
      "Real samples Training loss: 0.5441690683364868\n",
      "Fake samples Training loss: 1.4124462604522705\n",
      "Real samples Training loss: 0.43940356373786926\n",
      "Fake samples Training loss: 1.393157720565796\n",
      "Real samples Training loss: 0.48561716079711914\n",
      "Fake samples Training loss: 1.2049665451049805\n",
      "Real samples Training loss: 0.5285938382148743\n",
      "Fake samples Training loss: 1.2442811727523804\n",
      "Real samples Training loss: 0.5054619908332825\n",
      "Fake samples Training loss: 1.2430615425109863\n",
      "Real samples Training loss: 0.4580084979534149\n",
      "Fake samples Training loss: 1.188559651374817\n",
      "Real samples Training loss: 0.5563827753067017\n",
      "Fake samples Training loss: 1.2940192222595215\n",
      "Real samples Training loss: 0.548288881778717\n",
      "Fake samples Training loss: 1.3077102899551392\n",
      "Real samples Training loss: 0.4372524619102478\n",
      "Fake samples Training loss: 1.2997478246688843\n",
      "Real samples Training loss: 0.4855159521102905\n",
      "Fake samples Training loss: 1.2974964380264282\n",
      "Real samples Training loss: 0.4058842658996582\n",
      "Fake samples Training loss: 1.222514271736145\n",
      "Real samples Training loss: 0.48308464884757996\n",
      "Fake samples Training loss: 1.2060637474060059\n",
      "Real samples Training loss: 0.4977782666683197\n",
      "Fake samples Training loss: 1.2550911903381348\n",
      "Real samples Training loss: 0.5026179552078247\n",
      "Fake samples Training loss: 1.0761371850967407\n",
      "Real samples Training loss: 0.4547264873981476\n",
      "Fake samples Training loss: 1.121167778968811\n",
      "Real samples Training loss: 0.44843432307243347\n",
      "Fake samples Training loss: 1.0663236379623413\n",
      "Real samples Training loss: 0.5138283371925354\n",
      "Fake samples Training loss: 1.2013412714004517\n",
      "Real samples Training loss: 0.484328031539917\n",
      "Fake samples Training loss: 1.4784713983535767\n",
      "Real samples Training loss: 0.45359665155410767\n",
      "Fake samples Training loss: 1.283315896987915\n",
      "Real samples Training loss: 0.49589431285858154\n",
      "Fake samples Training loss: 1.1836023330688477\n",
      "Real samples Training loss: 0.47817888855934143\n",
      "Fake samples Training loss: 1.1876921653747559\n",
      "Real samples Training loss: 0.4661150276660919\n",
      "Fake samples Training loss: 1.126940369606018\n",
      "Real samples Training loss: 0.5002161264419556\n",
      "Fake samples Training loss: 1.3912739753723145\n",
      "Real samples Training loss: 0.42875099182128906\n",
      "Fake samples Training loss: 1.2765324115753174\n",
      "Real samples Training loss: 0.45509272813796997\n",
      "Fake samples Training loss: 1.2313035726547241\n",
      "Real samples Training loss: 0.5040326714515686\n",
      "Fake samples Training loss: 1.2221134901046753\n",
      "Real samples Training loss: 0.4939398169517517\n",
      "Fake samples Training loss: 1.251548171043396\n",
      "Real samples Training loss: 0.4889164865016937\n",
      "Fake samples Training loss: 1.2586653232574463\n",
      "Real samples Training loss: 0.4451996088027954\n",
      "Fake samples Training loss: 1.2132134437561035\n",
      "Real samples Training loss: 0.471766859292984\n",
      "Fake samples Training loss: 1.2160402536392212\n",
      "Real samples Training loss: 0.4505690932273865\n",
      "Fake samples Training loss: 1.4237792491912842\n",
      "Real samples Training loss: 0.4879302382469177\n",
      "Fake samples Training loss: 1.2196464538574219\n",
      "Real samples Training loss: 0.54405277967453\n",
      "Fake samples Training loss: 1.2691816091537476\n",
      "Real samples Training loss: 0.4767306447029114\n",
      "Fake samples Training loss: 1.0921201705932617\n",
      "Real samples Training loss: 0.4388886094093323\n",
      "Fake samples Training loss: 1.0251541137695312\n",
      "Real samples Training loss: 0.5072695016860962\n",
      "Fake samples Training loss: 1.2880311012268066\n",
      "Real samples Training loss: 0.5251516103744507\n",
      "Fake samples Training loss: 1.1711605787277222\n",
      "Real samples Training loss: 0.4445042312145233\n",
      "Fake samples Training loss: 1.3729183673858643\n",
      "Real samples Training loss: 0.4670872390270233\n",
      "Fake samples Training loss: 1.3242024183273315\n",
      "Real samples Training loss: 0.5113033652305603\n",
      "Fake samples Training loss: 1.2884249687194824\n",
      "Real samples Training loss: 0.4260631203651428\n",
      "Fake samples Training loss: 1.2092562913894653\n",
      "Real samples Training loss: 0.4683164060115814\n",
      "Fake samples Training loss: 1.3459532260894775\n",
      "Real samples Training loss: 0.48381954431533813\n",
      "Fake samples Training loss: 1.2374227046966553\n",
      "Real samples Training loss: 0.521608293056488\n",
      "Fake samples Training loss: 1.2039262056350708\n",
      "Real samples Training loss: 0.5061885714530945\n",
      "Fake samples Training loss: 1.2347159385681152\n",
      "Real samples Training loss: 0.40006330609321594\n",
      "Fake samples Training loss: 1.2865513563156128\n",
      "Real samples Training loss: 0.5045693516731262\n",
      "Fake samples Training loss: 1.1902519464492798\n",
      "Real samples Training loss: 0.514654815196991\n",
      "Fake samples Training loss: 1.2772574424743652\n",
      "Real samples Training loss: 0.4168309271335602\n",
      "Fake samples Training loss: 1.0637822151184082\n",
      "Real samples Training loss: 0.4705876111984253\n",
      "Fake samples Training loss: 1.32636559009552\n",
      "Real samples Training loss: 0.48423394560813904\n",
      "Fake samples Training loss: 1.1847424507141113\n",
      "Real samples Training loss: 0.45169326663017273\n",
      "Fake samples Training loss: 1.2335138320922852\n",
      "Real samples Training loss: 0.4574916362762451\n",
      "Fake samples Training loss: 1.2359098196029663\n",
      "Real samples Training loss: 0.49027618765830994\n",
      "Fake samples Training loss: 1.3434959650039673\n",
      "Real samples Training loss: 0.5480763912200928\n",
      "Fake samples Training loss: 1.2418901920318604\n",
      "Real samples Training loss: 0.5275037288665771\n",
      "Fake samples Training loss: 1.326624870300293\n",
      "Real samples Training loss: 0.5008610486984253\n",
      "Fake samples Training loss: 1.4143123626708984\n",
      "Real samples Training loss: 0.4644140601158142\n",
      "Fake samples Training loss: 1.327234148979187\n",
      "Real samples Training loss: 0.4476817846298218\n",
      "Fake samples Training loss: 1.2422958612442017\n",
      "Real samples Training loss: 0.45403698086738586\n",
      "Fake samples Training loss: 1.1917827129364014\n",
      "Real samples Training loss: 0.526280403137207\n",
      "Fake samples Training loss: 1.3635557889938354\n",
      "Real samples Training loss: 0.4987395405769348\n",
      "Fake samples Training loss: 1.2065112590789795\n",
      "Real samples Training loss: 0.525586724281311\n",
      "Fake samples Training loss: 1.3195888996124268\n",
      "Real samples Training loss: 0.4345850944519043\n",
      "Fake samples Training loss: 1.1855144500732422\n",
      "Real samples Training loss: 0.4632464051246643\n",
      "Fake samples Training loss: 1.2555298805236816\n",
      "Real samples Training loss: 0.4680939316749573\n",
      "Fake samples Training loss: 1.263510823249817\n",
      "Real samples Training loss: 0.5338309407234192\n",
      "Fake samples Training loss: 1.306304693222046\n",
      "Real samples Training loss: 0.5381782054901123\n",
      "Fake samples Training loss: 1.3586986064910889\n",
      "Real samples Training loss: 0.5266699194908142\n",
      "Fake samples Training loss: 1.1483772993087769\n",
      "Real samples Training loss: 0.4543999433517456\n",
      "Fake samples Training loss: 1.3331832885742188\n",
      "Real samples Training loss: 0.4834213852882385\n",
      "Fake samples Training loss: 1.2191400527954102\n",
      "Real samples Training loss: 0.4912746548652649\n",
      "Fake samples Training loss: 1.247023344039917\n",
      "Real samples Training loss: 0.49807146191596985\n",
      "Fake samples Training loss: 1.28357994556427\n",
      "Real samples Training loss: 0.506057620048523\n",
      "Fake samples Training loss: 1.1750245094299316\n",
      "Real samples Training loss: 0.3878764808177948\n",
      "Fake samples Training loss: 1.2224442958831787\n",
      "Real samples Training loss: 0.4670572280883789\n",
      "Fake samples Training loss: 1.1609865427017212\n",
      "Real samples Training loss: 0.4837571382522583\n",
      "Fake samples Training loss: 1.2646831274032593\n",
      "Real samples Training loss: 0.5213294625282288\n",
      "Fake samples Training loss: 1.3276102542877197\n",
      "Real samples Training loss: 0.4395991265773773\n",
      "Fake samples Training loss: 1.1955766677856445\n",
      "Real samples Training loss: 0.47342440485954285\n",
      "Fake samples Training loss: 1.3148521184921265\n",
      "Real samples Training loss: 0.5080502033233643\n",
      "Fake samples Training loss: 1.3589085340499878\n",
      "Real samples Training loss: 0.5092251300811768\n",
      "Fake samples Training loss: 1.3011656999588013\n",
      "Real samples Training loss: 0.4379124641418457\n",
      "Fake samples Training loss: 1.2051602602005005\n",
      "Real samples Training loss: 0.49236950278282166\n",
      "Fake samples Training loss: 1.1975897550582886\n",
      "Real samples Training loss: 0.4495477080345154\n",
      "Fake samples Training loss: 1.0448576211929321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.41681233048439026\n",
      "Fake samples Training loss: 1.3366881608963013\n",
      "Real samples Training loss: 0.4531225562095642\n",
      "Fake samples Training loss: 1.2217310667037964\n",
      "Real samples Training loss: 0.4667012393474579\n",
      "Fake samples Training loss: 1.232901692390442\n",
      "Real samples Training loss: 0.4616532027721405\n",
      "Fake samples Training loss: 1.1982862949371338\n",
      "Real samples Training loss: 0.43264174461364746\n",
      "Fake samples Training loss: 1.2120401859283447\n",
      "Real samples Training loss: 0.5099674463272095\n",
      "Fake samples Training loss: 1.131259560585022\n",
      "Real samples Training loss: 0.4776438772678375\n",
      "Fake samples Training loss: 1.2112473249435425\n",
      "Real samples Training loss: 0.5375850200653076\n",
      "Fake samples Training loss: 1.2436844110488892\n",
      "Real samples Training loss: 0.44246503710746765\n",
      "Fake samples Training loss: 1.2899467945098877\n",
      "Real samples Training loss: 0.49885547161102295\n",
      "Fake samples Training loss: 1.2751392126083374\n",
      "Real samples Training loss: 0.5133713483810425\n",
      "Fake samples Training loss: 1.3374027013778687\n",
      "Real samples Training loss: 0.45281514525413513\n",
      "Fake samples Training loss: 1.334363579750061\n",
      "Real samples Training loss: 0.4593592882156372\n",
      "Fake samples Training loss: 1.1928471326828003\n",
      "Real samples Training loss: 0.4264431297779083\n",
      "Fake samples Training loss: 1.1669375896453857\n",
      "Real samples Training loss: 0.5164594054222107\n",
      "Fake samples Training loss: 1.2857544422149658\n",
      "Real samples Training loss: 0.4652036130428314\n",
      "Fake samples Training loss: 1.1114102602005005\n",
      "Real samples Training loss: 0.4835171401500702\n",
      "Fake samples Training loss: 1.2331522703170776\n",
      "Real samples Training loss: 0.5082468390464783\n",
      "Fake samples Training loss: 1.1716266870498657\n",
      "Real samples Training loss: 0.5019496083259583\n",
      "Fake samples Training loss: 1.1586588621139526\n",
      "Real samples Training loss: 0.5325452089309692\n",
      "Fake samples Training loss: 1.196018099784851\n",
      "Real samples Training loss: 0.42430001497268677\n",
      "Fake samples Training loss: 1.2151519060134888\n",
      "Real samples Training loss: 0.44986385107040405\n",
      "Fake samples Training loss: 1.3624063730239868\n",
      "Real samples Training loss: 0.47807055711746216\n",
      "Fake samples Training loss: 1.1346418857574463\n",
      "Real samples Training loss: 0.4609188437461853\n",
      "Fake samples Training loss: 1.3193094730377197\n",
      "Real samples Training loss: 0.5440506935119629\n",
      "Fake samples Training loss: 1.1831083297729492\n",
      "Real samples Training loss: 0.5471778512001038\n",
      "Fake samples Training loss: 1.2317478656768799\n",
      "Real samples Training loss: 0.46010729670524597\n",
      "Fake samples Training loss: 1.3018969297409058\n",
      "Real samples Training loss: 0.46796536445617676\n",
      "Fake samples Training loss: 1.1731464862823486\n",
      "Real samples Training loss: 0.5087897777557373\n",
      "Fake samples Training loss: 1.3566912412643433\n",
      "Real samples Training loss: 0.49509790539741516\n",
      "Fake samples Training loss: 1.2029314041137695\n",
      "Real samples Training loss: 0.5156164169311523\n",
      "Fake samples Training loss: 1.2996150255203247\n",
      "Real samples Training loss: 0.5428104996681213\n",
      "Fake samples Training loss: 1.2216929197311401\n",
      "Real samples Training loss: 0.46031105518341064\n",
      "Fake samples Training loss: 1.3070579767227173\n",
      "Real samples Training loss: 0.509780764579773\n",
      "Fake samples Training loss: 1.177363395690918\n",
      "Real samples Training loss: 0.5214824676513672\n",
      "Fake samples Training loss: 1.247481107711792\n",
      "Real samples Training loss: 0.4827688932418823\n",
      "Fake samples Training loss: 1.1976534128189087\n",
      "Real samples Training loss: 0.49558761715888977\n",
      "Fake samples Training loss: 1.3858880996704102\n",
      "Real samples Training loss: 0.47505706548690796\n",
      "Fake samples Training loss: 1.1679887771606445\n",
      "Real samples Training loss: 0.46253702044487\n",
      "Fake samples Training loss: 1.0981227159500122\n",
      "Real samples Training loss: 0.46874991059303284\n",
      "Fake samples Training loss: 1.1695337295532227\n",
      "Real samples Training loss: 0.4657788872718811\n",
      "Fake samples Training loss: 1.289955496788025\n",
      "Real samples Training loss: 0.5037689208984375\n",
      "Fake samples Training loss: 1.0870825052261353\n",
      "Real samples Training loss: 0.42735278606414795\n",
      "Fake samples Training loss: 1.1593509912490845\n",
      "Real samples Training loss: 0.5170115828514099\n",
      "Fake samples Training loss: 1.2832914590835571\n",
      "Real samples Training loss: 0.542470395565033\n",
      "Fake samples Training loss: 1.4307277202606201\n",
      "Real samples Training loss: 0.459042489528656\n",
      "Fake samples Training loss: 1.1904046535491943\n",
      "Real samples Training loss: 0.4902496635913849\n",
      "Fake samples Training loss: 1.3390308618545532\n",
      "Real samples Training loss: 0.464809387922287\n",
      "Fake samples Training loss: 1.1899601221084595\n",
      "Real samples Training loss: 0.46641281247138977\n",
      "Fake samples Training loss: 1.3176859617233276\n",
      "Real samples Training loss: 0.5171496272087097\n",
      "Fake samples Training loss: 1.1162135601043701\n",
      "Real samples Training loss: 0.528897225856781\n",
      "Fake samples Training loss: 1.2423979043960571\n",
      "Real samples Training loss: 0.4870174825191498\n",
      "Fake samples Training loss: 1.198944330215454\n",
      "Real samples Training loss: 0.4599383473396301\n",
      "Fake samples Training loss: 1.175310730934143\n",
      "Real samples Training loss: 0.5045720338821411\n",
      "Fake samples Training loss: 1.3352553844451904\n",
      "Real samples Training loss: 0.4788086414337158\n",
      "Fake samples Training loss: 1.3411283493041992\n",
      "Real samples Training loss: 0.4782632887363434\n",
      "Fake samples Training loss: 1.2230943441390991\n",
      "Real samples Training loss: 0.4586901068687439\n",
      "Fake samples Training loss: 1.1800751686096191\n",
      "Real samples Training loss: 0.44161978363990784\n",
      "Fake samples Training loss: 1.1839324235916138\n",
      "Real samples Training loss: 0.44894134998321533\n",
      "Fake samples Training loss: 1.3162074089050293\n",
      "Real samples Training loss: 0.44519269466400146\n",
      "Fake samples Training loss: 1.2143720388412476\n",
      "Real samples Training loss: 0.5679541230201721\n",
      "Fake samples Training loss: 1.1499282121658325\n",
      "Real samples Training loss: 0.49639278650283813\n",
      "Fake samples Training loss: 1.317926287651062\n",
      "Real samples Training loss: 0.4432124197483063\n",
      "Fake samples Training loss: 1.313893437385559\n",
      "Real samples Training loss: 0.4141106605529785\n",
      "Fake samples Training loss: 1.1700628995895386\n",
      "Real samples Training loss: 0.4472801685333252\n",
      "Fake samples Training loss: 1.2495819330215454\n",
      "Real samples Training loss: 0.5065323710441589\n",
      "Fake samples Training loss: 1.3550996780395508\n",
      "Real samples Training loss: 0.47583308815956116\n",
      "Fake samples Training loss: 1.2075821161270142\n",
      "Real samples Training loss: 0.49070149660110474\n",
      "Fake samples Training loss: 1.3221367597579956\n",
      "Real samples Training loss: 0.49767741560935974\n",
      "Fake samples Training loss: 1.054155945777893\n",
      "Real samples Training loss: 0.5009703636169434\n",
      "Fake samples Training loss: 1.375959873199463\n",
      "Real samples Training loss: 0.4723431169986725\n",
      "Fake samples Training loss: 1.2135839462280273\n",
      "Real samples Training loss: 0.4581860899925232\n",
      "Fake samples Training loss: 1.0906872749328613\n",
      "Real samples Training loss: 0.48915600776672363\n",
      "Fake samples Training loss: 1.2683961391448975\n",
      "Real samples Training loss: 0.4620582163333893\n",
      "Fake samples Training loss: 1.1775356531143188\n",
      "Real samples Training loss: 0.5245574712753296\n",
      "Fake samples Training loss: 1.2515168190002441\n",
      "Real samples Training loss: 0.5422226190567017\n",
      "Fake samples Training loss: 1.2144255638122559\n",
      "Real samples Training loss: 0.48356521129608154\n",
      "Fake samples Training loss: 1.3083652257919312\n",
      "Real samples Training loss: 0.48722320795059204\n",
      "Fake samples Training loss: 1.1950905323028564\n",
      "Real samples Training loss: 0.46165987849235535\n",
      "Fake samples Training loss: 1.3204280138015747\n",
      "Real samples Training loss: 0.4625801742076874\n",
      "Fake samples Training loss: 1.3544775247573853\n",
      "Real samples Training loss: 0.542686402797699\n",
      "Fake samples Training loss: 1.1555593013763428\n",
      "Real samples Training loss: 0.5305742621421814\n",
      "Fake samples Training loss: 1.196563959121704\n",
      "Real samples Training loss: 0.5189903974533081\n",
      "Fake samples Training loss: 1.4655076265335083\n",
      "Real samples Training loss: 0.49125128984451294\n",
      "Fake samples Training loss: 1.2660048007965088\n",
      "Real samples Training loss: 0.5263939499855042\n",
      "Fake samples Training loss: 1.2889615297317505\n",
      "Real samples Training loss: 0.4366386830806732\n",
      "Fake samples Training loss: 1.3824552297592163\n",
      "Real samples Training loss: 0.46400004625320435\n",
      "Fake samples Training loss: 1.2113029956817627\n",
      "Real samples Training loss: 0.48932549357414246\n",
      "Fake samples Training loss: 1.2236889600753784\n",
      "Real samples Training loss: 0.5261168479919434\n",
      "Fake samples Training loss: 1.3297048807144165\n",
      "Real samples Training loss: 0.47784632444381714\n",
      "Fake samples Training loss: 1.1202641725540161\n",
      "Real samples Training loss: 0.47548767924308777\n",
      "Fake samples Training loss: 1.2372682094573975\n",
      "Real samples Training loss: 0.4729711413383484\n",
      "Fake samples Training loss: 1.2728683948516846\n",
      "Real samples Training loss: 0.4364676773548126\n",
      "Fake samples Training loss: 1.1882127523422241\n",
      "Real samples Training loss: 0.5138653516769409\n",
      "Fake samples Training loss: 1.2167518138885498\n",
      "Real samples Training loss: 0.5049389600753784\n",
      "Fake samples Training loss: 1.2631617784500122\n",
      "Real samples Training loss: 0.42807260155677795\n",
      "Fake samples Training loss: 1.185165524482727\n",
      "Real samples Training loss: 0.5272290706634521\n",
      "Fake samples Training loss: 1.1327800750732422\n",
      "Real samples Training loss: 0.5018351078033447\n",
      "Fake samples Training loss: 1.3434655666351318\n",
      "Real samples Training loss: 0.45486435294151306\n",
      "Fake samples Training loss: 1.2550137042999268\n",
      "Real samples Training loss: 0.4570513069629669\n",
      "Fake samples Training loss: 1.079958200454712\n",
      "Real samples Training loss: 0.4567307233810425\n",
      "Fake samples Training loss: 1.255976915359497\n",
      "Real samples Training loss: 0.421283096075058\n",
      "Fake samples Training loss: 1.1891001462936401\n",
      "Real samples Training loss: 0.4881404638290405\n",
      "Fake samples Training loss: 1.1703641414642334\n",
      "Real samples Training loss: 0.4726591110229492\n",
      "Fake samples Training loss: 1.2247616052627563\n",
      "Real samples Training loss: 0.44676852226257324\n",
      "Fake samples Training loss: 1.3003348112106323\n",
      "Real samples Training loss: 0.445425420999527\n",
      "Fake samples Training loss: 1.0978294610977173\n",
      "Real samples Training loss: 0.4651280641555786\n",
      "Fake samples Training loss: 1.2560930252075195\n",
      "Real samples Training loss: 0.5266579985618591\n",
      "Fake samples Training loss: 1.3449183702468872\n",
      "Real samples Training loss: 0.43930310010910034\n",
      "Fake samples Training loss: 1.1941723823547363\n",
      "Real samples Training loss: 0.5041170120239258\n",
      "Fake samples Training loss: 1.3414791822433472\n",
      "Real samples Training loss: 0.45667514204978943\n",
      "Fake samples Training loss: 1.2588011026382446\n",
      "Real samples Training loss: 0.436443030834198\n",
      "Fake samples Training loss: 1.3853044509887695\n",
      "Real samples Training loss: 0.46153953671455383\n",
      "Fake samples Training loss: 1.4386062622070312\n",
      "Real samples Training loss: 0.5075334310531616\n",
      "Fake samples Training loss: 1.265915036201477\n",
      "Real samples Training loss: 0.4950677454471588\n",
      "Fake samples Training loss: 1.161731481552124\n",
      "Real samples Training loss: 0.5623347759246826\n",
      "Fake samples Training loss: 1.1668710708618164\n",
      "Real samples Training loss: 0.4682300090789795\n",
      "Fake samples Training loss: 1.3947781324386597\n",
      "Real samples Training loss: 0.5045686364173889\n",
      "Fake samples Training loss: 1.3052704334259033\n",
      "Real samples Training loss: 0.4520779848098755\n",
      "Fake samples Training loss: 1.2389134168624878\n",
      "Real samples Training loss: 0.5322937369346619\n",
      "Fake samples Training loss: 1.3190656900405884\n",
      "Real samples Training loss: 0.4943966567516327\n",
      "Fake samples Training loss: 1.1504971981048584\n",
      "Real samples Training loss: 0.504961371421814\n",
      "Fake samples Training loss: 1.3381794691085815\n",
      "Real samples Training loss: 0.4520682990550995\n",
      "Fake samples Training loss: 1.4068297147750854\n",
      "Real samples Training loss: 0.4944206476211548\n",
      "Fake samples Training loss: 1.214776635169983\n",
      "Real samples Training loss: 0.4424422085285187\n",
      "Fake samples Training loss: 1.2058274745941162\n",
      "Real samples Training loss: 0.4614869952201843\n",
      "Fake samples Training loss: 1.1478047370910645\n",
      "Real samples Training loss: 0.5238946676254272\n",
      "Fake samples Training loss: 1.2233525514602661\n",
      "Real samples Training loss: 0.48990094661712646\n",
      "Fake samples Training loss: 1.2050073146820068\n",
      "Real samples Training loss: 0.46100562810897827\n",
      "Fake samples Training loss: 1.258785605430603\n",
      "Real samples Training loss: 0.47585728764533997\n",
      "Fake samples Training loss: 1.1368228197097778\n",
      "Real samples Training loss: 0.4678162634372711\n",
      "Fake samples Training loss: 1.2684075832366943\n",
      "Real samples Training loss: 0.4793596565723419\n",
      "Fake samples Training loss: 1.3120087385177612\n",
      "Real samples Training loss: 0.5127130150794983\n",
      "Fake samples Training loss: 1.2754271030426025\n",
      "Real samples Training loss: 0.43521639704704285\n",
      "Fake samples Training loss: 1.2268387079238892\n",
      "Real samples Training loss: 0.4764888286590576\n",
      "Fake samples Training loss: 1.3070957660675049\n",
      "Real samples Training loss: 0.47789931297302246\n",
      "Fake samples Training loss: 1.231216311454773\n",
      "Real samples Training loss: 0.4477989971637726\n",
      "Fake samples Training loss: 1.248123049736023\n",
      "Real samples Training loss: 0.4558013677597046\n",
      "Fake samples Training loss: 1.2544946670532227\n",
      "Real samples Training loss: 0.44631484150886536\n",
      "Fake samples Training loss: 1.227310061454773\n",
      "Real samples Training loss: 0.46769291162490845\n",
      "Fake samples Training loss: 1.2088083028793335\n",
      "Real samples Training loss: 0.4889701306819916\n",
      "Fake samples Training loss: 1.3274457454681396\n",
      "Real samples Training loss: 0.48628464341163635\n",
      "Fake samples Training loss: 1.2102333307266235\n",
      "Real samples Training loss: 0.4220173954963684\n",
      "Fake samples Training loss: 1.1555519104003906\n",
      "Real samples Training loss: 0.5345585942268372\n",
      "Fake samples Training loss: 1.1324235200881958\n",
      "Real samples Training loss: 0.5175144672393799\n",
      "Fake samples Training loss: 1.2542685270309448\n",
      "Real samples Training loss: 0.5142130255699158\n",
      "Fake samples Training loss: 1.3292189836502075\n",
      "Real samples Training loss: 0.5134720802307129\n",
      "Fake samples Training loss: 1.34831964969635\n",
      "Real samples Training loss: 0.5197832584381104\n",
      "Fake samples Training loss: 1.296478033065796\n",
      "Real samples Training loss: 0.4446253478527069\n",
      "Fake samples Training loss: 1.3653007745742798\n",
      "Real samples Training loss: 0.5476856231689453\n",
      "Fake samples Training loss: 1.232305884361267\n",
      "Real samples Training loss: 0.43915316462516785\n",
      "Fake samples Training loss: 1.345664143562317\n",
      "Real samples Training loss: 0.4384058117866516\n",
      "Fake samples Training loss: 1.2934355735778809\n",
      "Real samples Training loss: 0.5010482668876648\n",
      "Fake samples Training loss: 1.352079153060913\n",
      "Real samples Training loss: 0.523478090763092\n",
      "Fake samples Training loss: 1.23282790184021\n",
      "Real samples Training loss: 0.4878060519695282\n",
      "Fake samples Training loss: 1.2958459854125977\n",
      "Real samples Training loss: 0.568488597869873\n",
      "Fake samples Training loss: 1.1179945468902588\n",
      "Real samples Training loss: 0.4274452030658722\n",
      "Fake samples Training loss: 1.329877495765686\n",
      "Real samples Training loss: 0.5020650625228882\n",
      "Fake samples Training loss: 1.244310975074768\n",
      "Real samples Training loss: 0.4993324875831604\n",
      "Fake samples Training loss: 1.1256392002105713\n",
      "Real samples Training loss: 0.524777352809906\n",
      "Fake samples Training loss: 1.3751590251922607\n",
      "Real samples Training loss: 0.5241711735725403\n",
      "Fake samples Training loss: 1.104325294494629\n",
      "Real samples Training loss: 0.496584951877594\n",
      "Fake samples Training loss: 1.331131100654602\n",
      "Real samples Training loss: 0.4954261779785156\n",
      "Fake samples Training loss: 1.1961148977279663\n",
      "Real samples Training loss: 0.44235095381736755\n",
      "Fake samples Training loss: 1.2163819074630737\n",
      "Real samples Training loss: 0.4572424292564392\n",
      "Fake samples Training loss: 1.4745910167694092\n",
      "Real samples Training loss: 0.46327972412109375\n",
      "Fake samples Training loss: 1.4094310998916626\n",
      "Real samples Training loss: 0.4410962462425232\n",
      "Fake samples Training loss: 1.231946587562561\n",
      "Real samples Training loss: 0.508083701133728\n",
      "Fake samples Training loss: 1.2544926404953003\n",
      "Real samples Training loss: 0.5181604027748108\n",
      "Fake samples Training loss: 1.1206493377685547\n",
      "Real samples Training loss: 0.5113909244537354\n",
      "Fake samples Training loss: 1.2158236503601074\n",
      "Real samples Training loss: 0.561784029006958\n",
      "Fake samples Training loss: 1.327194333076477\n",
      "Real samples Training loss: 0.527651309967041\n",
      "Fake samples Training loss: 1.4716973304748535\n",
      "Real samples Training loss: 0.4936327338218689\n",
      "Fake samples Training loss: 1.288425087928772\n",
      "Real samples Training loss: 0.4580649137496948\n",
      "Fake samples Training loss: 1.211930274963379\n",
      "Real samples Training loss: 0.5242038369178772\n",
      "Fake samples Training loss: 1.2578761577606201\n",
      "Real samples Training loss: 0.5053017735481262\n",
      "Fake samples Training loss: 1.240972638130188\n",
      "Real samples Training loss: 0.47092437744140625\n",
      "Fake samples Training loss: 1.180956482887268\n",
      "Real samples Training loss: 0.504884660243988\n",
      "Fake samples Training loss: 1.2676880359649658\n",
      "Real samples Training loss: 0.5182976126670837\n",
      "Fake samples Training loss: 1.3665786981582642\n",
      "Real samples Training loss: 0.4806858003139496\n",
      "Fake samples Training loss: 1.1598902940750122\n",
      "Real samples Training loss: 0.5063811540603638\n",
      "Fake samples Training loss: 1.3519604206085205\n",
      "Real samples Training loss: 0.44749143719673157\n",
      "Fake samples Training loss: 1.1109460592269897\n",
      "Real samples Training loss: 0.4892479479312897\n",
      "Fake samples Training loss: 1.1077710390090942\n",
      "Real samples Training loss: 0.4601762890815735\n",
      "Fake samples Training loss: 1.3989044427871704\n",
      "Real samples Training loss: 0.4652389883995056\n",
      "Fake samples Training loss: 1.138447642326355\n",
      "Real samples Training loss: 0.47017139196395874\n",
      "Fake samples Training loss: 1.0232890844345093\n",
      "Real samples Training loss: 0.46853941679000854\n",
      "Fake samples Training loss: 1.221558928489685\n",
      "Real samples Training loss: 0.46562522649765015\n",
      "Fake samples Training loss: 1.2641404867172241\n",
      "Real samples Training loss: 0.41315674781799316\n",
      "Fake samples Training loss: 1.148622751235962\n",
      "Real samples Training loss: 0.5248304009437561\n",
      "Fake samples Training loss: 1.3011168241500854\n",
      "Real samples Training loss: 0.5136309862136841\n",
      "Fake samples Training loss: 1.3314722776412964\n",
      "Real samples Training loss: 0.44521695375442505\n",
      "Fake samples Training loss: 1.1849201917648315\n",
      "Real samples Training loss: 0.49039173126220703\n",
      "Fake samples Training loss: 1.2471669912338257\n",
      "Real samples Training loss: 0.4937487840652466\n",
      "Fake samples Training loss: 1.4251405000686646\n",
      "Real samples Training loss: 0.4437572658061981\n",
      "Fake samples Training loss: 1.356209397315979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.38547831773757935\n",
      "Fake samples Training loss: 1.1591230630874634\n",
      "Real samples Training loss: 0.47654908895492554\n",
      "Fake samples Training loss: 1.4377838373184204\n",
      "Real samples Training loss: 0.4331575334072113\n",
      "Fake samples Training loss: 1.231176733970642\n",
      "Real samples Training loss: 0.4765367805957794\n",
      "Fake samples Training loss: 1.1051397323608398\n",
      "Real samples Training loss: 0.49682989716529846\n",
      "Fake samples Training loss: 1.2849785089492798\n",
      "Real samples Training loss: 0.5201462507247925\n",
      "Fake samples Training loss: 1.2400367259979248\n",
      "Real samples Training loss: 0.5350037217140198\n",
      "Fake samples Training loss: 1.316571831703186\n",
      "Real samples Training loss: 0.4903305470943451\n",
      "Fake samples Training loss: 1.2694706916809082\n",
      "Real samples Training loss: 0.5480263233184814\n",
      "Fake samples Training loss: 1.1068782806396484\n",
      "Real samples Training loss: 0.5387601852416992\n",
      "Fake samples Training loss: 1.2843842506408691\n",
      "Real samples Training loss: 0.5067285895347595\n",
      "Fake samples Training loss: 1.2747827768325806\n",
      "Real samples Training loss: 0.47117409110069275\n",
      "Fake samples Training loss: 1.3202844858169556\n",
      "Real samples Training loss: 0.430961936712265\n",
      "Fake samples Training loss: 1.2333306074142456\n",
      "Real samples Training loss: 0.4643703103065491\n",
      "Fake samples Training loss: 1.3105744123458862\n",
      "Real samples Training loss: 0.5029835104942322\n",
      "Fake samples Training loss: 1.2218519449234009\n",
      "Real samples Training loss: 0.514746367931366\n",
      "Fake samples Training loss: 1.2471144199371338\n",
      "Real samples Training loss: 0.459872841835022\n",
      "Fake samples Training loss: 1.2433364391326904\n",
      "Real samples Training loss: 0.4429040551185608\n",
      "Fake samples Training loss: 1.25562584400177\n",
      "Real samples Training loss: 0.476077139377594\n",
      "Fake samples Training loss: 1.316573143005371\n",
      "Real samples Training loss: 0.48002293705940247\n",
      "Fake samples Training loss: 1.1960910558700562\n",
      "Real samples Training loss: 0.4994281232357025\n",
      "Fake samples Training loss: 1.232166051864624\n",
      "Real samples Training loss: 0.5306742191314697\n",
      "Fake samples Training loss: 1.3646094799041748\n",
      "Real samples Training loss: 0.49385473132133484\n",
      "Fake samples Training loss: 1.2475515604019165\n",
      "Real samples Training loss: 0.5128540992736816\n",
      "Fake samples Training loss: 1.319372534751892\n",
      "Real samples Training loss: 0.5082094669342041\n",
      "Fake samples Training loss: 1.2166012525558472\n",
      "Real samples Training loss: 0.488150954246521\n",
      "Fake samples Training loss: 1.175034761428833\n",
      "Real samples Training loss: 0.48558130860328674\n",
      "Fake samples Training loss: 1.3278968334197998\n",
      "Real samples Training loss: 0.4717864692211151\n",
      "Fake samples Training loss: 1.1778676509857178\n",
      "Real samples Training loss: 0.48768049478530884\n",
      "Fake samples Training loss: 1.430055022239685\n",
      "Real samples Training loss: 0.45163336396217346\n",
      "Fake samples Training loss: 1.1602141857147217\n",
      "Real samples Training loss: 0.509972870349884\n",
      "Fake samples Training loss: 1.1922039985656738\n",
      "Real samples Training loss: 0.5447424054145813\n",
      "Fake samples Training loss: 1.171521782875061\n",
      "Real samples Training loss: 0.4996926188468933\n",
      "Fake samples Training loss: 1.2063158750534058\n",
      "Real samples Training loss: 0.49413278698921204\n",
      "Fake samples Training loss: 1.177169919013977\n",
      "Real samples Training loss: 0.4317660629749298\n",
      "Fake samples Training loss: 1.2487155199050903\n",
      "Real samples Training loss: 0.5075591206550598\n",
      "Fake samples Training loss: 1.1920109987258911\n",
      "Real samples Training loss: 0.41134995222091675\n",
      "Fake samples Training loss: 1.290823221206665\n",
      "Real samples Training loss: 0.5274832844734192\n",
      "Fake samples Training loss: 1.304762840270996\n",
      "Real samples Training loss: 0.4505213797092438\n",
      "Fake samples Training loss: 1.238358497619629\n",
      "Real samples Training loss: 0.4786241948604584\n",
      "Fake samples Training loss: 1.4049153327941895\n",
      "Real samples Training loss: 0.4332219362258911\n",
      "Fake samples Training loss: 1.3132812976837158\n",
      "Real samples Training loss: 0.42339786887168884\n",
      "Fake samples Training loss: 1.4101123809814453\n",
      "Real samples Training loss: 0.4369219243526459\n",
      "Fake samples Training loss: 1.1894586086273193\n",
      "Real samples Training loss: 0.5040744543075562\n",
      "Fake samples Training loss: 1.1362073421478271\n",
      "Real samples Training loss: 0.4595275819301605\n",
      "Fake samples Training loss: 1.0346636772155762\n",
      "Real samples Training loss: 0.44393762946128845\n",
      "Fake samples Training loss: 1.288927674293518\n",
      "Real samples Training loss: 0.46027040481567383\n",
      "Fake samples Training loss: 1.4863756895065308\n",
      "Real samples Training loss: 0.43844982981681824\n",
      "Fake samples Training loss: 1.2784349918365479\n",
      "Real samples Training loss: 0.4763365387916565\n",
      "Fake samples Training loss: 1.3765015602111816\n",
      "Real samples Training loss: 0.49635589122772217\n",
      "Fake samples Training loss: 1.307569980621338\n",
      "Real samples Training loss: 0.45965084433555603\n",
      "Fake samples Training loss: 1.3843121528625488\n",
      "Real samples Training loss: 0.4874838888645172\n",
      "Fake samples Training loss: 1.2534297704696655\n",
      "Real samples Training loss: 0.4607030749320984\n",
      "Fake samples Training loss: 1.2636446952819824\n",
      "Real samples Training loss: 0.486191987991333\n",
      "Fake samples Training loss: 1.417718529701233\n",
      "Real samples Training loss: 0.5192069411277771\n",
      "Fake samples Training loss: 1.2375423908233643\n",
      "Real samples Training loss: 0.49018198251724243\n",
      "Fake samples Training loss: 1.2570750713348389\n",
      "Real samples Training loss: 0.49181434512138367\n",
      "Fake samples Training loss: 1.1223151683807373\n",
      "Real samples Training loss: 0.5036770701408386\n",
      "Fake samples Training loss: 1.3306527137756348\n",
      "Real samples Training loss: 0.5374842286109924\n",
      "Fake samples Training loss: 1.2240771055221558\n",
      "Real samples Training loss: 0.49647256731987\n",
      "Fake samples Training loss: 1.263330101966858\n",
      "Real samples Training loss: 0.43286386132240295\n",
      "Fake samples Training loss: 1.2150212526321411\n",
      "Real samples Training loss: 0.436586856842041\n",
      "Fake samples Training loss: 1.134035348892212\n",
      "Real samples Training loss: 0.5437443256378174\n",
      "Fake samples Training loss: 1.1370453834533691\n",
      "Real samples Training loss: 0.45893561840057373\n",
      "Fake samples Training loss: 1.298570156097412\n",
      "Real samples Training loss: 0.5023728013038635\n",
      "Fake samples Training loss: 1.074873447418213\n",
      "Real samples Training loss: 0.5164968371391296\n",
      "Fake samples Training loss: 1.1991536617279053\n",
      "Real samples Training loss: 0.49842989444732666\n",
      "Fake samples Training loss: 1.1967363357543945\n",
      "Real samples Training loss: 0.5047666430473328\n",
      "Fake samples Training loss: 1.2356252670288086\n",
      "Real samples Training loss: 0.4820386469364166\n",
      "Fake samples Training loss: 1.171156644821167\n",
      "Real samples Training loss: 0.46761587262153625\n",
      "Fake samples Training loss: 1.322539210319519\n",
      "Real samples Training loss: 0.5374910235404968\n",
      "Fake samples Training loss: 1.3263732194900513\n",
      "Real samples Training loss: 0.42491263151168823\n",
      "Fake samples Training loss: 1.2038822174072266\n",
      "Real samples Training loss: 0.4782288372516632\n",
      "Fake samples Training loss: 1.249030351638794\n",
      "Real samples Training loss: 0.45164766907691956\n",
      "Fake samples Training loss: 1.0765748023986816\n",
      "Real samples Training loss: 0.48199525475502014\n",
      "Fake samples Training loss: 1.2622621059417725\n",
      "Real samples Training loss: 0.47104084491729736\n",
      "Fake samples Training loss: 1.1114673614501953\n",
      "Real samples Training loss: 0.5181005001068115\n",
      "Fake samples Training loss: 1.1364408731460571\n",
      "Real samples Training loss: 0.40150758624076843\n",
      "Fake samples Training loss: 1.1568758487701416\n",
      "Real samples Training loss: 0.46261388063430786\n",
      "Fake samples Training loss: 1.1993825435638428\n",
      "Real samples Training loss: 0.49133041501045227\n",
      "Fake samples Training loss: 1.2716783285140991\n",
      "Real samples Training loss: 0.43540653586387634\n",
      "Fake samples Training loss: 1.296855092048645\n",
      "Real samples Training loss: 0.5024352669715881\n",
      "Fake samples Training loss: 1.3271104097366333\n",
      "Real samples Training loss: 0.47273385524749756\n",
      "Fake samples Training loss: 1.3351370096206665\n",
      "Real samples Training loss: 0.46308597922325134\n",
      "Fake samples Training loss: 1.266839861869812\n",
      "Real samples Training loss: 0.4813966751098633\n",
      "Fake samples Training loss: 1.1331837177276611\n",
      "Real samples Training loss: 0.4677693843841553\n",
      "Fake samples Training loss: 1.4636437892913818\n",
      "Real samples Training loss: 0.46166670322418213\n",
      "Fake samples Training loss: 1.3246381282806396\n",
      "Real samples Training loss: 0.47229716181755066\n",
      "Fake samples Training loss: 1.2145143747329712\n",
      "Real samples Training loss: 0.5319293141365051\n",
      "Fake samples Training loss: 1.270140528678894\n",
      "Real samples Training loss: 0.425739049911499\n",
      "Fake samples Training loss: 1.3029158115386963\n",
      "Real samples Training loss: 0.5053606033325195\n",
      "Fake samples Training loss: 1.2545788288116455\n",
      "Real samples Training loss: 0.5009455680847168\n",
      "Fake samples Training loss: 1.293060064315796\n",
      "Real samples Training loss: 0.5376383662223816\n",
      "Fake samples Training loss: 1.1939655542373657\n",
      "Real samples Training loss: 0.503717839717865\n",
      "Fake samples Training loss: 1.2908951044082642\n",
      "Real samples Training loss: 0.5439673066139221\n",
      "Fake samples Training loss: 1.2865188121795654\n",
      "Real samples Training loss: 0.4827401638031006\n",
      "Fake samples Training loss: 1.3143991231918335\n",
      "Real samples Training loss: 0.41135141253471375\n",
      "Fake samples Training loss: 1.2074934244155884\n",
      "Real samples Training loss: 0.5100369453430176\n",
      "Fake samples Training loss: 1.2233392000198364\n",
      "Real samples Training loss: 0.5038342475891113\n",
      "Fake samples Training loss: 1.1915594339370728\n",
      "Real samples Training loss: 0.4183465242385864\n",
      "Fake samples Training loss: 1.21859872341156\n",
      "Real samples Training loss: 0.5668755769729614\n",
      "Fake samples Training loss: 1.183781385421753\n",
      "Real samples Training loss: 0.4650045335292816\n",
      "Fake samples Training loss: 1.2621545791625977\n",
      "Real samples Training loss: 0.5014742612838745\n",
      "Fake samples Training loss: 1.2139097452163696\n",
      "Real samples Training loss: 0.4583195745944977\n",
      "Fake samples Training loss: 1.134920358657837\n",
      "Real samples Training loss: 0.4736943542957306\n",
      "Fake samples Training loss: 1.2021524906158447\n",
      "Real samples Training loss: 0.4517609477043152\n",
      "Fake samples Training loss: 1.249585509300232\n",
      "Real samples Training loss: 0.5126551985740662\n",
      "Fake samples Training loss: 1.2719827890396118\n",
      "Real samples Training loss: 0.5096935629844666\n",
      "Fake samples Training loss: 1.1539214849472046\n",
      "Real samples Training loss: 0.5241872668266296\n",
      "Fake samples Training loss: 1.2822184562683105\n",
      "Real samples Training loss: 0.48780158162117004\n",
      "Fake samples Training loss: 1.0692317485809326\n",
      "Real samples Training loss: 0.5312851667404175\n",
      "Fake samples Training loss: 1.286037802696228\n",
      "Real samples Training loss: 0.4117441773414612\n",
      "Fake samples Training loss: 1.2186545133590698\n",
      "Real samples Training loss: 0.48174917697906494\n",
      "Fake samples Training loss: 1.2729146480560303\n",
      "Real samples Training loss: 0.4771932065486908\n",
      "Fake samples Training loss: 1.2675459384918213\n",
      "Real samples Training loss: 0.48449376225471497\n",
      "Fake samples Training loss: 1.265244722366333\n",
      "Real samples Training loss: 0.4906645715236664\n",
      "Fake samples Training loss: 1.2469223737716675\n",
      "Real samples Training loss: 0.5447482466697693\n",
      "Fake samples Training loss: 1.3154257535934448\n",
      "Real samples Training loss: 0.4198552072048187\n",
      "Fake samples Training loss: 1.2122502326965332\n",
      "Real samples Training loss: 0.5163403749465942\n",
      "Fake samples Training loss: 1.2120356559753418\n",
      "Real samples Training loss: 0.4688464105129242\n",
      "Fake samples Training loss: 1.282883644104004\n",
      "Real samples Training loss: 0.4967740774154663\n",
      "Fake samples Training loss: 1.145320177078247\n",
      "Real samples Training loss: 0.4875929653644562\n",
      "Fake samples Training loss: 1.1733275651931763\n",
      "Real samples Training loss: 0.4736340045928955\n",
      "Fake samples Training loss: 1.2139389514923096\n",
      "Real samples Training loss: 0.4508477747440338\n",
      "Fake samples Training loss: 1.1827808618545532\n",
      "Real samples Training loss: 0.4405163824558258\n",
      "Fake samples Training loss: 1.373311161994934\n",
      "Real samples Training loss: 0.44387319684028625\n",
      "Fake samples Training loss: 1.2524282932281494\n",
      "Real samples Training loss: 0.476936936378479\n",
      "Fake samples Training loss: 1.3097610473632812\n",
      "Real samples Training loss: 0.5017898082733154\n",
      "Fake samples Training loss: 1.2578634023666382\n",
      "Real samples Training loss: 0.4768458902835846\n",
      "Fake samples Training loss: 1.147243618965149\n",
      "Real samples Training loss: 0.4154471457004547\n",
      "Fake samples Training loss: 1.2805570363998413\n",
      "Real samples Training loss: 0.44887349009513855\n",
      "Fake samples Training loss: 1.197944164276123\n",
      "Real samples Training loss: 0.43736931681632996\n",
      "Fake samples Training loss: 1.2962493896484375\n",
      "Real samples Training loss: 0.4943515658378601\n",
      "Fake samples Training loss: 1.2737771272659302\n",
      "Real samples Training loss: 0.4929046630859375\n",
      "Fake samples Training loss: 1.2504231929779053\n",
      "Real samples Training loss: 0.5345402956008911\n",
      "Fake samples Training loss: 1.2428069114685059\n",
      "Real samples Training loss: 0.5176424980163574\n",
      "Fake samples Training loss: 1.2192498445510864\n",
      "Real samples Training loss: 0.4866926968097687\n",
      "Fake samples Training loss: 1.1617374420166016\n",
      "Real samples Training loss: 0.3843168318271637\n",
      "Fake samples Training loss: 1.1536822319030762\n",
      "Real samples Training loss: 0.47725191712379456\n",
      "Fake samples Training loss: 1.1758520603179932\n",
      "Real samples Training loss: 0.47219303250312805\n",
      "Fake samples Training loss: 1.2360574007034302\n",
      "Real samples Training loss: 0.5369073152542114\n",
      "Fake samples Training loss: 1.1885820627212524\n",
      "Real samples Training loss: 0.48280829191207886\n",
      "Fake samples Training loss: 1.3552443981170654\n",
      "Real samples Training loss: 0.49627938866615295\n",
      "Fake samples Training loss: 1.1331956386566162\n",
      "Real samples Training loss: 0.4283994734287262\n",
      "Fake samples Training loss: 1.1877809762954712\n",
      "Real samples Training loss: 0.4360867738723755\n",
      "Fake samples Training loss: 1.266940712928772\n",
      "Real samples Training loss: 0.4866868853569031\n",
      "Fake samples Training loss: 1.234643816947937\n",
      "Real samples Training loss: 0.5292391777038574\n",
      "Fake samples Training loss: 1.199205994606018\n",
      "Real samples Training loss: 0.5395570397377014\n",
      "Fake samples Training loss: 1.3395541906356812\n",
      "Real samples Training loss: 0.48576557636260986\n",
      "Fake samples Training loss: 1.1807833909988403\n",
      "Real samples Training loss: 0.450575590133667\n",
      "Fake samples Training loss: 1.3335121870040894\n",
      "Real samples Training loss: 0.48882392048835754\n",
      "Fake samples Training loss: 1.3744338750839233\n",
      "Real samples Training loss: 0.4250907897949219\n",
      "Fake samples Training loss: 1.1816025972366333\n",
      "Real samples Training loss: 0.4764934480190277\n",
      "Fake samples Training loss: 1.2594282627105713\n",
      "Real samples Training loss: 0.4154791235923767\n",
      "Fake samples Training loss: 1.3929404020309448\n",
      "Real samples Training loss: 0.5266147255897522\n",
      "Fake samples Training loss: 1.2855448722839355\n",
      "Real samples Training loss: 0.4708477556705475\n",
      "Fake samples Training loss: 1.2891958951950073\n",
      "Real samples Training loss: 0.4763266146183014\n",
      "Fake samples Training loss: 1.1546541452407837\n",
      "Real samples Training loss: 0.506077766418457\n",
      "Fake samples Training loss: 1.2418746948242188\n",
      "Real samples Training loss: 0.5027444362640381\n",
      "Fake samples Training loss: 1.2798570394515991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.5075874328613281\n",
      "Fake samples Training loss: 1.2767069339752197\n",
      "Real samples Training loss: 0.5011410117149353\n",
      "Fake samples Training loss: 1.209609031677246\n",
      "Real samples Training loss: 0.561060905456543\n",
      "Fake samples Training loss: 1.360368251800537\n",
      "Real samples Training loss: 0.4956732988357544\n",
      "Fake samples Training loss: 1.2191922664642334\n",
      "Real samples Training loss: 0.49237191677093506\n",
      "Fake samples Training loss: 1.3407833576202393\n",
      "Real samples Training loss: 0.4702903926372528\n",
      "Fake samples Training loss: 1.360749363899231\n",
      "Real samples Training loss: 0.47486594319343567\n",
      "Fake samples Training loss: 1.240267276763916\n",
      "Real samples Training loss: 0.5781522393226624\n",
      "Fake samples Training loss: 1.298391580581665\n",
      "Real samples Training loss: 0.4736701548099518\n",
      "Fake samples Training loss: 1.2885713577270508\n",
      "Real samples Training loss: 0.48887500166893005\n",
      "Fake samples Training loss: 1.2809511423110962\n",
      "Real samples Training loss: 0.4831644296646118\n",
      "Fake samples Training loss: 1.2669177055358887\n",
      "Real samples Training loss: 0.47476115822792053\n",
      "Fake samples Training loss: 1.1608003377914429\n",
      "Real samples Training loss: 0.4657261371612549\n",
      "Fake samples Training loss: 1.278671145439148\n",
      "Real samples Training loss: 0.5337221026420593\n",
      "Fake samples Training loss: 1.2468143701553345\n",
      "Real samples Training loss: 0.5407134890556335\n",
      "Fake samples Training loss: 1.1078686714172363\n",
      "Real samples Training loss: 0.5040900111198425\n",
      "Fake samples Training loss: 1.2914841175079346\n",
      "Real samples Training loss: 0.5357587933540344\n",
      "Fake samples Training loss: 1.0984236001968384\n",
      "Real samples Training loss: 0.48329639434814453\n",
      "Fake samples Training loss: 1.4371684789657593\n",
      "Real samples Training loss: 0.4157508909702301\n",
      "Fake samples Training loss: 1.2853063344955444\n",
      "Real samples Training loss: 0.46683362126350403\n",
      "Fake samples Training loss: 1.238233208656311\n",
      "Real samples Training loss: 0.4229590892791748\n",
      "Fake samples Training loss: 1.0606164932250977\n",
      "Real samples Training loss: 0.45232218503952026\n",
      "Fake samples Training loss: 1.2859728336334229\n",
      "Real samples Training loss: 0.47533947229385376\n",
      "Fake samples Training loss: 1.2639530897140503\n",
      "Real samples Training loss: 0.4960402250289917\n",
      "Fake samples Training loss: 1.3581361770629883\n",
      "Real samples Training loss: 0.4606982171535492\n",
      "Fake samples Training loss: 1.3781322240829468\n",
      "Real samples Training loss: 0.43033313751220703\n",
      "Fake samples Training loss: 1.1786205768585205\n",
      "Real samples Training loss: 0.4799153804779053\n",
      "Fake samples Training loss: 1.2752264738082886\n",
      "Real samples Training loss: 0.4696633517742157\n",
      "Fake samples Training loss: 1.1355782747268677\n",
      "Real samples Training loss: 0.4624839127063751\n",
      "Fake samples Training loss: 1.315029501914978\n",
      "Real samples Training loss: 0.44949010014533997\n",
      "Fake samples Training loss: 1.1914197206497192\n",
      "Real samples Training loss: 0.49006015062332153\n",
      "Fake samples Training loss: 1.2431354522705078\n",
      "Real samples Training loss: 0.4825851321220398\n",
      "Fake samples Training loss: 1.2797133922576904\n",
      "Real samples Training loss: 0.5046287775039673\n",
      "Fake samples Training loss: 1.300231695175171\n",
      "Real samples Training loss: 0.4884883165359497\n",
      "Fake samples Training loss: 1.3415385484695435\n",
      "Real samples Training loss: 0.5219976305961609\n",
      "Fake samples Training loss: 1.2189172506332397\n",
      "Real samples Training loss: 0.4392867982387543\n",
      "Fake samples Training loss: 1.282810091972351\n",
      "Real samples Training loss: 0.46861737966537476\n",
      "Fake samples Training loss: 1.3314818143844604\n",
      "Real samples Training loss: 0.4941619336605072\n",
      "Fake samples Training loss: 1.0415337085723877\n",
      "Real samples Training loss: 0.520738422870636\n",
      "Fake samples Training loss: 1.2552590370178223\n",
      "Real samples Training loss: 0.4810431897640228\n",
      "Fake samples Training loss: 1.3171842098236084\n",
      "Real samples Training loss: 0.44052404165267944\n",
      "Fake samples Training loss: 1.1891546249389648\n",
      "Real samples Training loss: 0.49126189947128296\n",
      "Fake samples Training loss: 1.1932425498962402\n",
      "Real samples Training loss: 0.41649121046066284\n",
      "Fake samples Training loss: 1.1374412775039673\n",
      "Real samples Training loss: 0.43550539016723633\n",
      "Fake samples Training loss: 1.2555521726608276\n",
      "Real samples Training loss: 0.42826080322265625\n",
      "Fake samples Training loss: 1.1807893514633179\n",
      "Real samples Training loss: 0.51849365234375\n",
      "Fake samples Training loss: 1.1122804880142212\n",
      "Real samples Training loss: 0.4752647578716278\n",
      "Fake samples Training loss: 1.288094162940979\n",
      "Real samples Training loss: 0.45008930563926697\n",
      "Fake samples Training loss: 1.2799785137176514\n",
      "Real samples Training loss: 0.545440673828125\n",
      "Fake samples Training loss: 1.17878258228302\n",
      "Real samples Training loss: 0.4838028848171234\n",
      "Fake samples Training loss: 1.2586257457733154\n",
      "Real samples Training loss: 0.4640391767024994\n",
      "Fake samples Training loss: 1.112493872642517\n",
      "Real samples Training loss: 0.45628178119659424\n",
      "Fake samples Training loss: 1.2759876251220703\n",
      "Real samples Training loss: 0.4883020520210266\n",
      "Fake samples Training loss: 1.2183462381362915\n",
      "Real samples Training loss: 0.4707602858543396\n",
      "Fake samples Training loss: 1.278947353363037\n",
      "Real samples Training loss: 0.5066065192222595\n",
      "Fake samples Training loss: 1.238240361213684\n",
      "Real samples Training loss: 0.4851210415363312\n",
      "Fake samples Training loss: 1.2117913961410522\n",
      "Real samples Training loss: 0.49621888995170593\n",
      "Fake samples Training loss: 1.0729827880859375\n",
      "Real samples Training loss: 0.4842023253440857\n",
      "Fake samples Training loss: 1.3239858150482178\n",
      "Real samples Training loss: 0.46896231174468994\n",
      "Fake samples Training loss: 1.4058070182800293\n",
      "Real samples Training loss: 0.46732041239738464\n",
      "Fake samples Training loss: 1.4821909666061401\n",
      "Real samples Training loss: 0.4903308153152466\n",
      "Fake samples Training loss: 1.1400682926177979\n",
      "Real samples Training loss: 0.43125414848327637\n",
      "Fake samples Training loss: 1.452030897140503\n",
      "Real samples Training loss: 0.4671362042427063\n",
      "Fake samples Training loss: 1.2788277864456177\n",
      "Real samples Training loss: 0.5138317346572876\n",
      "Fake samples Training loss: 1.154329538345337\n",
      "Real samples Training loss: 0.514276921749115\n",
      "Fake samples Training loss: 1.2370094060897827\n",
      "Real samples Training loss: 0.5109217762947083\n",
      "Fake samples Training loss: 1.1604448556900024\n",
      "Real samples Training loss: 0.5419247150421143\n",
      "Fake samples Training loss: 1.336260437965393\n",
      "Real samples Training loss: 0.49937161803245544\n",
      "Fake samples Training loss: 1.3858307600021362\n",
      "Real samples Training loss: 0.4680318832397461\n",
      "Fake samples Training loss: 1.2394294738769531\n",
      "Real samples Training loss: 0.4837036728858948\n",
      "Fake samples Training loss: 1.1813198328018188\n",
      "Real samples Training loss: 0.49278679490089417\n",
      "Fake samples Training loss: 1.2202073335647583\n",
      "Real samples Training loss: 0.5004714727401733\n",
      "Fake samples Training loss: 1.1754759550094604\n",
      "Real samples Training loss: 0.45792102813720703\n",
      "Fake samples Training loss: 1.2808773517608643\n",
      "Real samples Training loss: 0.4514741003513336\n",
      "Fake samples Training loss: 1.082289218902588\n",
      "Real samples Training loss: 0.514199435710907\n",
      "Fake samples Training loss: 1.0810030698776245\n",
      "Real samples Training loss: 0.39804425835609436\n",
      "Fake samples Training loss: 1.1433134078979492\n",
      "Real samples Training loss: 0.4765250086784363\n",
      "Fake samples Training loss: 1.3059593439102173\n",
      "Real samples Training loss: 0.5190010666847229\n",
      "Fake samples Training loss: 1.2702234983444214\n",
      "Real samples Training loss: 0.452436625957489\n",
      "Fake samples Training loss: 1.171911597251892\n",
      "Real samples Training loss: 0.4575457274913788\n",
      "Fake samples Training loss: 1.3023144006729126\n",
      "Real samples Training loss: 0.4640403687953949\n",
      "Fake samples Training loss: 1.4505051374435425\n",
      "Real samples Training loss: 0.4963681995868683\n",
      "Fake samples Training loss: 1.2064391374588013\n",
      "Real samples Training loss: 0.5202789306640625\n",
      "Fake samples Training loss: 1.462541103363037\n",
      "Real samples Training loss: 0.4173490107059479\n",
      "Fake samples Training loss: 1.0989762544631958\n",
      "Real samples Training loss: 0.4433673620223999\n",
      "Fake samples Training loss: 1.0017447471618652\n",
      "Real samples Training loss: 0.48227500915527344\n",
      "Fake samples Training loss: 1.250217318534851\n",
      "Real samples Training loss: 0.4919721782207489\n",
      "Fake samples Training loss: 1.3861944675445557\n",
      "Real samples Training loss: 0.4919321835041046\n",
      "Fake samples Training loss: 1.3159819841384888\n",
      "Real samples Training loss: 0.49524837732315063\n",
      "Fake samples Training loss: 1.2407622337341309\n",
      "Real samples Training loss: 0.4977930784225464\n",
      "Fake samples Training loss: 1.3339742422103882\n",
      "Real samples Training loss: 0.42036622762680054\n",
      "Fake samples Training loss: 1.2283188104629517\n",
      "Real samples Training loss: 0.4640752077102661\n",
      "Fake samples Training loss: 1.2637996673583984\n",
      "Real samples Training loss: 0.5039891600608826\n",
      "Fake samples Training loss: 1.3458137512207031\n",
      "Real samples Training loss: 0.4513131380081177\n",
      "Fake samples Training loss: 1.1610251665115356\n",
      "Real samples Training loss: 0.46312054991722107\n",
      "Fake samples Training loss: 1.4210306406021118\n",
      "Real samples Training loss: 0.4788587987422943\n",
      "Fake samples Training loss: 1.2267485857009888\n",
      "Real samples Training loss: 0.5099629759788513\n",
      "Fake samples Training loss: 1.260875940322876\n",
      "Real samples Training loss: 0.5069826245307922\n",
      "Fake samples Training loss: 1.282646894454956\n",
      "Real samples Training loss: 0.4470055103302002\n",
      "Fake samples Training loss: 1.1362721920013428\n",
      "Real samples Training loss: 0.46310746669769287\n",
      "Fake samples Training loss: 1.3973612785339355\n",
      "Real samples Training loss: 0.5385058522224426\n",
      "Fake samples Training loss: 1.1655464172363281\n",
      "Real samples Training loss: 0.476439893245697\n",
      "Fake samples Training loss: 1.2905762195587158\n",
      "Real samples Training loss: 0.49493852257728577\n",
      "Fake samples Training loss: 1.1017590761184692\n",
      "Real samples Training loss: 0.45421016216278076\n",
      "Fake samples Training loss: 1.2475425004959106\n",
      "Real samples Training loss: 0.5071027874946594\n",
      "Fake samples Training loss: 1.25242280960083\n",
      "Real samples Training loss: 0.4463019073009491\n",
      "Fake samples Training loss: 1.310662031173706\n",
      "Real samples Training loss: 0.4413730502128601\n",
      "Fake samples Training loss: 1.4242196083068848\n",
      "Real samples Training loss: 0.4868963658809662\n",
      "Fake samples Training loss: 1.2781087160110474\n",
      "Real samples Training loss: 0.48684531450271606\n",
      "Fake samples Training loss: 1.2183235883712769\n",
      "Real samples Training loss: 0.46474915742874146\n",
      "Fake samples Training loss: 1.281242847442627\n",
      "Real samples Training loss: 0.46994680166244507\n",
      "Fake samples Training loss: 1.223646640777588\n",
      "Real samples Training loss: 0.4169662594795227\n",
      "Fake samples Training loss: 1.2329998016357422\n",
      "Real samples Training loss: 0.49536001682281494\n",
      "Fake samples Training loss: 1.2969976663589478\n",
      "Real samples Training loss: 0.4661390781402588\n",
      "Fake samples Training loss: 1.192928433418274\n",
      "Real samples Training loss: 0.4599185585975647\n",
      "Fake samples Training loss: 1.1043803691864014\n",
      "Real samples Training loss: 0.536826491355896\n",
      "Fake samples Training loss: 1.271436333656311\n",
      "Real samples Training loss: 0.468228816986084\n",
      "Fake samples Training loss: 1.2561532258987427\n",
      "Real samples Training loss: 0.485967755317688\n",
      "Fake samples Training loss: 1.2214553356170654\n",
      "Real samples Training loss: 0.4523601830005646\n",
      "Fake samples Training loss: 1.0973012447357178\n",
      "Real samples Training loss: 0.43472719192504883\n",
      "Fake samples Training loss: 1.1864348649978638\n",
      "Real samples Training loss: 0.42968472838401794\n",
      "Fake samples Training loss: 1.1211225986480713\n",
      "Real samples Training loss: 0.5500457286834717\n",
      "Fake samples Training loss: 1.2800614833831787\n",
      "Real samples Training loss: 0.4190910756587982\n",
      "Fake samples Training loss: 1.158339500427246\n",
      "Real samples Training loss: 0.43168842792510986\n",
      "Fake samples Training loss: 1.1729991436004639\n",
      "Real samples Training loss: 0.4982636868953705\n",
      "Fake samples Training loss: 1.2116012573242188\n",
      "Real samples Training loss: 0.45470938086509705\n",
      "Fake samples Training loss: 1.265472173690796\n",
      "Real samples Training loss: 0.41074657440185547\n",
      "Fake samples Training loss: 1.3262759447097778\n",
      "Real samples Training loss: 0.4667084813117981\n",
      "Fake samples Training loss: 1.1735590696334839\n",
      "Real samples Training loss: 0.5025572180747986\n",
      "Fake samples Training loss: 1.143043041229248\n",
      "Real samples Training loss: 0.4944092333316803\n",
      "Fake samples Training loss: 1.2637122869491577\n",
      "Real samples Training loss: 0.42392146587371826\n",
      "Fake samples Training loss: 1.293911099433899\n",
      "Real samples Training loss: 0.5057157278060913\n",
      "Fake samples Training loss: 1.2423971891403198\n",
      "Real samples Training loss: 0.4643282890319824\n",
      "Fake samples Training loss: 1.1557600498199463\n",
      "Real samples Training loss: 0.4841289222240448\n",
      "Fake samples Training loss: 1.1867425441741943\n",
      "Real samples Training loss: 0.44598785042762756\n",
      "Fake samples Training loss: 1.1980009078979492\n",
      "Real samples Training loss: 0.5179316401481628\n",
      "Fake samples Training loss: 1.3088734149932861\n",
      "Real samples Training loss: 0.4743734300136566\n",
      "Fake samples Training loss: 1.2512813806533813\n",
      "Real samples Training loss: 0.4867156147956848\n",
      "Fake samples Training loss: 1.1200940608978271\n",
      "Real samples Training loss: 0.4588545560836792\n",
      "Fake samples Training loss: 1.3932870626449585\n",
      "Real samples Training loss: 0.524986982345581\n",
      "Fake samples Training loss: 1.2784373760223389\n",
      "Real samples Training loss: 0.5654658079147339\n",
      "Fake samples Training loss: 1.2547636032104492\n",
      "Real samples Training loss: 0.5131906270980835\n",
      "Fake samples Training loss: 1.33773672580719\n",
      "Real samples Training loss: 0.4871554672718048\n",
      "Fake samples Training loss: 1.2183650732040405\n",
      "Real samples Training loss: 0.4982949197292328\n",
      "Fake samples Training loss: 1.159290075302124\n",
      "Real samples Training loss: 0.44350162148475647\n",
      "Fake samples Training loss: 1.2263903617858887\n",
      "Real samples Training loss: 0.4707369804382324\n",
      "Fake samples Training loss: 1.2393701076507568\n",
      "Real samples Training loss: 0.4775499701499939\n",
      "Fake samples Training loss: 1.3202431201934814\n",
      "Real samples Training loss: 0.4801827073097229\n",
      "Fake samples Training loss: 1.0883393287658691\n",
      "Real samples Training loss: 0.5004491209983826\n",
      "Fake samples Training loss: 1.1507066488265991\n",
      "Real samples Training loss: 0.4213477373123169\n",
      "Fake samples Training loss: 1.3375072479248047\n",
      "Real samples Training loss: 0.47672221064567566\n",
      "Fake samples Training loss: 1.3543418645858765\n",
      "Real samples Training loss: 0.4550740122795105\n",
      "Fake samples Training loss: 1.2789827585220337\n",
      "Real samples Training loss: 0.4476114511489868\n",
      "Fake samples Training loss: 1.169316053390503\n",
      "Real samples Training loss: 0.49629268050193787\n",
      "Fake samples Training loss: 1.1167892217636108\n",
      "Real samples Training loss: 0.49389970302581787\n",
      "Fake samples Training loss: 1.2587823867797852\n",
      "Real samples Training loss: 0.5761563777923584\n",
      "Fake samples Training loss: 1.3823555707931519\n",
      "Real samples Training loss: 0.4610387980937958\n",
      "Fake samples Training loss: 1.2914085388183594\n",
      "Real samples Training loss: 0.43434178829193115\n",
      "Fake samples Training loss: 1.2619824409484863\n",
      "Real samples Training loss: 0.4092402160167694\n",
      "Fake samples Training loss: 1.3784161806106567\n",
      "Real samples Training loss: 0.5104478001594543\n",
      "Fake samples Training loss: 1.3076590299606323\n",
      "Real samples Training loss: 0.4606667160987854\n",
      "Fake samples Training loss: 1.226424217224121\n",
      "Real samples Training loss: 0.4509822130203247\n",
      "Fake samples Training loss: 1.3552030324935913\n",
      "Real samples Training loss: 0.44290417432785034\n",
      "Fake samples Training loss: 1.4561331272125244\n",
      "Real samples Training loss: 0.46339938044548035\n",
      "Fake samples Training loss: 1.2102383375167847\n",
      "Real samples Training loss: 0.48706483840942383\n",
      "Fake samples Training loss: 1.2562634944915771\n",
      "Real samples Training loss: 0.40927639603614807\n",
      "Fake samples Training loss: 1.3746939897537231\n",
      "Real samples Training loss: 0.48014262318611145\n",
      "Fake samples Training loss: 1.4128464460372925\n",
      "Real samples Training loss: 0.5263001918792725\n",
      "Fake samples Training loss: 1.3339259624481201\n",
      "Real samples Training loss: 0.4880257248878479\n",
      "Fake samples Training loss: 1.243204951286316\n",
      "Real samples Training loss: 0.5058801770210266\n",
      "Fake samples Training loss: 1.297337293624878\n",
      "Real samples Training loss: 0.46692079305648804\n",
      "Fake samples Training loss: 1.256042242050171\n",
      "Real samples Training loss: 0.5073281526565552\n",
      "Fake samples Training loss: 1.3112883567810059\n",
      "Real samples Training loss: 0.4436042010784149\n",
      "Fake samples Training loss: 1.3017783164978027\n",
      "Real samples Training loss: 0.48368966579437256\n",
      "Fake samples Training loss: 1.1436017751693726\n",
      "Real samples Training loss: 0.49395254254341125\n",
      "Fake samples Training loss: 1.2223652601242065\n",
      "Real samples Training loss: 0.5839371681213379\n",
      "Fake samples Training loss: 1.2093783617019653\n",
      "Real samples Training loss: 0.5441904067993164\n",
      "Fake samples Training loss: 1.1530596017837524\n",
      "Real samples Training loss: 0.4640321433544159\n",
      "Fake samples Training loss: 1.2669728994369507\n",
      "Real samples Training loss: 0.48956388235092163\n",
      "Fake samples Training loss: 1.2126771211624146\n",
      "Real samples Training loss: 0.488111287355423\n",
      "Fake samples Training loss: 1.2294756174087524\n",
      "Real samples Training loss: 0.45440027117729187\n",
      "Fake samples Training loss: 1.033207893371582\n",
      "Real samples Training loss: 0.44680896401405334\n",
      "Fake samples Training loss: 1.0318694114685059\n",
      "Real samples Training loss: 0.5349684357643127\n",
      "Fake samples Training loss: 1.1057014465332031\n",
      "Real samples Training loss: 0.42968353629112244\n",
      "Fake samples Training loss: 1.1395052671432495\n",
      "Real samples Training loss: 0.4355465769767761\n",
      "Fake samples Training loss: 1.2050634622573853\n",
      "Real samples Training loss: 0.4229632616043091\n",
      "Fake samples Training loss: 1.3100632429122925\n",
      "Real samples Training loss: 0.5124752521514893\n",
      "Fake samples Training loss: 1.256113886833191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.4996854066848755\n",
      "Fake samples Training loss: 1.3562690019607544\n",
      "Real samples Training loss: 0.457155704498291\n",
      "Fake samples Training loss: 1.3171899318695068\n",
      "Real samples Training loss: 0.4878997504711151\n",
      "Fake samples Training loss: 1.1952111721038818\n",
      "Real samples Training loss: 0.5036983489990234\n",
      "Fake samples Training loss: 1.2941161394119263\n",
      "Real samples Training loss: 0.47090041637420654\n",
      "Fake samples Training loss: 1.1230273246765137\n",
      "Real samples Training loss: 0.4564599394798279\n",
      "Fake samples Training loss: 1.2863454818725586\n",
      "Real samples Training loss: 0.4786820113658905\n",
      "Fake samples Training loss: 1.3264249563217163\n",
      "Real samples Training loss: 0.49393850564956665\n",
      "Fake samples Training loss: 1.239558458328247\n",
      "Real samples Training loss: 0.49120405316352844\n",
      "Fake samples Training loss: 1.2012323141098022\n",
      "Real samples Training loss: 0.47039732336997986\n",
      "Fake samples Training loss: 1.3259788751602173\n",
      "Real samples Training loss: 0.390549898147583\n",
      "Fake samples Training loss: 1.3497775793075562\n",
      "Real samples Training loss: 0.4550318419933319\n",
      "Fake samples Training loss: 1.1552096605300903\n",
      "Real samples Training loss: 0.43892186880111694\n",
      "Fake samples Training loss: 1.305629849433899\n",
      "Real samples Training loss: 0.5155791640281677\n",
      "Fake samples Training loss: 1.1216822862625122\n",
      "Real samples Training loss: 0.5534496903419495\n",
      "Fake samples Training loss: 1.2074235677719116\n",
      "Real samples Training loss: 0.5049955248832703\n",
      "Fake samples Training loss: 1.2558971643447876\n",
      "Real samples Training loss: 0.4763922691345215\n",
      "Fake samples Training loss: 1.298095464706421\n",
      "Real samples Training loss: 0.5056253671646118\n",
      "Fake samples Training loss: 1.1464576721191406\n",
      "Real samples Training loss: 0.501896858215332\n",
      "Fake samples Training loss: 1.2538591623306274\n",
      "Real samples Training loss: 0.52275151014328\n",
      "Fake samples Training loss: 1.218704104423523\n",
      "Real samples Training loss: 0.49670445919036865\n",
      "Fake samples Training loss: 1.2990111112594604\n",
      "Real samples Training loss: 0.5270501375198364\n",
      "Fake samples Training loss: 1.230857014656067\n",
      "Real samples Training loss: 0.5147016048431396\n",
      "Fake samples Training loss: 1.3232719898223877\n",
      "Real samples Training loss: 0.49951428174972534\n",
      "Fake samples Training loss: 1.3531678915023804\n",
      "Real samples Training loss: 0.538306474685669\n",
      "Fake samples Training loss: 1.2130630016326904\n",
      "Real samples Training loss: 0.49124786257743835\n",
      "Fake samples Training loss: 1.3233753442764282\n",
      "Real samples Training loss: 0.4433066248893738\n",
      "Fake samples Training loss: 1.2363674640655518\n",
      "Real samples Training loss: 0.45337411761283875\n",
      "Fake samples Training loss: 1.1468729972839355\n",
      "Real samples Training loss: 0.48640677332878113\n",
      "Fake samples Training loss: 1.2500295639038086\n",
      "Real samples Training loss: 0.47158002853393555\n",
      "Fake samples Training loss: 1.23060142993927\n",
      "Real samples Training loss: 0.5081177949905396\n",
      "Fake samples Training loss: 1.1989718675613403\n",
      "Real samples Training loss: 0.4620250165462494\n",
      "Fake samples Training loss: 1.263558268547058\n",
      "Real samples Training loss: 0.48440787196159363\n",
      "Fake samples Training loss: 1.1694639921188354\n",
      "Real samples Training loss: 0.4554310441017151\n",
      "Fake samples Training loss: 1.2034952640533447\n",
      "Real samples Training loss: 0.48844629526138306\n",
      "Fake samples Training loss: 1.270753264427185\n",
      "Real samples Training loss: 0.4124102294445038\n",
      "Fake samples Training loss: 1.2134591341018677\n",
      "Real samples Training loss: 0.5157013535499573\n",
      "Fake samples Training loss: 1.2178159952163696\n",
      "Real samples Training loss: 0.5251407027244568\n",
      "Fake samples Training loss: 1.2198776006698608\n",
      "Real samples Training loss: 0.42746853828430176\n",
      "Fake samples Training loss: 1.1534125804901123\n",
      "Real samples Training loss: 0.46144095063209534\n",
      "Fake samples Training loss: 1.2514783143997192\n",
      "Real samples Training loss: 0.47966206073760986\n",
      "Fake samples Training loss: 1.2708379030227661\n",
      "Real samples Training loss: 0.4576050341129303\n",
      "Fake samples Training loss: 1.295013427734375\n",
      "Real samples Training loss: 0.41761577129364014\n",
      "Fake samples Training loss: 1.2234488725662231\n",
      "Real samples Training loss: 0.5105839967727661\n",
      "Fake samples Training loss: 1.171667218208313\n",
      "Real samples Training loss: 0.4568609297275543\n",
      "Fake samples Training loss: 1.2795202732086182\n",
      "Real samples Training loss: 0.5043928027153015\n",
      "Fake samples Training loss: 1.1821367740631104\n",
      "Real samples Training loss: 0.5110031962394714\n",
      "Fake samples Training loss: 1.1883479356765747\n",
      "Real samples Training loss: 0.550484836101532\n",
      "Fake samples Training loss: 1.1479207277297974\n",
      "Real samples Training loss: 0.48898398876190186\n",
      "Fake samples Training loss: 1.2583799362182617\n",
      "Real samples Training loss: 0.5044746994972229\n",
      "Fake samples Training loss: 1.1585496664047241\n",
      "Real samples Training loss: 0.4695737659931183\n",
      "Fake samples Training loss: 1.201899766921997\n",
      "Real samples Training loss: 0.4114431142807007\n",
      "Fake samples Training loss: 1.2093007564544678\n",
      "Real samples Training loss: 0.515397310256958\n",
      "Fake samples Training loss: 1.3260266780853271\n",
      "Real samples Training loss: 0.49018508195877075\n",
      "Fake samples Training loss: 1.372799277305603\n",
      "Real samples Training loss: 0.5032033920288086\n",
      "Fake samples Training loss: 1.2667468786239624\n",
      "Real samples Training loss: 0.46109291911125183\n",
      "Fake samples Training loss: 1.1868834495544434\n",
      "Real samples Training loss: 0.5316519141197205\n",
      "Fake samples Training loss: 1.1115710735321045\n",
      "Real samples Training loss: 0.42403027415275574\n",
      "Fake samples Training loss: 1.2404159307479858\n",
      "Real samples Training loss: 0.49859336018562317\n",
      "Fake samples Training loss: 1.2441104650497437\n",
      "Real samples Training loss: 0.5199897289276123\n",
      "Fake samples Training loss: 1.179100513458252\n",
      "Real samples Training loss: 0.4521912932395935\n",
      "Fake samples Training loss: 1.3400256633758545\n",
      "Real samples Training loss: 0.5178831219673157\n",
      "Fake samples Training loss: 1.3090862035751343\n",
      "Real samples Training loss: 0.4454529285430908\n",
      "Fake samples Training loss: 1.3587031364440918\n",
      "Real samples Training loss: 0.44529351592063904\n",
      "Fake samples Training loss: 1.2387417554855347\n",
      "Real samples Training loss: 0.4543929696083069\n",
      "Fake samples Training loss: 1.2951725721359253\n",
      "Real samples Training loss: 0.4436308741569519\n",
      "Fake samples Training loss: 1.1408464908599854\n",
      "Real samples Training loss: 0.39781492948532104\n",
      "Fake samples Training loss: 1.2851197719573975\n",
      "Real samples Training loss: 0.44674739241600037\n",
      "Fake samples Training loss: 1.340834379196167\n",
      "Real samples Training loss: 0.493826687335968\n",
      "Fake samples Training loss: 1.1973639726638794\n",
      "Real samples Training loss: 0.5043208599090576\n",
      "Fake samples Training loss: 1.1193164587020874\n",
      "Real samples Training loss: 0.45211341977119446\n",
      "Fake samples Training loss: 1.366090178489685\n",
      "Real samples Training loss: 0.4296852648258209\n",
      "Fake samples Training loss: 1.3163824081420898\n",
      "Real samples Training loss: 0.48333442211151123\n",
      "Fake samples Training loss: 1.251454472541809\n",
      "Real samples Training loss: 0.5176998972892761\n",
      "Fake samples Training loss: 1.086761713027954\n",
      "Real samples Training loss: 0.5240292549133301\n",
      "Fake samples Training loss: 1.2667827606201172\n",
      "Real samples Training loss: 0.5584436655044556\n",
      "Fake samples Training loss: 1.2735464572906494\n",
      "Real samples Training loss: 0.44142159819602966\n",
      "Fake samples Training loss: 1.041759729385376\n",
      "Real samples Training loss: 0.40855538845062256\n",
      "Fake samples Training loss: 1.2035013437271118\n",
      "Real samples Training loss: 0.4792339503765106\n",
      "Fake samples Training loss: 1.3334851264953613\n",
      "Real samples Training loss: 0.46762579679489136\n",
      "Fake samples Training loss: 1.240574598312378\n",
      "Real samples Training loss: 0.4539141356945038\n",
      "Fake samples Training loss: 1.061810851097107\n",
      "Real samples Training loss: 0.4544040858745575\n",
      "Fake samples Training loss: 1.408413052558899\n",
      "Real samples Training loss: 0.4329683482646942\n",
      "Fake samples Training loss: 1.2434064149856567\n",
      "Real samples Training loss: 0.4920918941497803\n",
      "Fake samples Training loss: 1.260181188583374\n",
      "Real samples Training loss: 0.5258285403251648\n",
      "Fake samples Training loss: 1.2693759202957153\n",
      "Real samples Training loss: 0.4740210175514221\n",
      "Fake samples Training loss: 1.224668264389038\n",
      "Real samples Training loss: 0.4790792465209961\n",
      "Fake samples Training loss: 1.302037000656128\n",
      "Real samples Training loss: 0.4387930929660797\n",
      "Fake samples Training loss: 1.21012544631958\n",
      "Real samples Training loss: 0.47664856910705566\n",
      "Fake samples Training loss: 1.0810177326202393\n",
      "Real samples Training loss: 0.46588391065597534\n",
      "Fake samples Training loss: 1.1781307458877563\n",
      "Real samples Training loss: 0.4560868442058563\n",
      "Fake samples Training loss: 1.254689335823059\n",
      "Real samples Training loss: 0.47681841254234314\n",
      "Fake samples Training loss: 1.2234282493591309\n",
      "Real samples Training loss: 0.4656762182712555\n",
      "Fake samples Training loss: 1.2749125957489014\n",
      "Real samples Training loss: 0.5052067637443542\n",
      "Fake samples Training loss: 1.2282737493515015\n",
      "Real samples Training loss: 0.47981569170951843\n",
      "Fake samples Training loss: 1.2513811588287354\n",
      "Real samples Training loss: 0.5451740026473999\n",
      "Fake samples Training loss: 1.4350996017456055\n",
      "Real samples Training loss: 0.4416891932487488\n",
      "Fake samples Training loss: 1.2257732152938843\n",
      "Real samples Training loss: 0.456660658121109\n",
      "Fake samples Training loss: 1.2520689964294434\n",
      "Real samples Training loss: 0.5011698007583618\n",
      "Fake samples Training loss: 1.274440050125122\n",
      "Real samples Training loss: 0.4291481077671051\n",
      "Fake samples Training loss: 1.226301670074463\n",
      "Real samples Training loss: 0.48571765422821045\n",
      "Fake samples Training loss: 1.2572158575057983\n",
      "Real samples Training loss: 0.5252082943916321\n",
      "Fake samples Training loss: 1.3560086488723755\n",
      "Real samples Training loss: 0.45581772923469543\n",
      "Fake samples Training loss: 1.3224202394485474\n",
      "Real samples Training loss: 0.5001651048660278\n",
      "Fake samples Training loss: 1.180803894996643\n",
      "Real samples Training loss: 0.4974566698074341\n",
      "Fake samples Training loss: 1.4309601783752441\n",
      "Real samples Training loss: 0.5374636650085449\n",
      "Fake samples Training loss: 1.2900327444076538\n",
      "Real samples Training loss: 0.45735418796539307\n",
      "Fake samples Training loss: 1.1937309503555298\n",
      "Real samples Training loss: 0.5438796877861023\n",
      "Fake samples Training loss: 1.3272881507873535\n",
      "Real samples Training loss: 0.44857290387153625\n",
      "Fake samples Training loss: 1.2001110315322876\n",
      "Real samples Training loss: 0.4361976683139801\n",
      "Fake samples Training loss: 1.3206467628479004\n",
      "Real samples Training loss: 0.4860778748989105\n",
      "Fake samples Training loss: 1.1787749528884888\n",
      "Real samples Training loss: 0.4422589838504791\n",
      "Fake samples Training loss: 1.2279936075210571\n",
      "Real samples Training loss: 0.44376590847969055\n",
      "Fake samples Training loss: 1.1335420608520508\n",
      "Real samples Training loss: 0.46162882447242737\n",
      "Fake samples Training loss: 1.2543611526489258\n",
      "Real samples Training loss: 0.501082181930542\n",
      "Fake samples Training loss: 1.1800758838653564\n",
      "Real samples Training loss: 0.41993874311447144\n",
      "Fake samples Training loss: 1.360200047492981\n",
      "Real samples Training loss: 0.4863729476928711\n",
      "Fake samples Training loss: 1.1795793771743774\n",
      "Real samples Training loss: 0.45839399099349976\n",
      "Fake samples Training loss: 1.306329369544983\n",
      "Real samples Training loss: 0.479165256023407\n",
      "Fake samples Training loss: 1.1562303304672241\n",
      "Real samples Training loss: 0.46004220843315125\n",
      "Fake samples Training loss: 1.2351058721542358\n",
      "Real samples Training loss: 0.4088163673877716\n",
      "Fake samples Training loss: 1.1318321228027344\n",
      "Real samples Training loss: 0.4486747980117798\n",
      "Fake samples Training loss: 1.2346731424331665\n",
      "Real samples Training loss: 0.46124356985092163\n",
      "Fake samples Training loss: 1.2553784847259521\n",
      "Real samples Training loss: 0.47943297028541565\n",
      "Fake samples Training loss: 1.307631492614746\n",
      "Real samples Training loss: 0.5210081934928894\n",
      "Fake samples Training loss: 1.1351401805877686\n",
      "Real samples Training loss: 0.4685623049736023\n",
      "Fake samples Training loss: 1.2283531427383423\n",
      "Real samples Training loss: 0.5215768814086914\n",
      "Fake samples Training loss: 1.2385454177856445\n",
      "Real samples Training loss: 0.4701962172985077\n",
      "Fake samples Training loss: 1.258931279182434\n",
      "Real samples Training loss: 0.43087050318717957\n",
      "Fake samples Training loss: 1.2178890705108643\n",
      "Real samples Training loss: 0.5214454531669617\n",
      "Fake samples Training loss: 1.1964820623397827\n",
      "Real samples Training loss: 0.516646683216095\n",
      "Fake samples Training loss: 1.3393163681030273\n",
      "Real samples Training loss: 0.42331668734550476\n",
      "Fake samples Training loss: 1.1402416229248047\n",
      "Real samples Training loss: 0.4833202064037323\n",
      "Fake samples Training loss: 1.1622850894927979\n",
      "Real samples Training loss: 0.4852380156517029\n",
      "Fake samples Training loss: 1.3354328870773315\n",
      "Real samples Training loss: 0.5240805745124817\n",
      "Fake samples Training loss: 1.2236578464508057\n",
      "Real samples Training loss: 0.482681542634964\n",
      "Fake samples Training loss: 1.205385446548462\n",
      "Real samples Training loss: 0.4856211841106415\n",
      "Fake samples Training loss: 1.149259328842163\n",
      "Real samples Training loss: 0.49005836248397827\n",
      "Fake samples Training loss: 1.292701244354248\n",
      "Real samples Training loss: 0.4443114697933197\n",
      "Fake samples Training loss: 1.1312973499298096\n",
      "Real samples Training loss: 0.48576074838638306\n",
      "Fake samples Training loss: 1.2418690919876099\n",
      "Real samples Training loss: 0.49108070135116577\n",
      "Fake samples Training loss: 1.3416523933410645\n",
      "Real samples Training loss: 0.5245172381401062\n",
      "Fake samples Training loss: 1.3047088384628296\n",
      "Real samples Training loss: 0.511661171913147\n",
      "Fake samples Training loss: 1.1989569664001465\n",
      "Real samples Training loss: 0.5273085832595825\n",
      "Fake samples Training loss: 1.1604119539260864\n",
      "Real samples Training loss: 0.5555614829063416\n",
      "Fake samples Training loss: 1.2603508234024048\n",
      "Real samples Training loss: 0.5185909271240234\n",
      "Fake samples Training loss: 1.287311315536499\n",
      "Real samples Training loss: 0.432976633310318\n",
      "Fake samples Training loss: 1.2184370756149292\n",
      "Real samples Training loss: 0.46782809495925903\n",
      "Fake samples Training loss: 1.3101425170898438\n",
      "Real samples Training loss: 0.48228320479393005\n",
      "Fake samples Training loss: 1.2270042896270752\n",
      "Real samples Training loss: 0.43009039759635925\n",
      "Fake samples Training loss: 1.3364198207855225\n",
      "Real samples Training loss: 0.4254666864871979\n",
      "Fake samples Training loss: 1.386081576347351\n",
      "Real samples Training loss: 0.5003206729888916\n",
      "Fake samples Training loss: 1.2610876560211182\n",
      "Real samples Training loss: 0.4678170382976532\n",
      "Fake samples Training loss: 1.399997591972351\n",
      "Real samples Training loss: 0.4628644585609436\n",
      "Fake samples Training loss: 1.1284043788909912\n",
      "Real samples Training loss: 0.4389326870441437\n",
      "Fake samples Training loss: 1.1967594623565674\n",
      "Real samples Training loss: 0.4689984619617462\n",
      "Fake samples Training loss: 1.4435731172561646\n",
      "Real samples Training loss: 0.4517778158187866\n",
      "Fake samples Training loss: 1.4773919582366943\n",
      "Real samples Training loss: 0.5047584772109985\n",
      "Fake samples Training loss: 1.1447718143463135\n",
      "Real samples Training loss: 0.4628749489784241\n",
      "Fake samples Training loss: 1.3434679508209229\n",
      "Real samples Training loss: 0.431755006313324\n",
      "Fake samples Training loss: 1.1809331178665161\n",
      "Real samples Training loss: 0.48544377088546753\n",
      "Fake samples Training loss: 1.2604621648788452\n",
      "Real samples Training loss: 0.4768087565898895\n",
      "Fake samples Training loss: 1.1882617473602295\n",
      "Real samples Training loss: 0.5905793905258179\n",
      "Fake samples Training loss: 1.3090243339538574\n",
      "Real samples Training loss: 0.40948060154914856\n",
      "Fake samples Training loss: 1.2418698072433472\n",
      "Real samples Training loss: 0.44061368703842163\n",
      "Fake samples Training loss: 1.240350604057312\n",
      "Real samples Training loss: 0.5115219354629517\n",
      "Fake samples Training loss: 1.058172345161438\n",
      "Real samples Training loss: 0.5153858065605164\n",
      "Fake samples Training loss: 1.1890978813171387\n",
      "Real samples Training loss: 0.42552676796913147\n",
      "Fake samples Training loss: 1.3053662776947021\n",
      "Real samples Training loss: 0.41081175208091736\n",
      "Fake samples Training loss: 1.3845127820968628\n",
      "Real samples Training loss: 0.513610303401947\n",
      "Fake samples Training loss: 1.1086512804031372\n",
      "Real samples Training loss: 0.46445080637931824\n",
      "Fake samples Training loss: 1.1568928956985474\n",
      "Real samples Training loss: 0.506106972694397\n",
      "Fake samples Training loss: 1.1785775423049927\n",
      "Real samples Training loss: 0.5236282348632812\n",
      "Fake samples Training loss: 1.2805843353271484\n",
      "Real samples Training loss: 0.41382497549057007\n",
      "Fake samples Training loss: 1.258789300918579\n",
      "Real samples Training loss: 0.5346276164054871\n",
      "Fake samples Training loss: 1.2946281433105469\n",
      "Real samples Training loss: 0.4323093593120575\n",
      "Fake samples Training loss: 1.331344485282898\n",
      "Real samples Training loss: 0.4327158033847809\n",
      "Fake samples Training loss: 1.3324148654937744\n",
      "Real samples Training loss: 0.5016910433769226\n",
      "Fake samples Training loss: 1.1999828815460205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.49769046902656555\n",
      "Fake samples Training loss: 1.1605275869369507\n",
      "Real samples Training loss: 0.5455095171928406\n",
      "Fake samples Training loss: 1.1235169172286987\n",
      "Real samples Training loss: 0.4447062313556671\n",
      "Fake samples Training loss: 1.2300119400024414\n",
      "Real samples Training loss: 0.47151562571525574\n",
      "Fake samples Training loss: 1.2544020414352417\n",
      "Real samples Training loss: 0.4628189206123352\n",
      "Fake samples Training loss: 1.285996913909912\n",
      "Real samples Training loss: 0.4648343026638031\n",
      "Fake samples Training loss: 1.3424540758132935\n",
      "Real samples Training loss: 0.5494853854179382\n",
      "Fake samples Training loss: 1.2290998697280884\n",
      "Real samples Training loss: 0.5153069496154785\n",
      "Fake samples Training loss: 1.3204360008239746\n",
      "Real samples Training loss: 0.46312516927719116\n",
      "Fake samples Training loss: 1.0808591842651367\n",
      "Real samples Training loss: 0.4867176413536072\n",
      "Fake samples Training loss: 1.257237195968628\n",
      "Real samples Training loss: 0.47789663076400757\n",
      "Fake samples Training loss: 1.1808134317398071\n",
      "Real samples Training loss: 0.4987276792526245\n",
      "Fake samples Training loss: 1.102612853050232\n",
      "Real samples Training loss: 0.4810354709625244\n",
      "Fake samples Training loss: 0.9771212339401245\n",
      "Real samples Training loss: 0.4734932482242584\n",
      "Fake samples Training loss: 1.1410634517669678\n",
      "Real samples Training loss: 0.4806228578090668\n",
      "Fake samples Training loss: 1.3590446710586548\n",
      "Real samples Training loss: 0.5150937438011169\n",
      "Fake samples Training loss: 1.0776867866516113\n",
      "Real samples Training loss: 0.443198025226593\n",
      "Fake samples Training loss: 1.2058802843093872\n",
      "Real samples Training loss: 0.48649078607559204\n",
      "Fake samples Training loss: 1.4231361150741577\n",
      "Real samples Training loss: 0.4442756175994873\n",
      "Fake samples Training loss: 1.2440539598464966\n",
      "Real samples Training loss: 0.45766186714172363\n",
      "Fake samples Training loss: 1.3414936065673828\n",
      "Real samples Training loss: 0.43583160638809204\n",
      "Fake samples Training loss: 1.3681038618087769\n",
      "Real samples Training loss: 0.460750937461853\n",
      "Fake samples Training loss: 1.1878570318222046\n",
      "Real samples Training loss: 0.4854108691215515\n",
      "Fake samples Training loss: 1.1641724109649658\n",
      "Real samples Training loss: 0.44576582312583923\n",
      "Fake samples Training loss: 1.3673055171966553\n",
      "Real samples Training loss: 0.48045238852500916\n",
      "Fake samples Training loss: 1.3051397800445557\n",
      "Real samples Training loss: 0.4931744635105133\n",
      "Fake samples Training loss: 1.0780351161956787\n",
      "Real samples Training loss: 0.535460352897644\n",
      "Fake samples Training loss: 1.3054542541503906\n",
      "Real samples Training loss: 0.43758079409599304\n",
      "Fake samples Training loss: 1.1652847528457642\n",
      "Real samples Training loss: 0.47074857354164124\n",
      "Fake samples Training loss: 1.2185059785842896\n",
      "Real samples Training loss: 0.4711243510246277\n",
      "Fake samples Training loss: 1.2907096147537231\n",
      "Real samples Training loss: 0.5284169316291809\n",
      "Fake samples Training loss: 1.2554880380630493\n",
      "Real samples Training loss: 0.5104219913482666\n",
      "Fake samples Training loss: 1.186560869216919\n",
      "Real samples Training loss: 0.5246229767799377\n",
      "Fake samples Training loss: 1.2423779964447021\n",
      "Real samples Training loss: 0.42557016015052795\n",
      "Fake samples Training loss: 1.1555047035217285\n",
      "Real samples Training loss: 0.4476046860218048\n",
      "Fake samples Training loss: 1.1723334789276123\n",
      "Real samples Training loss: 0.469046026468277\n",
      "Fake samples Training loss: 1.3744374513626099\n",
      "Real samples Training loss: 0.45404869318008423\n",
      "Fake samples Training loss: 1.2465494871139526\n",
      "Real samples Training loss: 0.5302308201789856\n",
      "Fake samples Training loss: 1.1259441375732422\n",
      "Real samples Training loss: 0.48122334480285645\n",
      "Fake samples Training loss: 1.175614356994629\n",
      "Real samples Training loss: 0.4940415322780609\n",
      "Fake samples Training loss: 1.3354054689407349\n",
      "Real samples Training loss: 0.42531466484069824\n",
      "Fake samples Training loss: 1.2986140251159668\n",
      "Real samples Training loss: 0.505523681640625\n",
      "Fake samples Training loss: 1.2350484132766724\n",
      "Real samples Training loss: 0.43606889247894287\n",
      "Fake samples Training loss: 1.3599129915237427\n",
      "Real samples Training loss: 0.4896390736103058\n",
      "Fake samples Training loss: 1.139125943183899\n",
      "Real samples Training loss: 0.46274518966674805\n",
      "Fake samples Training loss: 1.2453268766403198\n",
      "Real samples Training loss: 0.47144752740859985\n",
      "Fake samples Training loss: 1.2153633832931519\n",
      "Real samples Training loss: 0.4736129343509674\n",
      "Fake samples Training loss: 1.3457955121994019\n",
      "Real samples Training loss: 0.4456683397293091\n",
      "Fake samples Training loss: 1.1319782733917236\n",
      "Real samples Training loss: 0.43851593136787415\n",
      "Fake samples Training loss: 1.2172945737838745\n",
      "Real samples Training loss: 0.5125085115432739\n",
      "Fake samples Training loss: 1.1835861206054688\n",
      "Real samples Training loss: 0.4828810691833496\n",
      "Fake samples Training loss: 1.3871656656265259\n",
      "Real samples Training loss: 0.48283717036247253\n",
      "Fake samples Training loss: 1.2561039924621582\n",
      "Real samples Training loss: 0.4864020347595215\n",
      "Fake samples Training loss: 1.265191674232483\n",
      "Real samples Training loss: 0.44474950432777405\n",
      "Fake samples Training loss: 1.325477123260498\n",
      "Real samples Training loss: 0.4866916835308075\n",
      "Fake samples Training loss: 1.2060186862945557\n",
      "Real samples Training loss: 0.4463326334953308\n",
      "Fake samples Training loss: 1.308834433555603\n",
      "Real samples Training loss: 0.4706653654575348\n",
      "Fake samples Training loss: 1.3467084169387817\n",
      "Real samples Training loss: 0.5240176320075989\n",
      "Fake samples Training loss: 1.3464969396591187\n",
      "Real samples Training loss: 0.4678739905357361\n",
      "Fake samples Training loss: 1.3087832927703857\n",
      "Real samples Training loss: 0.512365996837616\n",
      "Fake samples Training loss: 1.2533245086669922\n",
      "Real samples Training loss: 0.5377541184425354\n",
      "Fake samples Training loss: 1.1946783065795898\n",
      "Real samples Training loss: 0.437542587518692\n",
      "Fake samples Training loss: 1.4300105571746826\n",
      "Real samples Training loss: 0.4521845281124115\n",
      "Fake samples Training loss: 1.0190085172653198\n",
      "Real samples Training loss: 0.4921456277370453\n",
      "Fake samples Training loss: 1.2591578960418701\n",
      "Real samples Training loss: 0.5229637622833252\n",
      "Fake samples Training loss: 1.2712212800979614\n",
      "Real samples Training loss: 0.4995325207710266\n",
      "Fake samples Training loss: 1.3746320009231567\n",
      "Real samples Training loss: 0.47358694672584534\n",
      "Fake samples Training loss: 1.2569037675857544\n",
      "Real samples Training loss: 0.48610377311706543\n",
      "Fake samples Training loss: 1.2930883169174194\n",
      "Real samples Training loss: 0.4818105697631836\n",
      "Fake samples Training loss: 1.156520128250122\n",
      "Real samples Training loss: 0.5118091702461243\n",
      "Fake samples Training loss: 1.1848751306533813\n",
      "Real samples Training loss: 0.5439023375511169\n",
      "Fake samples Training loss: 1.2025930881500244\n",
      "Real samples Training loss: 0.5110755562782288\n",
      "Fake samples Training loss: 1.2865560054779053\n",
      "Real samples Training loss: 0.5176973342895508\n",
      "Fake samples Training loss: 1.3562736511230469\n",
      "Real samples Training loss: 0.500969409942627\n",
      "Fake samples Training loss: 1.163543939590454\n",
      "Real samples Training loss: 0.43936172127723694\n",
      "Fake samples Training loss: 1.3767516613006592\n",
      "Real samples Training loss: 0.5172461271286011\n",
      "Fake samples Training loss: 1.1943861246109009\n",
      "Real samples Training loss: 0.46673762798309326\n",
      "Fake samples Training loss: 1.3087087869644165\n",
      "Real samples Training loss: 0.5149854421615601\n",
      "Fake samples Training loss: 1.1896127462387085\n",
      "Real samples Training loss: 0.46022287011146545\n",
      "Fake samples Training loss: 1.1743882894515991\n",
      "Real samples Training loss: 0.49230310320854187\n",
      "Fake samples Training loss: 1.359459400177002\n",
      "Real samples Training loss: 0.4113803505897522\n",
      "Fake samples Training loss: 1.064196228981018\n",
      "Real samples Training loss: 0.5250688791275024\n",
      "Fake samples Training loss: 1.141865611076355\n",
      "Real samples Training loss: 0.46222445368766785\n",
      "Fake samples Training loss: 1.4667880535125732\n",
      "Real samples Training loss: 0.49122315645217896\n",
      "Fake samples Training loss: 1.2220206260681152\n",
      "Real samples Training loss: 0.5030059218406677\n",
      "Fake samples Training loss: 1.1918559074401855\n",
      "Real samples Training loss: 0.48075807094573975\n",
      "Fake samples Training loss: 1.321115255355835\n",
      "Real samples Training loss: 0.45930737257003784\n",
      "Fake samples Training loss: 1.2642203569412231\n",
      "Real samples Training loss: 0.4830703139305115\n",
      "Fake samples Training loss: 1.2368828058242798\n",
      "Real samples Training loss: 0.49992403388023376\n",
      "Fake samples Training loss: 1.1542092561721802\n",
      "Real samples Training loss: 0.5482255220413208\n",
      "Fake samples Training loss: 1.2759965658187866\n",
      "Real samples Training loss: 0.559167742729187\n",
      "Fake samples Training loss: 1.1358277797698975\n",
      "Real samples Training loss: 0.5244699120521545\n",
      "Fake samples Training loss: 1.2364394664764404\n",
      "Real samples Training loss: 0.4673386514186859\n",
      "Fake samples Training loss: 1.328814148902893\n",
      "Real samples Training loss: 0.46140745282173157\n",
      "Fake samples Training loss: 1.1906903982162476\n",
      "Real samples Training loss: 0.4853455126285553\n",
      "Fake samples Training loss: 1.252599835395813\n",
      "Real samples Training loss: 0.43470045924186707\n",
      "Fake samples Training loss: 1.1534489393234253\n",
      "Real samples Training loss: 0.48765799403190613\n",
      "Fake samples Training loss: 1.253005027770996\n",
      "Real samples Training loss: 0.4966740608215332\n",
      "Fake samples Training loss: 1.3259828090667725\n",
      "Real samples Training loss: 0.5435622334480286\n",
      "Fake samples Training loss: 1.204235553741455\n",
      "Real samples Training loss: 0.43172135949134827\n",
      "Fake samples Training loss: 1.2447516918182373\n",
      "Real samples Training loss: 0.45686057209968567\n",
      "Fake samples Training loss: 1.197160243988037\n",
      "Real samples Training loss: 0.5286629796028137\n",
      "Fake samples Training loss: 1.1899760961532593\n",
      "Real samples Training loss: 0.5382468104362488\n",
      "Fake samples Training loss: 1.3044114112854004\n",
      "Real samples Training loss: 0.48517730832099915\n",
      "Fake samples Training loss: 1.1135880947113037\n",
      "Real samples Training loss: 0.48431718349456787\n",
      "Fake samples Training loss: 1.246667504310608\n",
      "Real samples Training loss: 0.5258389711380005\n",
      "Fake samples Training loss: 1.3704158067703247\n",
      "Real samples Training loss: 0.48224419355392456\n",
      "Fake samples Training loss: 1.2471706867218018\n",
      "Real samples Training loss: 0.4760928153991699\n",
      "Fake samples Training loss: 1.1808485984802246\n",
      "Real samples Training loss: 0.45966213941574097\n",
      "Fake samples Training loss: 1.318371057510376\n",
      "Real samples Training loss: 0.46197640895843506\n",
      "Fake samples Training loss: 1.37543785572052\n",
      "Real samples Training loss: 0.5399425625801086\n",
      "Fake samples Training loss: 1.5319812297821045\n",
      "Real samples Training loss: 0.4580381214618683\n",
      "Fake samples Training loss: 1.1517653465270996\n",
      "Real samples Training loss: 0.48151925206184387\n",
      "Fake samples Training loss: 1.1697512865066528\n",
      "Real samples Training loss: 0.5148223042488098\n",
      "Fake samples Training loss: 1.2382258176803589\n",
      "Real samples Training loss: 0.4604041576385498\n",
      "Fake samples Training loss: 1.3445416688919067\n",
      "Real samples Training loss: 0.542855978012085\n",
      "Fake samples Training loss: 1.326401948928833\n",
      "Real samples Training loss: 0.4428362548351288\n",
      "Fake samples Training loss: 1.3462581634521484\n",
      "Real samples Training loss: 0.48004835844039917\n",
      "Fake samples Training loss: 1.2316484451293945\n",
      "Real samples Training loss: 0.46786969900131226\n",
      "Fake samples Training loss: 1.1639400720596313\n",
      "Real samples Training loss: 0.45686817169189453\n",
      "Fake samples Training loss: 1.1655675172805786\n",
      "Real samples Training loss: 0.4461645781993866\n",
      "Fake samples Training loss: 1.2364145517349243\n",
      "Real samples Training loss: 0.48234254121780396\n",
      "Fake samples Training loss: 1.121429204940796\n",
      "Real samples Training loss: 0.4868345558643341\n",
      "Fake samples Training loss: 1.2297407388687134\n",
      "Real samples Training loss: 0.5564365386962891\n",
      "Fake samples Training loss: 1.2475783824920654\n",
      "Real samples Training loss: 0.4983139932155609\n",
      "Fake samples Training loss: 1.2676057815551758\n",
      "Real samples Training loss: 0.5034465789794922\n",
      "Fake samples Training loss: 1.3164314031600952\n",
      "Real samples Training loss: 0.45749738812446594\n",
      "Fake samples Training loss: 1.3346318006515503\n",
      "Real samples Training loss: 0.49059218168258667\n",
      "Fake samples Training loss: 1.3175764083862305\n",
      "Real samples Training loss: 0.45682406425476074\n",
      "Fake samples Training loss: 1.1820248365402222\n",
      "Real samples Training loss: 0.44971179962158203\n",
      "Fake samples Training loss: 1.3037927150726318\n",
      "Real samples Training loss: 0.5129439234733582\n",
      "Fake samples Training loss: 1.373457908630371\n",
      "Real samples Training loss: 0.5065948367118835\n",
      "Fake samples Training loss: 1.2885472774505615\n",
      "Real samples Training loss: 0.46366456151008606\n",
      "Fake samples Training loss: 1.2014533281326294\n",
      "Real samples Training loss: 0.407535582780838\n",
      "Fake samples Training loss: 1.1406267881393433\n",
      "Real samples Training loss: 0.48910000920295715\n",
      "Fake samples Training loss: 1.4244686365127563\n",
      "Real samples Training loss: 0.4594201147556305\n",
      "Fake samples Training loss: 1.4418553113937378\n",
      "Real samples Training loss: 0.45976710319519043\n",
      "Fake samples Training loss: 1.1991561651229858\n",
      "Real samples Training loss: 0.4673798978328705\n",
      "Fake samples Training loss: 1.4280200004577637\n",
      "Real samples Training loss: 0.5400959253311157\n",
      "Fake samples Training loss: 1.4211452007293701\n",
      "Real samples Training loss: 0.5183794498443604\n",
      "Fake samples Training loss: 1.247214436531067\n",
      "Real samples Training loss: 0.49397629499435425\n",
      "Fake samples Training loss: 1.4370797872543335\n",
      "Real samples Training loss: 0.5323919057846069\n",
      "Fake samples Training loss: 1.2698237895965576\n",
      "Real samples Training loss: 0.49040916562080383\n",
      "Fake samples Training loss: 1.1490795612335205\n",
      "Real samples Training loss: 0.4495229721069336\n",
      "Fake samples Training loss: 1.2156562805175781\n",
      "Real samples Training loss: 0.4868565499782562\n",
      "Fake samples Training loss: 1.1101906299591064\n",
      "Real samples Training loss: 0.4649539291858673\n",
      "Fake samples Training loss: 1.2222754955291748\n",
      "Real samples Training loss: 0.4726869463920593\n",
      "Fake samples Training loss: 1.1717528104782104\n",
      "Real samples Training loss: 0.4965362548828125\n",
      "Fake samples Training loss: 1.0645561218261719\n",
      "Real samples Training loss: 0.4856606721878052\n",
      "Fake samples Training loss: 1.2654650211334229\n",
      "Real samples Training loss: 0.4616749584674835\n",
      "Fake samples Training loss: 1.1648222208023071\n",
      "Real samples Training loss: 0.48413631319999695\n",
      "Fake samples Training loss: 1.2328174114227295\n",
      "Real samples Training loss: 0.4566066861152649\n",
      "Fake samples Training loss: 1.2272489070892334\n",
      "Real samples Training loss: 0.4411105215549469\n",
      "Fake samples Training loss: 1.218254804611206\n",
      "Real samples Training loss: 0.47936850786209106\n",
      "Fake samples Training loss: 1.1849040985107422\n",
      "Real samples Training loss: 0.5366670489311218\n",
      "Fake samples Training loss: 1.2655425071716309\n",
      "Real samples Training loss: 0.5279471278190613\n",
      "Fake samples Training loss: 1.1152015924453735\n",
      "Real samples Training loss: 0.41971680521965027\n",
      "Fake samples Training loss: 1.307844877243042\n",
      "Real samples Training loss: 0.4689866304397583\n",
      "Fake samples Training loss: 1.2258338928222656\n",
      "Real samples Training loss: 0.5083726644515991\n",
      "Fake samples Training loss: 1.403940200805664\n",
      "Real samples Training loss: 0.4980442523956299\n",
      "Fake samples Training loss: 1.1514724493026733\n",
      "Real samples Training loss: 0.44453272223472595\n",
      "Fake samples Training loss: 1.1743930578231812\n",
      "Real samples Training loss: 0.47743794322013855\n",
      "Fake samples Training loss: 1.1835969686508179\n",
      "Real samples Training loss: 0.4938727021217346\n",
      "Fake samples Training loss: 1.2352453470230103\n",
      "Real samples Training loss: 0.4437042772769928\n",
      "Fake samples Training loss: 1.1182880401611328\n",
      "Real samples Training loss: 0.4619031250476837\n",
      "Fake samples Training loss: 1.3780635595321655\n",
      "Real samples Training loss: 0.4848337769508362\n",
      "Fake samples Training loss: 1.4460070133209229\n",
      "Real samples Training loss: 0.5169892311096191\n",
      "Fake samples Training loss: 1.3566266298294067\n",
      "Real samples Training loss: 0.4662023186683655\n",
      "Fake samples Training loss: 1.1865501403808594\n",
      "Real samples Training loss: 0.5380294919013977\n",
      "Fake samples Training loss: 1.1647089719772339\n",
      "Real samples Training loss: 0.4662845730781555\n",
      "Fake samples Training loss: 1.141013741493225\n",
      "Real samples Training loss: 0.4589017331600189\n",
      "Fake samples Training loss: 1.0848028659820557\n",
      "Real samples Training loss: 0.5039334893226624\n",
      "Fake samples Training loss: 1.244315505027771\n",
      "Real samples Training loss: 0.4818017780780792\n",
      "Fake samples Training loss: 1.373165249824524\n",
      "Real samples Training loss: 0.43969854712486267\n",
      "Fake samples Training loss: 1.1780775785446167\n",
      "Real samples Training loss: 0.48490843176841736\n",
      "Fake samples Training loss: 1.3145314455032349\n",
      "Real samples Training loss: 0.48477184772491455\n",
      "Fake samples Training loss: 1.2611148357391357\n",
      "Real samples Training loss: 0.45553135871887207\n",
      "Fake samples Training loss: 1.2151308059692383\n",
      "Real samples Training loss: 0.4107661545276642\n",
      "Fake samples Training loss: 1.3346400260925293\n",
      "Real samples Training loss: 0.45221078395843506\n",
      "Fake samples Training loss: 1.2900253534317017\n",
      "Real samples Training loss: 0.47554394602775574\n",
      "Fake samples Training loss: 1.2901140451431274\n",
      "Real samples Training loss: 0.4239921569824219\n",
      "Fake samples Training loss: 1.2369154691696167\n",
      "Real samples Training loss: 0.4930286407470703\n",
      "Fake samples Training loss: 1.2657785415649414\n",
      "Real samples Training loss: 0.41568654775619507\n",
      "Fake samples Training loss: 1.1406422853469849\n",
      "Real samples Training loss: 0.4637146294116974\n",
      "Fake samples Training loss: 1.1710777282714844\n",
      "Real samples Training loss: 0.44932153820991516\n",
      "Fake samples Training loss: 1.3699339628219604\n",
      "Real samples Training loss: 0.4082787036895752\n",
      "Fake samples Training loss: 1.2249033451080322\n",
      "Real samples Training loss: 0.4174107611179352\n",
      "Fake samples Training loss: 1.163328766822815\n",
      "Real samples Training loss: 0.41468194127082825\n",
      "Fake samples Training loss: 1.194312572479248\n",
      "Real samples Training loss: 0.44532838463783264\n",
      "Fake samples Training loss: 1.1818609237670898\n",
      "Real samples Training loss: 0.42757871747016907\n",
      "Fake samples Training loss: 1.2486246824264526\n",
      "Real samples Training loss: 0.48822638392448425\n",
      "Fake samples Training loss: 1.3594390153884888\n",
      "Real samples Training loss: 0.5265828371047974\n",
      "Fake samples Training loss: 1.1327450275421143\n",
      "Real samples Training loss: 0.4238063395023346\n",
      "Fake samples Training loss: 1.2022383213043213\n",
      "Real samples Training loss: 0.48421710729599\n",
      "Fake samples Training loss: 1.260446310043335\n",
      "Real samples Training loss: 0.4402187168598175\n",
      "Fake samples Training loss: 1.1093933582305908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples Training loss: 0.47571074962615967\n",
      "Fake samples Training loss: 1.276076078414917\n",
      "Real samples Training loss: 0.45156535506248474\n",
      "Fake samples Training loss: 1.0916298627853394\n",
      "Real samples Training loss: 0.4518890976905823\n",
      "Fake samples Training loss: 1.336490511894226\n",
      "Real samples Training loss: 0.4494737684726715\n",
      "Fake samples Training loss: 1.1494672298431396\n",
      "Real samples Training loss: 0.4450019299983978\n",
      "Fake samples Training loss: 1.314070224761963\n",
      "Real samples Training loss: 0.47896143794059753\n",
      "Fake samples Training loss: 1.3753470182418823\n",
      "Real samples Training loss: 0.48195764422416687\n",
      "Fake samples Training loss: 1.1115721464157104\n",
      "Real samples Training loss: 0.486150860786438\n",
      "Fake samples Training loss: 1.1973589658737183\n",
      "Real samples Training loss: 0.5085659027099609\n",
      "Fake samples Training loss: 1.1732348203659058\n",
      "Real samples Training loss: 0.501758337020874\n",
      "Fake samples Training loss: 1.2303144931793213\n",
      "Real samples Training loss: 0.4681656062602997\n",
      "Fake samples Training loss: 1.1353590488433838\n",
      "Real samples Training loss: 0.5108174085617065\n",
      "Fake samples Training loss: 1.2296605110168457\n",
      "Real samples Training loss: 0.43362337350845337\n",
      "Fake samples Training loss: 1.1641441583633423\n",
      "Real samples Training loss: 0.455035537481308\n",
      "Fake samples Training loss: 1.232086181640625\n",
      "Real samples Training loss: 0.47424522042274475\n",
      "Fake samples Training loss: 1.2423028945922852\n",
      "Real samples Training loss: 0.5034814476966858\n",
      "Fake samples Training loss: 1.2628217935562134\n",
      "Real samples Training loss: 0.46710005402565\n",
      "Fake samples Training loss: 1.2488220930099487\n",
      "Real samples Training loss: 0.4309423267841339\n",
      "Fake samples Training loss: 1.125087022781372\n",
      "Real samples Training loss: 0.4779830276966095\n",
      "Fake samples Training loss: 1.1453678607940674\n",
      "Real samples Training loss: 0.5031832456588745\n",
      "Fake samples Training loss: 1.2830806970596313\n",
      "Real samples Training loss: 0.4059305191040039\n",
      "Fake samples Training loss: 1.245072603225708\n",
      "Real samples Training loss: 0.4849402904510498\n",
      "Fake samples Training loss: 1.1055281162261963\n",
      "Real samples Training loss: 0.459839791059494\n",
      "Fake samples Training loss: 1.1475989818572998\n",
      "Real samples Training loss: 0.46283596754074097\n",
      "Fake samples Training loss: 1.1852762699127197\n",
      "Real samples Training loss: 0.529758632183075\n",
      "Fake samples Training loss: 1.217276692390442\n",
      "Real samples Training loss: 0.41960832476615906\n",
      "Fake samples Training loss: 1.2021974325180054\n",
      "Real samples Training loss: 0.4579671323299408\n",
      "Fake samples Training loss: 1.129469871520996\n",
      "Real samples Training loss: 0.41987287998199463\n",
      "Fake samples Training loss: 1.475737452507019\n",
      "Real samples Training loss: 0.46638768911361694\n",
      "Fake samples Training loss: 1.273149013519287\n",
      "Real samples Training loss: 0.4017109274864197\n",
      "Fake samples Training loss: 1.3600654602050781\n",
      "Real samples Training loss: 0.46039271354675293\n",
      "Fake samples Training loss: 1.200425386428833\n",
      "Real samples Training loss: 0.49062371253967285\n",
      "Fake samples Training loss: 1.3049870729446411\n",
      "Real samples Training loss: 0.5128811597824097\n",
      "Fake samples Training loss: 1.3272607326507568\n",
      "Real samples Training loss: 0.41287940740585327\n",
      "Fake samples Training loss: 1.325884222984314\n",
      "Real samples Training loss: 0.5017102956771851\n",
      "Fake samples Training loss: 1.2813999652862549\n",
      "Real samples Training loss: 0.49665749073028564\n",
      "Fake samples Training loss: 1.2717528343200684\n",
      "Real samples Training loss: 0.4833877682685852\n",
      "Fake samples Training loss: 1.2721233367919922\n",
      "Real samples Training loss: 0.5404927730560303\n",
      "Fake samples Training loss: 1.277754306793213\n",
      "Real samples Training loss: 0.4927075505256653\n",
      "Fake samples Training loss: 1.2110403776168823\n",
      "Real samples Training loss: 0.4885213077068329\n",
      "Fake samples Training loss: 1.1438497304916382\n",
      "Real samples Training loss: 0.5190805792808533\n",
      "Fake samples Training loss: 1.301937460899353\n",
      "Real samples Training loss: 0.4295414388179779\n",
      "Fake samples Training loss: 1.2103852033615112\n",
      "Real samples Training loss: 0.5173287391662598\n",
      "Fake samples Training loss: 1.2247201204299927\n",
      "Real samples Training loss: 0.42176544666290283\n",
      "Fake samples Training loss: 1.2672085762023926\n",
      "Real samples Training loss: 0.5072937607765198\n",
      "Fake samples Training loss: 1.2534971237182617\n",
      "Real samples Training loss: 0.5316767692565918\n",
      "Fake samples Training loss: 1.1401208639144897\n",
      "Real samples Training loss: 0.4672355353832245\n",
      "Fake samples Training loss: 1.2344589233398438\n",
      "Real samples Training loss: 0.4756750166416168\n",
      "Fake samples Training loss: 1.2500724792480469\n",
      "Real samples Training loss: 0.5301944017410278\n",
      "Fake samples Training loss: 1.1930129528045654\n",
      "Real samples Training loss: 0.4424284100532532\n",
      "Fake samples Training loss: 1.3590881824493408\n",
      "Real samples Training loss: 0.501311182975769\n",
      "Fake samples Training loss: 1.2017216682434082\n",
      "Real samples Training loss: 0.5045667886734009\n",
      "Fake samples Training loss: 1.2349029779434204\n",
      "Real samples Training loss: 0.4523943066596985\n",
      "Fake samples Training loss: 1.267840027809143\n",
      "Real samples Training loss: 0.4907814562320709\n",
      "Fake samples Training loss: 1.2047585248947144\n",
      "Real samples Training loss: 0.4805702865123749\n",
      "Fake samples Training loss: 1.292513370513916\n",
      "Real samples Training loss: 0.5047328472137451\n",
      "Fake samples Training loss: 1.3711588382720947\n",
      "Real samples Training loss: 0.4814215302467346\n",
      "Fake samples Training loss: 1.1510608196258545\n",
      "Real samples Training loss: 0.4915476441383362\n",
      "Fake samples Training loss: 1.2096871137619019\n",
      "Real samples Training loss: 0.4749722182750702\n",
      "Fake samples Training loss: 1.221610188484192\n",
      "Real samples Training loss: 0.4693618714809418\n",
      "Fake samples Training loss: 1.2187168598175049\n",
      "Real samples Training loss: 0.47213688492774963\n",
      "Fake samples Training loss: 1.2874559164047241\n",
      "Real samples Training loss: 0.46968743205070496\n",
      "Fake samples Training loss: 1.1425530910491943\n",
      "Real samples Training loss: 0.5215036869049072\n",
      "Fake samples Training loss: 1.2052626609802246\n",
      "Real samples Training loss: 0.4834052622318268\n",
      "Fake samples Training loss: 1.1681567430496216\n",
      "Real samples Training loss: 0.4934358298778534\n",
      "Fake samples Training loss: 1.3841184377670288\n",
      "Real samples Training loss: 0.4573509395122528\n",
      "Fake samples Training loss: 1.1921119689941406\n",
      "Real samples Training loss: 0.4553791880607605\n",
      "Fake samples Training loss: 1.458471655845642\n",
      "Real samples Training loss: 0.46588134765625\n",
      "Fake samples Training loss: 1.088108777999878\n",
      "Real samples Training loss: 0.5347579717636108\n",
      "Fake samples Training loss: 1.1829618215560913\n",
      "Real samples Training loss: 0.46539363265037537\n",
      "Fake samples Training loss: 1.1717596054077148\n",
      "Real samples Training loss: 0.5659399628639221\n",
      "Fake samples Training loss: 1.3192683458328247\n",
      "Real samples Training loss: 0.4576888382434845\n",
      "Fake samples Training loss: 1.2470186948776245\n",
      "Real samples Training loss: 0.45765501260757446\n",
      "Fake samples Training loss: 1.2301647663116455\n",
      "Real samples Training loss: 0.4090254306793213\n",
      "Fake samples Training loss: 1.2504470348358154\n",
      "Real samples Training loss: 0.46267595887184143\n",
      "Fake samples Training loss: 1.3754305839538574\n",
      "Real samples Training loss: 0.4493250548839569\n",
      "Fake samples Training loss: 1.1669191122055054\n",
      "Real samples Training loss: 0.4412516951560974\n",
      "Fake samples Training loss: 1.2062973976135254\n",
      "Real samples Training loss: 0.4986405670642853\n",
      "Fake samples Training loss: 1.1991055011749268\n",
      "Real samples Training loss: 0.4987153708934784\n",
      "Fake samples Training loss: 1.0809519290924072\n",
      "Real samples Training loss: 0.5299367904663086\n",
      "Fake samples Training loss: 1.3551584482192993\n",
      "Real samples Training loss: 0.44493600726127625\n",
      "Fake samples Training loss: 1.1054140329360962\n",
      "Real samples Training loss: 0.43791380524635315\n",
      "Fake samples Training loss: 1.215311884880066\n",
      "Real samples Training loss: 0.47546640038490295\n",
      "Fake samples Training loss: 1.2443310022354126\n",
      "Real samples Training loss: 0.4940815269947052\n",
      "Fake samples Training loss: 1.3363380432128906\n",
      "Real samples Training loss: 0.43012142181396484\n",
      "Fake samples Training loss: 1.2793644666671753\n",
      "Real samples Training loss: 0.5670350193977356\n",
      "Fake samples Training loss: 1.156385064125061\n",
      "Real samples Training loss: 0.46557649970054626\n",
      "Fake samples Training loss: 1.2288776636123657\n",
      "Real samples Training loss: 0.5859130620956421\n",
      "Fake samples Training loss: 1.2810568809509277\n",
      "Real samples Training loss: 0.4614233076572418\n",
      "Fake samples Training loss: 1.2257928848266602\n",
      "Real samples Training loss: 0.47094130516052246\n",
      "Fake samples Training loss: 1.3786355257034302\n",
      "Real samples Training loss: 0.5027517676353455\n",
      "Fake samples Training loss: 1.3043742179870605\n",
      "Real samples Training loss: 0.47327959537506104\n",
      "Fake samples Training loss: 1.1798734664916992\n",
      "Real samples Training loss: 0.4903286397457123\n",
      "Fake samples Training loss: 1.2394417524337769\n",
      "Real samples Training loss: 0.501309871673584\n",
      "Fake samples Training loss: 1.2329299449920654\n",
      "Real samples Training loss: 0.4712107181549072\n",
      "Fake samples Training loss: 1.2525298595428467\n",
      "Real samples Training loss: 0.4771842956542969\n",
      "Fake samples Training loss: 1.1174601316452026\n",
      "Real samples Training loss: 0.4034994840621948\n",
      "Fake samples Training loss: 1.1386266946792603\n",
      "Real samples Training loss: 0.4466959238052368\n",
      "Fake samples Training loss: 1.1458076238632202\n",
      "Real samples Training loss: 0.5018943548202515\n",
      "Fake samples Training loss: 1.1905114650726318\n",
      "Real samples Training loss: 0.44221121072769165\n",
      "Fake samples Training loss: 1.1954962015151978\n",
      "Real samples Training loss: 0.41020748019218445\n",
      "Fake samples Training loss: 1.3278617858886719\n",
      "Real samples Training loss: 0.5585297346115112\n",
      "Fake samples Training loss: 1.1659953594207764\n",
      "Real samples Training loss: 0.48616597056388855\n",
      "Fake samples Training loss: 1.1051301956176758\n",
      "Real samples Training loss: 0.4336872696876526\n",
      "Fake samples Training loss: 1.1752259731292725\n",
      "Real samples Training loss: 0.519783079624176\n",
      "Fake samples Training loss: 1.3403244018554688\n",
      "Real samples Training loss: 0.4556428790092468\n",
      "Fake samples Training loss: 1.1161731481552124\n",
      "Real samples Training loss: 0.4882046580314636\n",
      "Fake samples Training loss: 1.1074512004852295\n",
      "Real samples Training loss: 0.5495870113372803\n",
      "Fake samples Training loss: 1.3895649909973145\n",
      "Real samples Training loss: 0.4891187250614166\n",
      "Fake samples Training loss: 1.224485158920288\n",
      "Real samples Training loss: 0.48644593358039856\n",
      "Fake samples Training loss: 1.2111965417861938\n",
      "Real samples Training loss: 0.44483694434165955\n",
      "Fake samples Training loss: 1.2031546831130981\n",
      "Real samples Training loss: 0.4765724539756775\n",
      "Fake samples Training loss: 1.1778510808944702\n",
      "Real samples Training loss: 0.41887351870536804\n",
      "Fake samples Training loss: 1.2503641843795776\n",
      "Real samples Training loss: 0.5786510109901428\n",
      "Fake samples Training loss: 1.2712794542312622\n",
      "Real samples Training loss: 0.43773889541625977\n",
      "Fake samples Training loss: 1.141149878501892\n",
      "Real samples Training loss: 0.5117406845092773\n",
      "Fake samples Training loss: 1.1114805936813354\n",
      "Real samples Training loss: 0.46787309646606445\n",
      "Fake samples Training loss: 1.1861042976379395\n",
      "Real samples Training loss: 0.5013830661773682\n",
      "Fake samples Training loss: 1.468291163444519\n",
      "Real samples Training loss: 0.4466269016265869\n",
      "Fake samples Training loss: 1.3678232431411743\n",
      "Real samples Training loss: 0.4302729368209839\n",
      "Fake samples Training loss: 1.1615984439849854\n",
      "Real samples Training loss: 0.4791378378868103\n",
      "Fake samples Training loss: 1.120858073234558\n",
      "Real samples Training loss: 0.49591395258903503\n",
      "Fake samples Training loss: 1.007573127746582\n",
      "Real samples Training loss: 0.4249134957790375\n",
      "Fake samples Training loss: 1.2598402500152588\n",
      "Real samples Training loss: 0.4673624634742737\n",
      "Fake samples Training loss: 1.2036223411560059\n",
      "Real samples Training loss: 0.46988579630851746\n",
      "Fake samples Training loss: 1.2442257404327393\n",
      "Real samples Training loss: 0.44662803411483765\n",
      "Fake samples Training loss: 1.194202184677124\n",
      "Real samples Training loss: 0.49317169189453125\n",
      "Fake samples Training loss: 1.1250252723693848\n",
      "Real samples Training loss: 0.4326059818267822\n",
      "Fake samples Training loss: 1.3259438276290894\n",
      "Real samples Training loss: 0.4482499659061432\n",
      "Fake samples Training loss: 1.3687424659729004\n",
      "Real samples Training loss: 0.47077295184135437\n",
      "Fake samples Training loss: 1.4533225297927856\n",
      "Real samples Training loss: 0.5410374999046326\n",
      "Fake samples Training loss: 1.227795124053955\n",
      "Real samples Training loss: 0.46968284249305725\n",
      "Fake samples Training loss: 1.2845971584320068\n",
      "Real samples Training loss: 0.4552445113658905\n",
      "Fake samples Training loss: 1.1194285154342651\n",
      "Real samples Training loss: 0.5288082957267761\n",
      "Fake samples Training loss: 1.1505544185638428\n",
      "Real samples Training loss: 0.5098846554756165\n",
      "Fake samples Training loss: 1.286150336265564\n",
      "Real samples Training loss: 0.4628167152404785\n",
      "Fake samples Training loss: 1.1265437602996826\n",
      "Real samples Training loss: 0.5147989392280579\n",
      "Fake samples Training loss: 1.2470351457595825\n",
      "Real samples Training loss: 0.503770112991333\n",
      "Fake samples Training loss: 1.5577514171600342\n"
     ]
    }
   ],
   "source": [
    "discriminator_model =  define_discriminator()\n",
    "train_discriminator(discriminator_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8465],\n",
       "        [0.6038],\n",
       "        [0.5488],\n",
       "        [0.5307],\n",
       "        [0.4677],\n",
       "        [0.9347],\n",
       "        [0.6792],\n",
       "        [0.7909],\n",
       "        [0.7896],\n",
       "        [0.7920],\n",
       "        [0.8475],\n",
       "        [0.7878],\n",
       "        [0.4252],\n",
       "        [0.4030],\n",
       "        [0.8023],\n",
       "        [0.5922],\n",
       "        [0.7251],\n",
       "        [0.4534],\n",
       "        [0.6176],\n",
       "        [0.8727],\n",
       "        [0.4964],\n",
       "        [0.6127],\n",
       "        [0.4838],\n",
       "        [0.8891],\n",
       "        [0.4872],\n",
       "        [0.6352],\n",
       "        [0.7892],\n",
       "        [0.4882],\n",
       "        [0.6954],\n",
       "        [0.6657],\n",
       "        [0.7957],\n",
       "        [0.5689],\n",
       "        [0.4117],\n",
       "        [0.8102],\n",
       "        [0.7154],\n",
       "        [0.8292],\n",
       "        [0.7977],\n",
       "        [0.7661],\n",
       "        [0.9056],\n",
       "        [0.3527],\n",
       "        [0.8888],\n",
       "        [0.7041],\n",
       "        [0.9210],\n",
       "        [0.7894],\n",
       "        [0.8046],\n",
       "        [0.5350],\n",
       "        [0.9502],\n",
       "        [0.4232],\n",
       "        [0.7101],\n",
       "        [0.7320],\n",
       "        [0.6874],\n",
       "        [0.6775],\n",
       "        [0.5622],\n",
       "        [0.7694],\n",
       "        [0.9192],\n",
       "        [0.5346],\n",
       "        [0.8883],\n",
       "        [0.5127],\n",
       "        [0.5139],\n",
       "        [0.7005],\n",
       "        [0.7977],\n",
       "        [0.5244],\n",
       "        [0.5993],\n",
       "        [0.4377]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = generate_samples(64)\n",
    "discriminator_model(torch.from_numpy(X).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator model takes as input a point from the latent space and generates a new sample. A latent variable is a hidden or unobserved variable, and a latent space is a multi-dimensional vector space of these variables. We can define the size of the latent space for our problem and the shape or distribution of variables in the latent space.\n",
    "\n",
    "This is because the latent space has no meaning until the generator model starts assigning meaning to points in the space as it learns. After training, points in the latent space will correspond to points in the output space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use gaussian distribution to represent the variables in the latent space. We will generate new inputs by \n",
    "drawing random numbers from a standard Gaussian distribution, i.e. mean of zero and a standard deviation of one.\n",
    "\n",
    "Inputs: Point in latent space i.e  vector of Gaussian random numbers.\n",
    "Outputs: Two-element vector representing a generated sample for our function (x and 21 + x*x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(n_inputs):\n",
    "    model = nn.Sequential(nn.Linear(n_inputs, 15),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(),\n",
    "                          nn.Linear(15,15),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(15, 2), # 2 here refers to number of outputs(can be parameterized)\n",
    "                         )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"343pt\" height=\"864pt\"\n",
       " viewBox=\"0.00 0.00 343.49 864.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.842927 0.842927) rotate(0) translate(4 1021)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1021 403.5,-1021 403.5,4 -4,4\"/>\n",
       "<!-- 139992067299368 -->\n",
       "<g id=\"node1\" class=\"node\"><title>139992067299368</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"374,-21 269,-21 269,-0 374,-0 374,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"321.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">SigmoidBackward</text>\n",
       "</g>\n",
       "<!-- 139992067298920 -->\n",
       "<g id=\"node2\" class=\"node\"><title>139992067298920</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"367.5,-78 275.5,-78 275.5,-57 367.5,-57 367.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"321.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139992067298920&#45;&gt;139992067299368 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>139992067298920&#45;&gt;139992067299368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M321.5,-56.9197C321.5,-49.9083 321.5,-40.1442 321.5,-31.4652\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"325,-31.3408 321.5,-21.3408 318,-31.3409 325,-31.3408\"/>\n",
       "</g>\n",
       "<!-- 139992067299424 -->\n",
       "<g id=\"node3\" class=\"node\"><title>139992067299424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"327,-142 216,-142 216,-121 327,-121 327,-142\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-128.4\" font-family=\"Times,serif\" font-size=\"12.00\">SqueezeBackward3</text>\n",
       "</g>\n",
       "<!-- 139992067299424&#45;&gt;139992067298920 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>139992067299424&#45;&gt;139992067298920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.312,-120.812C286.754,-111.585 298.133,-97.4746 307.199,-86.233\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"310.024,-88.3054 313.577,-78.3241 304.575,-83.9111 310.024,-88.3054\"/>\n",
       "</g>\n",
       "<!-- 139990935449104 -->\n",
       "<g id=\"node4\" class=\"node\"><title>139990935449104</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"314,-206 229,-206 229,-185 314,-185 314,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-192.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 139990935449104&#45;&gt;139992067299424 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>139990935449104&#45;&gt;139992067299424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M271.5,-184.812C271.5,-176.218 271.5,-163.388 271.5,-152.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"275,-152.324 271.5,-142.324 268,-152.324 275,-152.324\"/>\n",
       "</g>\n",
       "<!-- 139990935447312 -->\n",
       "<g id=\"node5\" class=\"node\"><title>139990935447312</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"275.5,-263 151.5,-263 151.5,-242 275.5,-242 275.5,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-249.4\" font-family=\"Times,serif\" font-size=\"12.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935447312&#45;&gt;139990935449104 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>139990935447312&#45;&gt;139990935449104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M223.603,-241.92C231.9,-234.051 243.854,-222.716 253.743,-213.338\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"256.274,-215.761 261.122,-206.341 251.458,-210.682 256.274,-215.761\"/>\n",
       "</g>\n",
       "<!-- 139990935446528 -->\n",
       "<g id=\"node6\" class=\"node\"><title>139990935446528</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"260.5,-327 166.5,-327 166.5,-306 260.5,-306 260.5,-327\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-313.4\" font-family=\"Times,serif\" font-size=\"12.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139990935446528&#45;&gt;139990935447312 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>139990935446528&#45;&gt;139990935447312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213.5,-305.812C213.5,-297.218 213.5,-284.388 213.5,-273.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217,-273.324 213.5,-263.324 210,-273.324 217,-273.324\"/>\n",
       "</g>\n",
       "<!-- 139990934517520 -->\n",
       "<g id=\"node7\" class=\"node\"><title>139990934517520</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"259.5,-391 167.5,-391 167.5,-370 259.5,-370 259.5,-391\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-377.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139990934517520&#45;&gt;139990935446528 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>139990934517520&#45;&gt;139990935446528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213.5,-369.812C213.5,-361.218 213.5,-348.388 213.5,-337.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217,-337.324 213.5,-327.324 210,-337.324 217,-337.324\"/>\n",
       "</g>\n",
       "<!-- 139990934517128 -->\n",
       "<g id=\"node8\" class=\"node\"><title>139990934517128</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"219,-455 108,-455 108,-434 219,-434 219,-455\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-441.4\" font-family=\"Times,serif\" font-size=\"12.00\">SqueezeBackward3</text>\n",
       "</g>\n",
       "<!-- 139990934517128&#45;&gt;139990934517520 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>139990934517128&#45;&gt;139990934517520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.312,-433.812C178.754,-424.585 190.133,-410.475 199.199,-399.233\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"202.024,-401.305 205.577,-391.324 196.575,-396.911 202.024,-401.305\"/>\n",
       "</g>\n",
       "<!-- 139990934517072 -->\n",
       "<g id=\"node9\" class=\"node\"><title>139990934517072</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"206,-519 121,-519 121,-498 206,-498 206,-519\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-505.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 139990934517072&#45;&gt;139990934517128 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>139990934517072&#45;&gt;139990934517128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-497.812C163.5,-489.218 163.5,-476.388 163.5,-465.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-465.324 163.5,-455.324 160,-465.324 167,-465.324\"/>\n",
       "</g>\n",
       "<!-- 139990934519424 -->\n",
       "<g id=\"node10\" class=\"node\"><title>139990934519424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"167.5,-576 43.5,-576 43.5,-555 167.5,-555 167.5,-576\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-562.4\" font-family=\"Times,serif\" font-size=\"12.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 139990934519424&#45;&gt;139990934517072 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>139990934519424&#45;&gt;139990934517072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.603,-554.92C123.9,-547.051 135.854,-535.716 145.743,-526.338\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"148.274,-528.761 153.122,-519.341 143.458,-523.682 148.274,-528.761\"/>\n",
       "</g>\n",
       "<!-- 139990934518976 -->\n",
       "<g id=\"node11\" class=\"node\"><title>139990934518976</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-640 60,-640 60,-619 151,-619 151,-640\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-626.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139990934518976&#45;&gt;139990934519424 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>139990934518976&#45;&gt;139990934519424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105.5,-618.812C105.5,-610.218 105.5,-597.388 105.5,-586.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109,-586.324 105.5,-576.324 102,-586.324 109,-586.324\"/>\n",
       "</g>\n",
       "<!-- 139990934518920 -->\n",
       "<g id=\"node12\" class=\"node\"><title>139990934518920</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"152.5,-704 58.5,-704 58.5,-683 152.5,-683 152.5,-704\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-690.4\" font-family=\"Times,serif\" font-size=\"12.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139990934518920&#45;&gt;139990934518976 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>139990934518920&#45;&gt;139990934518976</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105.5,-682.812C105.5,-674.218 105.5,-661.388 105.5,-650.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109,-650.324 105.5,-640.324 102,-650.324 109,-650.324\"/>\n",
       "</g>\n",
       "<!-- 139990934518752 -->\n",
       "<g id=\"node13\" class=\"node\"><title>139990934518752</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151.5,-761 59.5,-761 59.5,-740 151.5,-740 151.5,-761\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-747.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139990934518752&#45;&gt;139990934518920 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>139990934518752&#45;&gt;139990934518920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105.5,-739.92C105.5,-732.908 105.5,-723.144 105.5,-714.465\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109,-714.341 105.5,-704.341 102,-714.341 109,-714.341\"/>\n",
       "</g>\n",
       "<!-- 139990934519368 -->\n",
       "<g id=\"node14\" class=\"node\"><title>139990934519368</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"111,-825 7.10543e-15,-825 7.10543e-15,-804 111,-804 111,-825\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-811.4\" font-family=\"Times,serif\" font-size=\"12.00\">SqueezeBackward3</text>\n",
       "</g>\n",
       "<!-- 139990934519368&#45;&gt;139990934518752 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>139990934519368&#45;&gt;139990934518752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M63.3125,-803.812C70.754,-794.585 82.1334,-780.475 91.1992,-769.233\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"94.0242,-771.305 97.5774,-761.324 88.5753,-766.911 94.0242,-771.305\"/>\n",
       "</g>\n",
       "<!-- 139990934875104 -->\n",
       "<g id=\"node15\" class=\"node\"><title>139990934875104</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"98,-889 13,-889 13,-868 98,-868 98,-889\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-875.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 139990934875104&#45;&gt;139990934519368 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>139990934875104&#45;&gt;139990934519368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.5,-867.812C55.5,-859.218 55.5,-846.388 55.5,-835.585\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.0001,-835.324 55.5,-825.324 52.0001,-835.324 59.0001,-835.324\"/>\n",
       "</g>\n",
       "<!-- 139990934875944 -->\n",
       "<g id=\"node16\" class=\"node\"><title>139990934875944</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"92,-946 19,-946 19,-925 92,-925 92,-946\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-932.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 139990934875944&#45;&gt;139990934875104 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>139990934875944&#45;&gt;139990934875104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.5,-924.92C55.5,-917.908 55.5,-908.144 55.5,-899.465\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.0001,-899.341 55.5,-889.341 52.0001,-899.341 59.0001,-899.341\"/>\n",
       "</g>\n",
       "<!-- 139990934875832 -->\n",
       "<g id=\"node17\" class=\"node\"><title>139990934875832</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"82.5,-1017 28.5,-1017 28.5,-982 82.5,-982 82.5,-1017\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-989.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (15, 2)</text>\n",
       "</g>\n",
       "<!-- 139990934875832&#45;&gt;139990934875944 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>139990934875832&#45;&gt;139990934875944</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.5,-981.885C55.5,-973.994 55.5,-964.505 55.5,-956.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.0001,-956.018 55.5,-946.018 52.0001,-956.018 59.0001,-956.018\"/>\n",
       "</g>\n",
       "<!-- 139990934516288 -->\n",
       "<g id=\"node18\" class=\"node\"><title>139990934516288</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"183.5,-832 129.5,-832 129.5,-797 183.5,-797 183.5,-832\"/>\n",
       "<text text-anchor=\"middle\" x=\"156.5\" y=\"-804.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (15)</text>\n",
       "</g>\n",
       "<!-- 139990934516288&#45;&gt;139990934518752 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>139990934516288&#45;&gt;139990934518752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.833,-796.885C135.678,-788.187 126.926,-777.547 119.694,-768.756\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.385,-766.517 113.329,-761.018 116.979,-770.964 122.385,-766.517\"/>\n",
       "</g>\n",
       "<!-- 139990934517016 -->\n",
       "<g id=\"node19\" class=\"node\"><title>139990934517016</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"259,-576 186,-576 186,-555 259,-555 259,-576\"/>\n",
       "<text text-anchor=\"middle\" x=\"222.5\" y=\"-562.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 139990934517016&#45;&gt;139990934517072 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>139990934517016&#45;&gt;139990934517072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212.223,-554.92C203.782,-547.051 191.623,-535.716 181.563,-526.338\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.758,-523.599 174.057,-519.341 178.985,-528.72 183.758,-523.599\"/>\n",
       "</g>\n",
       "<!-- 139990934517968 -->\n",
       "<g id=\"node20\" class=\"node\"><title>139990934517968</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"251,-647 194,-647 194,-612 251,-612 251,-647\"/>\n",
       "<text text-anchor=\"middle\" x=\"222.5\" y=\"-619.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (15, 15)</text>\n",
       "</g>\n",
       "<!-- 139990934517968&#45;&gt;139990934517016 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>139990934517968&#45;&gt;139990934517016</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.5,-611.885C222.5,-603.994 222.5,-594.505 222.5,-586.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226,-586.018 222.5,-576.018 219,-586.018 226,-586.018\"/>\n",
       "</g>\n",
       "<!-- 139990934515952 -->\n",
       "<g id=\"node21\" class=\"node\"><title>139990934515952</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"291.5,-462 237.5,-462 237.5,-427 291.5,-427 291.5,-462\"/>\n",
       "<text text-anchor=\"middle\" x=\"264.5\" y=\"-434.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (15)</text>\n",
       "</g>\n",
       "<!-- 139990934515952&#45;&gt;139990934517520 -->\n",
       "<g id=\"edge20\" class=\"edge\"><title>139990934515952&#45;&gt;139990934517520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M250.833,-426.885C243.678,-418.187 234.926,-407.547 227.694,-398.756\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"230.385,-396.517 221.329,-391.018 224.979,-400.964 230.385,-396.517\"/>\n",
       "</g>\n",
       "<!-- 139990935449160 -->\n",
       "<g id=\"node22\" class=\"node\"><title>139990935449160</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"367,-263 294,-263 294,-242 367,-242 367,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"330.5\" y=\"-249.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 139990935449160&#45;&gt;139990935449104 -->\n",
       "<g id=\"edge21\" class=\"edge\"><title>139990935449160&#45;&gt;139990935449104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M320.223,-241.92C311.782,-234.051 299.623,-222.716 289.563,-213.338\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.758,-210.599 282.057,-206.341 286.985,-215.72 291.758,-210.599\"/>\n",
       "</g>\n",
       "<!-- 139990934515840 -->\n",
       "<g id=\"node23\" class=\"node\"><title>139990934515840</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"357.5,-334 303.5,-334 303.5,-299 357.5,-299 357.5,-334\"/>\n",
       "<text text-anchor=\"middle\" x=\"330.5\" y=\"-306.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1, 15)</text>\n",
       "</g>\n",
       "<!-- 139990934515840&#45;&gt;139990935449160 -->\n",
       "<g id=\"edge22\" class=\"edge\"><title>139990934515840&#45;&gt;139990935449160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.5,-298.885C330.5,-290.994 330.5,-281.505 330.5,-273.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"334,-273.018 330.5,-263.018 327,-273.018 334,-273.018\"/>\n",
       "</g>\n",
       "<!-- 139990935445744 -->\n",
       "<g id=\"node24\" class=\"node\"><title>139990935445744</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"399.5,-149 345.5,-149 345.5,-114 399.5,-114 399.5,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"372.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139990935445744&#45;&gt;139992067298920 -->\n",
       "<g id=\"edge23\" class=\"edge\"><title>139990935445744&#45;&gt;139992067298920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M358.833,-113.885C351.678,-105.187 342.926,-94.5469 335.694,-85.756\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"338.385,-83.5172 329.329,-78.0178 332.979,-87.9641 338.385,-83.5172\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f5271715da0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model  = define_generator(5)\n",
    "\n",
    "make_dot(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=15, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=15, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n):\n",
    "    # generate points in the latent space\n",
    "    x_input = np.random.randn(latent_dim * n)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n)\n",
    "    # predict outputs\n",
    "    x_input = Variable(torch.from_numpy(x_input)).float()\n",
    "    X = generator(x_input)\n",
    "    X = X.data.numpy()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAc1klEQVR4nO3df5Acd3nn8fejRTZyCKyNVQSttJZIFFNyDFaY2CQ6AhinbOJgq8BgGbgzVaZUPuK6EIIqS5lyfL67QqBKgCpcueg4riCE+Bdks4UVlCCZqsQVcVrdytaJQ7FwbEtrBwS2SEDCWknP/TGz9uxs90z3dPfMt7s/ryqV50fP9DMz6376+3x/tLk7IiJSX0uGHYCIiAyXEoGISM0pEYiI1JwSgYhIzSkRiIjU3EuGHUCcCy+80FevXj3sMERESmXfvn0/dPflaV4TbCJYvXo109PTww5DRKRUzOzJtK9RaUhEpOaUCEREak6JQESk5pQIRERqTolARKTmlAhERGpOiUBEpOZySQRmdo2ZHTKzw2Y20WW7d5mZm1kjj/2KiEh2mROBmY0AdwNvB9YBN5nZuojtfh74PeDbWfcpIiL5yaNFcDlw2N0fd/dTwD3A9RHb/Rfgk8DPctiniIjkJI8lJsaAI233jwJXtG9gZr8KrHL3B81sSw77FBGphMmZWbbtPMTTx0+yYnQZW66+mI3rxwYaQ+FrDZnZEuBPgA8k2HYzsBlgfHy82MBERIZscmaWj33tACfnzgAwe/wkH/vaAYCBJoM8SkOzwKq2+ytbj837eeBXgG+Z2RPAG4GpqA5jd9/u7g13byxfnmrxPBGR0tm289ALSWDeybkzbNt5aKBx5NEi2AusNbM1NBPAJuC980+6+4+BC+fvm9m3gI+6u5YWFamQEEocZfP08ZOpHi9K5haBu58GbgN2Av8PuM/dD5rZXWZ2Xdb3F5HwzZc4Zo+fxHmxxDE5M9vztXW2YnRZqseLkss8Anff4e6/7O6/6O7/rfXYHe4+FbHtW9QaEKmWUEocZbPl6otZtnRkwWPLlo6w5eqLBxpHsBemEZHyCKXEUTbzpbNhl9SUCEQksxWjy5iNOOgPusRRRhvXjw29L0VrDYlIZqGUOKQ/ahGISGahlDikP0oEIpKLEEoc0h+VhkREak6JQESk5pQIRERqTn0EIhWk5R4kDSUCkYoJZUVLKQ+VhkQqRss9SFpKBCIVo+UeJC0lApGKCWVFSykPJQKRitFyD8WanJllw9bdrJl4kA1bd1diqW11FotUjJZ7KE5VO+KVCEQqSMs9FKNbR3yZv2+VhkREEqpqR7wSgYhIQlXtiFciEBFJqKod8eojEBFJqKod8UoEIiIpVLEjXqUhEZGaUyIQEak5JQIRkZpTIhARqblcEoGZXWNmh8zssJlNRDx/q5kdMLP9ZvYPZrYuj/2KiEh2mROBmY0AdwNvB9YBN0Uc6L/i7pe6+2XAp4A/ybpfERHJRx4tgsuBw+7+uLufAu4Brm/fwN3/te3uzwGew35FRCQHecwjGAOOtN0/ClzRuZGZ/S7wEeAc4MqoNzKzzcBmgPHx8RxCExGRXgbWWezud7v7LwJ/CHw8Zpvt7t5w98by5csHFZqI1EgVryeQVR4tgllgVdv9la3H4twD/GkO+xURSaWq1xPIKo8WwV5grZmtMbNzgE3AVPsGZra27e61wGM57FdEJJVu1xOos8wtAnc/bWa3ATuBEeAL7n7QzO4Cpt19CrjNzK4C5oDngJuz7ldEJK2qXk8gq1wWnXP3HcCOjsfuaLv9e3nsR0QkixWjy5iNOOiX/XoCWWlmsYjURlWvJ5CVlqEWkdqo6vUEslIiEJFaqeL1BLJSIhBJaHJmVmeSUklKBCIJaPy5VJk6i0US0PhzqTIlApEENP5cqkyJQCSBuHHmdR9/LtWgRCCSgMafS5Wps1gkAY0/lypTIhBJSOPPpapUGhIRqTklAhGRmlMiEBGpOSUCEZGaUyIQEak5JQIRkZpTIhARqTklAhGRmlMiEBGpOSUCEZGaUyIQEak5rTUkIkHRJUEHT4lARIKhS4IOhxKBSODqdIbc7ZKgVf3MIcilj8DMrjGzQ2Z22MwmIp7/iJl9x8weNbNdZnZRHvsVqbr5M+TZ4ydxXjxDnpyZHXZohdAlQYcjcyIwsxHgbuDtwDrgJjNb17HZDNBw99cBDwCfyrpfkTrodoZcRbok6HDk0SK4HDjs7o+7+yngHuD69g3c/SF3P9G6uwdYmcN+RSqvbmfIuiTocOSRCMaAI233j7Yei3ML8DdRT5jZZjObNrPpY8eO5RCaSLnV7Qx54/oxPvHOSxkbXYYBY6PL+MQ7L1X/QMEG2llsZu8HGsCbo5539+3AdoBGo+EDDE0kSFuuvnjBKBqo/hmyLgk6eHkkgllgVdv9la3HFjCzq4DbgTe7+/M57Fek8uYPiHUZNSTDkUci2AusNbM1NBPAJuC97RuY2Xrgz4Br3P0HOexTpDZ0hixFy5wI3P20md0G7ARGgC+4+0EzuwuYdvcpYBvwMuB+MwN4yt2vy7pvEclHneYqyGK59BG4+w5gR8djd7TdviqP/YhI/gY9m1dJJzxadE6k5gY5V6FuE+TKQolApOYGOVehbhPkykKJQKTmBjlXoW4T5MpCiUCk5vqZzTs5M8uGrbtZM/EgG7buTlzaqdsEubJQIhCpubSzebPU+bWERJi0DLWIpJqrkGWp6BAmyGnU0mJKBCKSStY6/zAnyOnCN9FUGhKRVMpc59eopWhKBCKSSpnr/Bq1FE2lIZGM6lZz7rfOH8L3tGJ0GbMRB/0ytGaKpEQgkkFda85p6/yhfE91XNY7CSWCGgnhjKxqBn2x9bL+hqFclD6EUUshUiKoiVDOyKpmkDXnMv+GIdXmtaz3YuosrgmNlijGIEfQlPk3LPNIozpQIqiJkM7IqmSQI2jK/BuWeaRRHag0VBMaLREvS909r5pzkhjK/BuqNh82cw/zGvGNRsOnp6eHHUZldNaXoXlG1m1NmToI4XtJGkMIsUr4zGyfuzfSvEaloZpIu7BYXYRQd08aQ9l/w35XLJXiqTRUIxotsVgIdfc0MZT1NyzziKc6UItAai2E0SwhxFC0EFpeEk8tAqm1EGaaZokhqpMZwuuUDaHlJfGUCKTWQhjNkmXtns5yy5b7HwGDuTP+wmMhlGAGOeKprLOvh0mjhkQGKM+D1IatuyMPrlHGRpfx8MSVfe0nD4Ma8aSRVRo1JBK0LJd4jJKmrDLsEsygRjypL6I/uZSGzOwa4LPACPB5d9/a8fxvAp8BXgdscvcH8tivSJn0OkilbSnElVvito0T10r5+OQB/vLbRzjjzogZN12xisZFF2SafFf0Wbn6IvqTuUVgZiPA3cDbgXXATWa2rmOzp4APAF/Juj+Rsoo7GM23DNK2FKKWbVi6xFg6Ygse69bxHNdKed//+Ee+vOcpzrRKx2fc+fKep/jIfftza9EUoQ4jsIqQR2nocuCwuz/u7qeAe4Dr2zdw9yfc/VHgbA77ExmqfidGxR2MRsz6KmdElVu2vfv1bLvh9YlLMHGtlIe/92zk9mc7uhRDK7toTaP+5FEaGgOOtN0/ClzRzxuZ2WZgM8D4+Hj2yERylmViVNww0c4D8bwk5Yy4ckvSEkweJZOQyi4hjAIro6CGj7r7dmA7NEcNDTkckUWyXGAl7iC1beehoS0ml6afodt7hKSss6+HKY9EMAusaru/svWYSOVk7YyMOkhNP/ksX97z1KJt3/ra5ekD7KGzY/itr13OV/fNLmql/Or4KyLLQ0tsYXkolLKL5g5kk0cfwV5grZmtMbNzgE3AVA7vKxKcIjojH/rusVSP9yuqY/ir+2Z51xvGFvUpvLsxvujgsAR47xXjwS16l/ew3DrK3CJw99Nmdhuwk+bw0S+4+0EzuwuYdvcpM/s14K+A84F3mNl/dvdLsu5bpGhJz6CznBUPashjXFnroe8eWzTZbMPW3YtGdpyFyG2HLZTrIZdZLn0E7r4D2NHx2B1tt/fSLBmJlEZUx/D8GfRD3z2WWxliUMsvpEk4/SanYZRoNHcgu6A6i0VCkuYMOotBLXyXJuH0k5yGtdR0ma/cFgotMSESY1BnmoNafiHpGPvJmVlOnDq96PW9klNc4rxz6mDsa+LmZKSZq6G5A9mpRSASI+5M02nW0PMsewxiyGOSMfZRi7YBjC5byp3XXdI1xrgEefzkHJMzs4teG9eCmH7y2QX9ML1aFpo7kJ1WHxWJEXdQnFfFVS3jVjRNsnppt9VQo14ft/2I2QtLW6SNQbT6qEhXaZeGaC/ZRAlteYU8ZCmHdSvFpOmQjkoCSWOQ/qg0JLXQb0fmfMlmzcSDRB2e2g9OoU1q6ieeuHLYErPI8k7nfjonnM17xbKlifcV1yJQ529x1CKQWsi6Tn2viWShTWqanJlly/2PLIhny/2P9LWiKTTP0qM+T+fnjkoCAD89dXrRa+M6eW+6YpU6fwdMiaDG+l1Fs4yyjgDqNTIltAui3Dl1kLmOo/LcWedjX3u0628+Xw4bsYVLWUP054n63FHmzvii13aOlhpdtpSXLl3CX+x5ipcuXcLosqVBzWCuMiWCmgrtDLZoWZeG6DXEM7RJTcdPzkU+fnLubM/ffOP6Mc4mrNNnvUraxvVjPDxxJZ++8TKeP32W507M4cBzJ+Z4/vRZPn3jZTw8caWSQMGUCGoqtDPYom25+uJFF2xZOmKpyg3zB61/3nrtooNTWS+IEvebJ/08aT5ft23r9vcYGiWCmiryDDbYklPnSW6OI6dDm9R0/nmLO2fjRP3mST9PXJ9CJyP9qKJuj0u+lAhqqqgz2FBLTtt2Hoqsmed1xjmo2cFJ/dE7LlnUAoozGpE0kn6ezu3iON1HZ8X93Y2etzTMk4qK0fDRmipqfZtQV4IcxBlnSBdEiZpt+9bXLufevUeYO7MwIf7kZ6cjh4Ym/Tzt23WbkNZN1N/j0hHjJz87zXMnmv0dg1q7qI6UCGqqqGn5oTbxQ16YrKj5B1EH8q8/8syijuT5llHSfXaLt98TjKi/x58+f3pRrCfnzvAH9z2y4DWSnRJBjRVxBhvqAXdQK3ymNegVO38cM5ooaaKOivfD9+7nzqmDC9Yi6iexdf49rpl4MHK7+TkN86+R7JQIJFehHnBDXZhs0KW0rIk6bt7A8ZNzCw7O3WJP2gLqdj3lEMqNVaJEILkK9YALYdXw5w26lJY1UXeL6+TcGT5873627TwU+5unaQFFxZo0FklHiUByF+IBN6ui6viDLqVlTdTdztLndTu4p2kBzd//g/se0dpDBVMiEOmhyDp+0jP0PBNR0kQdtc9eZ+nz4g7uaVtA868PsdxYJZpHINJDkbNek4zXH8bcjLh9AnzinZcmmrAWdXCPWoW02+MQ3hyNKlKLQKSHouv4vc7QhzE3I26fH753P2Ojy/ijd1zywnZxpaKo0k3EWnZAczTT6tYooairoVWx3BgStQhEehj2OkLDmJvR7b3bWwcPT1zJZ268LPHyGsdPRA9fbe8BOH5yLtGS2ZIfJQKRHoa9jtAwElGv924vjaUp3SSNOc/lP6Q3lYZEehj2kNhhzM1I0inc3mpIWrpJ2tkM9BydJPlRIhBJYJg16jSJKK/RRe37TNMHkOZ9m5e2jL4sJTRXLI27PKbkyzzmR0j1JmbXAJ8FRoDPu/vWjufPBb4EvAH4EXCjuz/R7T0bjYZPT09njk2kLjqHuUKz5ZB1hE3c+77rDWM89N1jmZLO5MwsWx54ZNFCePPGRpfx8MSVfcdeR2a2z90baV6TuUVgZiPA3cBvAUeBvWY25e7fadvsFuA5d/8lM9sEfBK4Meu+RaRpcmY2cuJVHqOL4lYy/eq+2UVzK6affDZVcph/7sP37o98XrOHByOP0tDlwGF3fxzAzO4BrgfaE8H1wJ2t2w8AnzMz8zyaIyI9FDUrOBTzZ+xxJZY8DqadpbENW3dHDi/9iz1PvTACKOnEu43rx2JLUJo9PBh5JIIx4Ejb/aPAFXHbuPtpM/sx8Ergh+0bmdlmYDPA+Ph4DqFJ3Q16dc+s+klavS4gX8TBNC65dKai9tFF3T5XqIsV1kVQw0fdfbu7N9y9sXz58mGHIxVQpmvh9juDuNsZf1EH0zTJZf5zdPtcmj08XHm0CGaBVW33V7Yei9rmqJm9BHgFzU5jkUIVPRkrz7JTvzOI4xaCGzFLfDBN+zmizuCN6MtAj5gl+lyaPTw8ebQI9gJrzWyNmZ0DbAKmOraZAm5u3b4B2K3+ARmEIidj5b0GUL9JK27C2x+/5/WJDqwfnzzA79+7P9XniDqDf98bxyPjKLLvQvKRuUXQqvnfBuykOXz0C+5+0MzuAqbdfQr4n8Cfm9lh4FmayUKkcEXWnvNcA2hyZjZ2TH2vpJVlwtvkzOyCDt55ST5H1Bl846ILFsWhjuDw5TKhzN13ADs6Hruj7fbPgHfnsS+RNIqcFZxX2anbqJ+kSavfssq2nYciyznQ3xl7ZxyTM7P89PnTkdueOHVaE8YCoZnFUnlF1Z7zuqhM3KifNDX+brrV/7sd7LOesUdNRGv33Im5oEdw1UlQo4ZEyiSvxejiDsZn3XNJAt36MeIO9gYLPsfkzCwbtu5mzcSDbNi6O1E/SK9hrRDuCK66qXQi6OePV4pRxd8iryGPRXZo9xo+G5XMDHjfG8df+BxFDGvtZzspTmVLQ2WbSFRlVf4t8ig7Fdmh3asfI0kfSt7DWqO2k+GqbCIYxlWdJJp+i+6K7NBO0o/RK5llGdbaa8npzhKUDEdlE8Ewruok0fRb9Ja2ZZFkAljciJ20rY1+O8U7l7LunHDWWYKS4alsH8GwLy8oL9Jvka8kNfv5bY6fXHhpyPPPW5q6HyNLp/jG9WM8PHElT2y9lk/feNmC/pRP33gZ/3XjpYnjkOJUtkWgRazCod8iX0lKbXEjds475yWpz8DzKl1pCYlwVTYRDPvygvIi/Rb5SlJqy7scp4N4tVU2EYD+eEOi3yJaP4vWJanZ5zXZTeqhsn0EIqHrd3x+kpp91sluVZz3IfGUCESGpN9rJSSZyJZlslveq6pK+CpdGhIJWZY6fpJSW5aF6DTvo16UCCqo6tforYpQ6/ia91E/Kg1VjJr15ZHXonV507yP+lEiqJgyXaO37kK9Tm+oCUqKo9JQxahZXy791PGLLv1p3kf9KBFUTKh1Z8nHoFZy1byPelFpqGLUrK82lf6kCGoRVIya9dWm0p8UQYmggtSsry6V/qQIKg2JlIhKf1IEtQik1Mo4eS5LzCr9SRGUCKS0yngt5DxiVulP8papNGRmF5jZ35nZY63/nh+z3TfM7LiZfT3L/kTalXEETRljlurL2kcwAexy97XArtb9KNuAf59xXyILlHEETRljlurLmgiuB77Yuv1FYGPURu6+C/i3jPsSWaCMa+KUMWapvqyJ4FXu/kzr9r8Ar8r4fiKJlXEETZEXjNHFZKRfPTuLzeybwC9EPHV7+x13dzPzLMGY2WZgM8D4+HiWt5IaKOMImiwxd+toBkrXcS7hMPf+j91mdgh4i7s/Y2avBr7l7pGnNmb2FuCj7v47Sd670Wj49PR037GJVM2GrbsjJ5ONtcpKcc89PHFl4bFJOMxsn7s30rwma2loCri5dftm4K8zvp+IxOjW0axOaMki6zyCrcB9ZnYL8CTwHgAzawC3uvsHW/f/Hngt8DIzOwrc4u47M+5bpPLaJ58tMeNMRAt+RZcWgTqhJYlMicDdfwS8LeLxaeCDbffflGU/InXU2ScQlQTaO5rbt+18TqQbzSwWCVTU5DOAETPOukd2NJep41zCoUQgEqi4+v5Zd/5567WLHtfSE9IvrT4qEihNPpNBUSIQCVQZJ8xJOak0JBKoMk6Yk3JSIhAJmOr+MggqDYmI1JwSgYhIzak0JJKTMl42UwSUCERyUcbLZorMU2lIJAe6BKWUmRKBSA60+qeUmRKBSA40C1jKTIlAJAeaBSxlps5iqYRhj9jRLGApMyUCKb1QRuxoFrCUlUpDUnoasSOSjRKBlJ5G7Ihko0QgpacROyLZKBFI6WnEjkg26iyW0tOIHZFslAikEjRiR6R/Kg2JiNScEoGISM0pEYiI1FymRGBmF5jZ35nZY63/nh+xzWVm9o9mdtDMHjWzG7PsU0RE8pW1RTAB7HL3tcCu1v1OJ4D/4O6XANcAnzGz0Yz7FRGRnGRNBNcDX2zd/iKwsXMDd/8nd3+sdftp4AfA8oz7FRGRnGRNBK9y92dat/8FeFW3jc3scuAc4Hsxz282s2kzmz527FjG0EREJIme8wjM7JvAL0Q8dXv7HXd3M/Mu7/Nq4M+Bm939bNQ27r4d2A7QaDRi30tERPLTMxG4+1Vxz5nZ983s1e7+TOtA/4OY7V4OPAjc7u57+o5WRERyl7U0NAXc3Lp9M/DXnRuY2TnAXwFfcvcHMu5PRERyZu79V2DM7JXAfcA48CTwHnd/1swawK3u/kEzez/wv4CDbS/9gLvv7/Hex1rvWbQLgR8OYD/9CDU2xZVeqLEprvRCjW0+rovcPdWAnEyJoArMbNrdG8OOI0qosSmu9EKNTXGlF2psWeLSzGIRkZpTIhARqTklgtZw1UCFGpviSi/U2BRXeqHG1ndcte8jEBGpO7UIRERqTolARKTmapcIkiyd3drujJntb/2bCim21rYvN7OjZva5EOIys4vM7P+0vq+DZnZrIHENZRn0FH9n3zCz42b29YLjucbMDpnZYTNbtEqwmZ1rZve2nv+2ma0uMp4Ucf1m6+/qtJndMIiYEsb1ETP7TutvapeZXRRQbLea2YHW/4v/YGbrer6pu9fqH/ApYKJ1ewL4ZMx2Pwk1ttbznwW+AnwuhLhoLiZ4buv2y4AngBUBxPXLwNrW7RXAM8BoCN9Z67m3Ae8Avl5gLCM0F3p8Tet3egRY17HNh4D/3rq9Cbh3AN9RkrhWA68DvgTcUHRMKeJ6K3Be6/Z/HMT3lSK2l7fdvg74Rq/3rV2LgARLZw9RotjM7A00V3r921DicvdT7v586+65DKa1GfIy6Il+S3ffBfxbwbFcDhx298fd/RRwTyu+du3xPgC8zcxs2HG5+xPu/igQuVDlEON6yN1PtO7uAVYGFNu/tt39OaDniKA6JoKkS2e/tLUk9h4zG1Sy6BmbmS0B/hj46IBiShQXgJmtMrNHgSM0z4CfDiGutvi6LoOes1SxFWyM5m8y72jrscht3P008GPglQHENQxp47oF+JtCI3pRotjM7HfN7Hs0W6b/qdeb9lx9tIxyWjr7InefNbPXALvN7IC7Zz6A5BDbh4Ad7n40zxO2PL4zdz8CvM7MVgCTZvaAu39/2HG13qfnMujDik3Kq7WWWgN487BjaefudwN3m9l7gY/z4uKgkSqZCDyHpbPdfbb138fN7FvAenI4k8whtl8H3mRmH6JZiz/HzH7i7lGXCR1kXO3v9bSZ/V/gTTTLDEONywpaBj3P76xgs8CqtvsrW49FbXPUzF4CvAL4UQBxDUOiuMzsKppJ/81tZdEgYmtzD/Cnvd60jqWhJEtnn29m57ZuXwhsAL4TQmzu/j53H3f31TTLQ1/KmgTyiMvMVprZstbt84F/BxwKIK5hLYPeM7YB2gusNbM1re9jE8342rXHewOw21u9jUOOaxh6xmVm64E/A65z90Em+SSxrW27ey3wWM93HURPd0j/aNY9d7W+nG8CF7QebwCfb93+DeAAzR75A8AtocTWsf0HGMyooSTf2W8Bj7a+s0eBzYHE9X5gDtjf9u+yEGJr3f974Bhwkma99+qC4vlt4J9otmpvbz12F80DGcBLgfuBw8D/Bl5T9HeUMK5fa30vP6XZQjkYSFzfBL7f9jc1NYi4Esb2WZrL/u8HHgIu6fWeWmJCRKTm6lgaEhGRNkoEIiI1p0QgIlJzSgQiIjWnRCAiUnNKBCIiNadEICJSc/8fIfhzi1qX1d4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent_dim = 5\n",
    "# define the discriminator model\n",
    "generator_model = define_generator(latent_dim)\n",
    "# generate and plot generated samples\n",
    "X = generate_fake_samples(generator_model, latent_dim, 100)\n",
    "# plot the results\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator has not been trained and hence it does not generate the plot we expect . once we train \n",
    "it using the discriminator we will get the shape on the original data as seen in the first plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights in the generator model are updated based on the performance of the discriminator model.\n",
    "\n",
    "When the discriminator is good at detecting fake samples, the generator is updated more, and when the \n",
    "discriminator model is relatively poor or confused when detecting fake samples,\n",
    "the generator model is updated less\n",
    "\n",
    "This illustrates that both are adversaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion for generator\n",
    "criterion1 = nn.BCELoss()\n",
    "optimizer1 = optim.SGD(generator_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n",
    "# # prepare real samples\n",
    "#     x_real, y_real = generate_samples(n)\n",
    "#     # evaluate discriminator on real examples\n",
    "#     _, acc_real = discriminator.eval(x_real, y_real)\n",
    "#     # prepare fake examples\n",
    "#     x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "#     # evaluate discriminator on fake examples\n",
    "#     _, acc_fake = discriminator.eval(x_fake, y_fake)\n",
    "#     # summarize discriminator performance\n",
    "#     print(epoch, acc_real, acc_fake)\n",
    "#     # scatter plot real and fake data points\n",
    "#     plt.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "#     plt.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "#     plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAN(discriminator,generator ,n_epochs=10000, n_batch=64):\n",
    "    half_batch = int(n_batch/2)\n",
    "    losses=[]\n",
    "    for i in range(n_epochs):\n",
    "       \n",
    "        X_real, y_real = generate_samples(half_batch)\n",
    "        X_real = Variable(torch.from_numpy(X_real))\n",
    "        output = discriminator(X_real.float())\n",
    "        y_real = Variable(torch.FloatTensor(y_real))\n",
    "        loss = criterion(output, y_real)\n",
    "        loss.backward()\n",
    "        \n",
    "        X_fake = generate_latent_points(latent_dim, half_batch)\n",
    "\n",
    "        X_fake = Variable(torch.from_numpy(X_fake)).float()\n",
    "        fake_gen_output = generator(X_fake.float()).detach()\n",
    "        fake_output = discriminator(fake_gen_output)\n",
    "        fake_loss = criterion(fake_output, Variable(torch.zeros(half_batch,1)))\n",
    "        fake_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #generating data for input for generator\n",
    "        gen_inp = generate_latent_points(latent_dim, half_batch)\n",
    "\n",
    "        generator_input = Variable(torch.from_numpy(gen_inp)).float()\n",
    "        gen_out = generator(generator_input)\n",
    "        dis_out_gen_training = discriminator(gen_out)\n",
    "        gen_loss = criterion1(dis_out_gen_training,\n",
    "        Variable(torch.ones(half_batch,1)))\n",
    "        discriminator.zero_grad()\n",
    "        generator.zero_grad()\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optimizer1.step()\n",
    "        losses.append( gen_loss )\n",
    "        if( (i+1) % 1000)==0 :\n",
    "            print( \"Epoch %6d.Loss %5.3f  GEN Loss %5.3f\" % ( i, fake_loss, gen_loss ) )\n",
    "            plt.scatter(gen_out.data.numpy()[:,0],gen_out.data.numpy()[:,1])\n",
    "            plt.show()\n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    999.Loss 0.634  GEN Loss 0.759\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch   1999.Loss 0.634  GEN Loss 0.763\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch   2999.Loss 0.626  GEN Loss 0.751\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch   3999.Loss 0.628  GEN Loss 0.762\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch   4999.Loss 0.633  GEN Loss 0.762\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch   5999.Loss 0.636  GEN Loss 0.759\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch   6999.Loss 0.638  GEN Loss 0.762\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch   7999.Loss 0.628  GEN Loss 0.758\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch   8999.Loss 0.624  GEN Loss 0.759\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch   9999.Loss 0.636  GEN Loss 0.759\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "discriminator_model =  define_discriminator(2)\n",
    "generator_model = define_generator(5)\n",
    "trained_gen = train_GAN(discriminator=discriminator_model,generator=generator_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f520c5ab978>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "        X_fake = generate_latent_points(5, 64)\n",
    "\n",
    "        X_fake = Variable(torch.from_numpy(X_fake)).float()\n",
    "        fake_gen_output = generator_model(X_fake.float()).detach()\n",
    "        plt.scatter(fake_gen_output[:,0],fake_gen_output[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
